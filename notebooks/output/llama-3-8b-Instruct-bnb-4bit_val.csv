Unnamed: 0,target,generated_val
0,"Fix bug related to CPU training
Resolves #389
Encoder training works on CPU with this fix.",Remove unnecessary synchronization code for CPU devices.
1,"Improved docs of ImageDataGenerator
Shown default values of arguments in ImageDataGenerator docs
",Add zoom range and channel shift range options to the ImageDataGenerator.
2,"mexc networks updated to working values
fixes: #15251

```
% mexc withdraw USDT 11 '""0xa4d50e44877cbf7fe983420967d27a783807a79f""' '{""network"": ""BEP20""}'
2022-10-12T01:35:32.374Z
Node.js: v18.4.0
CCXT v1.95.35
(node:7423) ExperimentalWarning: The Fetch API is an experimental feature. This feature could change at any time
(Use `node --trace-warnings ...` to show where the warning was created)
mexc.withdraw (USDT, 11, 0xa4d50e44877cbf7fe983420967d27a783807a79f, [object Object])
2022-10-12T01:35:34.921Z iteration 0 passed in 379 ms

{
  info: { withdrawId: 'fe2a535f5cf6427aa470690b5def4f12' },
  id: 'fe2a535f5cf6427aa470690b5def4f12'
}
2022-10-12T01:35:34.921Z iteration 1 passed in 379 ms
```

```
% mexc withdraw USDT 10 TEY6qjnKDyyq5jDc3DJizWLCdUySrpQ4yp '{""network"": ""trc20""}'
2022-10-12T01:48:47.684Z
Node.js: v18.4.0
CCXT v1.95.35
(node:7980) ExperimentalWarning: The Fetch API is an experimental feature. This feature could change at any time
(Use `node --trace-warnings ...` to show where the warning was created)
mexc.withdraw (USDT, 10, TEY6qjnKDyyq5jDc3DJizWLCdUySrpQ4yp, [object Object])
2022-10-12T01:48:50.053Z iteration 0 passed in 374 ms

{
  info: { withdrawId: '8fea1067efd94e87bc0dba3baadbecfd' },
  id: '8fea1067efd94e87bc0dba3baadbecfd'
}
2022-10-12T01:48:50.053Z iteration 1 passed in 374 ms
```",Update network mapping to use uppercase for TRC20 and ERC20.
3,"Remove another reference to simplehttp
Since the command `a2enconf letsencrypt` is listed on this page, make sure the config file is named accordingly.
","Update file path in Create Apache configuration file from ""letsencrypt-simplehttp.conf"" to ""letsencrypt.conf""."
4,"client: allow callers to add information to the user agent
This change introduces a new flag to allow callers to add information to the user agent without replacing it entirely.

This allows people re-packaging or wrapping Certbot to influence its user agent string. They may which to do this so that stats/metrics related to their distribution are available to boulder.

This is beneficial for both the Certbot team and the party re-packaging Certbot as it allows the custom user agent to match the Certbot user agent as closely as possible, allowing data about use of the re-packaged version to be collected along side or separately from vanilla certbot.

Resolves #4682",Add a new option to the user agent string to allow a comment to be added. This comment can be used when repackaging Certbot or calling it from another tool to allow additional statistical data to be collected.
5,"ref(py): Sort imports in projectoptions module (1 files)
",Remove import of uuid.
6,"Fix global_steps checkpoint loading.
","### Update README.md to reflect improved DeepSpeed performanceThe README.md file has been updated to reflect the improved performance of DeepSpeed, a deep learning model training framework. The changes include:* Improved training speed: DeepSpeed can now train deep learning models with over a hundred billion parameters on current generation of GPU clusters, achieving over 5x in system performance compared to the state-of-art.* Minimal code change: Early adopters of DeepSpeed have already produced a language model (LM) with over 17B parameters called ""LLaMA"".### Update deepspeed/pt/deepspeed_light.py to reflect improved performanceThe deepspeed/pt/deepspeed_light.py file has been updated to reflect the improved performance of DeepSpeed. The changes include:* Renaming `global_step` to `global_steps` to reflect the ability to train models with multiple global steps.### Update tests/unit/test_checkpointing.py to reflect improved performanceThe tests/unit/test_checkpointing.py file has been updated to reflect the improved performance of DeepSpeed. The changes include:* Added tests for comparing DeepSpeed states, including `csr_tensor_module_names`, `skipped_steps`, and `global_steps`.* Added tests for comparing LR scheduler states, including `state_dict` and `lr_scheduler`.* Added tests for comparing model states, including `module` and `optimizer`.* Added tests for comparing optimizer states, including `optimizer` and `state_dict`."
7,"Primer: pyramid and sqlalchemy are now formatted with latest Black
* https://github.com/Pylons/pyramid/pull/3616
* https://github.com/sqlalchemy/sqlalchemy/commit/c3f102c9fe9811fd5286628cc6aafa5fbc324621

Turns Primer CI back green.

","Change the value of ""expect_formatting_changes"" from true to false in the primer.json file."
8,"Add Open Brewery DB
Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [x] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x] Your additions are ordered alphabetically
- [x] Your submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column should be padded with one space on either side
- [x] You have searched the repository for any relevant issues or pull requests
- [x] Any category you are creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Added Open Brewery DB to the list of APIs.
9,"Bump prisma from 4.11.0 to 4.13.0 in /website
Bumps [prisma](https://github.com/prisma/prisma/tree/HEAD/packages/cli) from 4.11.0 to 4.13.0.

Release notes
Sourced from prisma's releases.

4.13.0
üåü Help us spread the word about Prisma by starring the repo or tweeting about the release. üåü
Highlights
Introspection stopgaps
The Prisma Schema Language (PSL) currently doesn't support all database features and functionality of our target databases. The PSL is an abstraction over SQL and will keep evolving to address gaps in our database feature matrix.
Before this release, prisma db pull did not pick up the unsupported features in a database. It was easy to lose them when running prisma migrate dev based on an existing Prisma schema if not included in a migration file using custom migrations.
To avoid this, we added Introspection Stopgaps that surface the existence of these features in your database and link to our documentation on how to manually work around the Prisma Schema with unsupported database features (‚ÄùStopgaps‚Äù as we will remove them as soon as we implement full support for these features).
In this release, we added stopgaps for the following features:

Partitioned tables
PostgreSQL Row Level Security
Index sort order, NULLS FIRST / NULLS LAST
CockroachDB row-level TTL
Comments
PostgreSQL deferred constraints

Prisma CLI will output warnings on introspection (prisma db pull) and add comments to your Prisma schema. In the coming releases, we will expand this to many more features labeled with topic: database-functionality on GitHub.
Improved support for Netlify and Vercel build process
Netlify and Vercel cache project dependencies during the build process and reuse that cache until dependencies change. While this helps speed up the build process, any postinstall scripts of these dependencies will not be executed.
Prisma uses a postinstall script in its package to automatically trigger the customized generation of Prisma Client for your Prisma Schema. When a dependency cache is used, that generation process is not triggered, and an outdated Prisma Client may be used in your application.
When you update your Prisma Schema but not your dependencies, Prisma Client will not be generated for the new schema. For example, columns you added recently to one of your models will not be present in the Prisma Client API - causing errors.
This problem can be avoided by:

Adding a custom postinstall script in your package.json file
Manually adding a prisma generate step to the ‚ÄúBuild‚Äù scripts of Vercel and Netlify.

We now added detection of this scenario and will prevent a build without an additional prisma generate. This will ensure you're aware of the problem early and get guidance on how to fix this problem. You can read more on how to do this in our docs ‚Äî Vercel caching troubleshooting, Netlify caching troubleshooting.
Better support for pnpm as a package manager
Before this release, Prisma only used npm scripts which would lead to undesirable behavior for a project using a different package manager such as pnpm and yarn. This release improves the detection of the package managers in your project by using ni. If you're still running into this problem, let us know by creating a GitHub issue.
Segmentation fault and TLS connection error fix
In this release, we've fixed a TLS connection error segmentation fault. This mostly affected users running on Node.js 17 or later with OpenSSL 1.1 when using TLS to connect to their database.
JSON protocol Preview feature feedback
We have fixed multiple bugs for the jsonProtocol Preview feature and are close to making it Generally Available. We are still looking for feedback about its usage to ensure it is ready and works as expected for everyone.


... (truncated)


Commits

d29c126 docs(cli): add --generator option to the help hint (#18592)
e50b0e2 chore(deps): update studio to v0.484.0 (#18636)
047fda8 chore(deps): update engines to 4.13.0-15.f8f78f335fd86dea323d7fbc581fdf500d74...
906d265 chore: Remove unused variables and enable no-unused-vars rule (#18584)
5acfc52 fix(client): throw error on caching platforms (#18437)
efab78e fix(cli): Exclude source maps from bundled client (#18571)
12af01f refactor: remove unused imports from Generate.ts (#18486)
fb51d43 refactor: rename variants of BinaryType enum (#18455)
92d87a6 chore(deps): update jest to v29.4.3 (#17802)
See full diff in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=prisma&package-manager=npm_and_yarn&previous-version=4.11.0&new-version=4.13.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


",Update Prisma version from 4.11.0 to 4.13.0.
10,"Add `requests-async` link to ""non-blocking"" docs.
Links to https://github.com/encode/requests-async for an asyncio non-blocking option.",Update documentation to include `requests-async` as an example of combining Requests with an asynchronicity framework.
11,"Bump tough-cookie from 4.1.2 to 4.1.3 in /component-lib
Bumps [tough-cookie](https://github.com/salesforce/tough-cookie) from 4.1.2 to 4.1.3.

Release notes
Sourced from tough-cookie's releases.

4.1.3
Security fix for Prototype Pollution discovery in #282. This is a minor release, although output from the inspect utility is affected by this change, we felt this change was important enough to be pushed into the next patch.



Commits

4ff4d29 4.1.3 release preparation, update the package and lib/version to 4.1.3. (#284)
12d4747 Prevent prototype pollution in cookie memstore (#283)
f06b72d Fix documentation for store.findCookies, missing allowSpecialUseDomain proper...
See full diff in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tough-cookie&package-manager=npm_and_yarn&previous-version=4.1.2&new-version=4.1.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/streamlit/streamlit/network/alerts).

",Update yarn.lock: Change version of tough-cookie from 4.1.2 to 4.1.3.
12,"[letstest] create and reuse a persistent boulder AMI
Closes #2017 
",### Change: Added new command line options to `letsencrypt-auto` and updated tests to use these options.
13,"remove Animelab extractor
<!--
# Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)
- I trust Github actions to take care of Flake8. The AnimeLabIE test fails (as expected). I did not check that the AnimeLabShowsIE test fails, because I don't have an AnimeLab account

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))

---

### Description of your *pull request* and other information

This removes the Animelab extractor, because Animelab has been [shut down](https://www.animelab.com/sunset)
Please note: animelab.py currently includes a line `# TODO implement myqueue`. I don't know what myqueue is, nor could I find an explanation in the commit history. However, I am guessing that it's similar to Youtube's ""watch later"" feature, especially because of all the results that `animelab ""my queue""` returns on Google. If any maintainers want to be extra sure that it isn't anything that could still be useful, I'd suggest pinging mariuszskon

<!--
copilot:emoji
-->

:recycle::truck::scissors:

<!--
1.  :recycle: This emoji can be used to indicate a refactoring process that aims to improve the code structure, readability, or maintainability without changing its functionality or behavior.
2. :truck: This emoji can be used to indicate a file or code movement from one location to another, such as moving extractors to a separate module or renaming a file or directory.
3. :scissors: This emoji can be used to indicate a code removal or deletion, such as removing unused imports or redundant code.
-->",### DIFF:The Animelab extractor has been removed.
14,"fix: dark theme background to 404 && 500 pages
Before: 
![image](https://user-images.githubusercontent.com/65485999/216893439-a0a137ac-62c3-4628-a609-6f4e3e519dd1.png)
Now:
![image](https://user-images.githubusercontent.com/65485999/216893490-7fb7cf70-b0e3-40e5-9bf6-09b13ae50f71.png)
",Update Chakra UI imports to include `useColorMode` and add color mode support to the 404 and 500 error pages.
15,"Skip tests if redbaron is not installed
This will make sure these tests will be skipped if you don't have redbaron installed.
",Update RedBaron import to use pytest.importorskip.
16,"[polskieradio] category fixes, tag support
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

add support for 2 other pagination methods, categories with direct audio (instead of links to articles with audio), tag pages on legacy frontends


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))


",Change value of PolskieRadioRadioKierowcowIE from legacy frontend to new next.js frontend.
17,"Merge dev branch
",Changes to documentation for parameters and model tabs.
18,"set_time: use python library
",Update the date comparison logic to use the year 2023 instead of 2021.
19,"Add correlation metrics for Reward Modeling
The earlier version of this has some alignment issues which are now resolved. 

[Comparison](https://wandb.ai/open-assistant/reward-model?workspace=user-shahules786) of `main` and new PR runs 

","Added new metrics to the model training configuration and implemented the corresponding metrics functions in the `metrics.py` file. The new metrics are ""kendalltau"" and ""spearmanr"", which are used to evaluate the model's performance."
20,"[`bnb`] Add simple check for bnb import
# What does this PR do?

as discussed internally @sgugger let's add a GPU check inside `is_bnb_available` 

",Update the check for bitsandbytes availability to ensure it only returns True if CUDA is available.
21,"[dependencies] Handle deprecation of `sqlite3.version`
Refs:
https://github.com/python/cpython/issues/93370
https://github.com/python/cpython/pull/93482
https://github.com/python/cpython/commit/ffc58a9710172b2d716a810a9f303828f3ebf108

Closes #8152


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [x] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at e19a282

### Summary
üõ†Ô∏èüö´üóÉÔ∏è

<!--
1.  üõ†Ô∏è - This emoji represents the modification of the `get_package_info` function to check for a custom attribute, which is a kind of fix or improvement to the existing code.
2.  üö´ - This emoji represents the suppression of the deprecation warning that is raised by accessing the `sqlite3.version` attribute, which is a kind of prevention or avoidance of a potential problem or annoyance for the users.
3.  üóÉÔ∏è - This emoji represents the setting of the custom attribute `_yt_dlp__version` for the sqlite3 module, which is a kind of data or information management or storage.
-->
Modify `get_package_info` to support custom version attributes for dependencies and set `_yt_dlp__version` for `sqlite3` to avoid deprecation warning and improve version output.

> _`yt_dlp` updates_
> _version info of modules_
> _autumn leaves fall_

### Walkthrough
*  Add custom attribute `_yt_dlp__version` to sqlite3 module to override its version information ([link](https://github.com/yt-dlp/yt-dlp/pull/8167/files?diff=unified&w=0#diff-de7a33dfc7bfbde1984024aefea7058b51f5de324d4460efdee999bf798ebd01R46-R50))
*  Modify `get_package_info` function to check for `_yt_dlp__version` attribute in imported modules before standard attributes ([link](https://github.com/yt-dlp/yt-dlp/pull/8167/files?diff=unified&w=0#diff-3a91d23c8ecd4737ba2b42ca27465fed4d85c2b53e4e7c6632bf2b1dfefcd99fL18-R18))




",Update the `get_package_info` function to use `_yt_dlp__version` instead of `__version__` when retrieving the SQLite version.
22,"Update builder.py


## üìå Checklist before creating the PR

- [ ] I have created an issue for this PR for traceability
- [x] The title follows the standard format: `[doc/gemini/tensor/...]: A concise description`
- [x] I have added relevant tags if possible for us to better distinguish different PRs


## üö® Issue number

Not involved


## üìù What does this PR do?

Optimization Code
The source code has not been modified, only a few spelling errors in the comments have been changed


## üí• Checklist before requesting a review

- [ ] I have linked my PR to an issue ([instruction](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue))
- [x] My issue clearly describes the problem/feature/proposal, with diagrams/charts/table/code if possible
- [x] I have performed a self-review of my code
- [x] I have added thorough tests.
- [x] I have added docstrings for all the functions/methods I implemented

## ‚≠êÔ∏è Do you enjoy contributing to Colossal-AI?

- [x] üåù Yes, I do.
- [ ] üåö No, I don't.

Tell us more if you don't enjoy contributing to Colossal-AI.
",Update documentation comments to match the changes in the code.
23,"Add update pip and setuptools section
## Following workflow in `CONTRIBUTING.rst`

```pwsh
> python -m venv env
> .\env\Scripts\activate
> pip --version
pip 19.2.3 from d:\python\flask\env\lib\site-packages\pip (python 3.8)
```

```pwsh
> pip install -r .\requirements\dev.txt
> pip install -e .
Obtaining file:///D:/PYTHON/flask
    ERROR: Command errored out with exit status 1:
     command: 'd:\python\flask\env\scripts\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'D:\\PYTHON\\flask\\setup.py'""'""'; __file__='""'""'D:\\PYTHON\\flask\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base 'C:\Users\oleksis\AppData\Local\Temp\pip-pip-egg-info-6n77jjdo'
         cwd: D:\PYTHON\flask\
    Complete output (33 lines):
    Traceback (most recent call last):
      File """", line 1, in 
      File ""D:\PYTHON\flask\setup.py"", line 4, in 
        setup(
      File ""d:\python\flask\env\lib\site-packages\setuptools\__init__.py"", line 145, in setup
        return distutils.core.setup(**attrs)
      File ""D:\Anaconda3\lib\distutils\core.py"", line 121, in setup
        dist.parse_config_files()
      File ""d:\python\flask\env\lib\site-packages\setuptools\dist.py"", line 699, in parse_config_files
        parse_configuration(self, self.command_options,
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 120, in parse_configuration
        meta.parse()
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 425, in parse
        section_parser_method(section_options)
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 398, in parse_section
        self[name] = value
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 183, in __setitem__
        value = parser(value)
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 513, in _parse_version
        version = self._parse_attr(value, self.package_dir)
      File ""d:\python\flask\env\lib\site-packages\setuptools\config.py"", line 348, in _parse_attr
        module = import_module(module_name)
      File ""D:\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File """", line 1014, in _gcd_import
      File """", line 991, in _find_and_load
      File """", line 975, in _find_and_load_unlocked
      File """", line 671, in _load_unlocked
      File """", line 783, in exec_module
      File """", line 219, in _call_with_frames_removed
      File ""D:\PYTHON\flask\src\flask\__init__.py"", line 3, in 
        from werkzeug.exceptions import abort as abort
    ModuleNotFoundError: No module named 'werkzeug'
    ----------------------------------------
WARNING: Discarding file:///D:/PYTHON/flask. Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
```

## Fix the issue
Requirement pip>=20.3 (from pip-tools==6.1.0->-r .\requirements\dev.txt (line 67))

```pwsh
> python -m pip install -U pip setuptools
> pip install -r .\requirements\dev.txt && pip install -e .
```

Checklist:

- [ ] Add tests that demonstrate the correct behavior of the change. Tests should fail without the change.
- [x] Add or update relevant docs, in the docs folder and in code.
- [ ] Add an entry in `CHANGES.rst` summarizing the change and linking to the issue.
- [ ] Add `.. versionchanged::` entries in any relevant code docs.
- [ ] Run `pre-commit` hooks and fix any issues.
- [ ] Run `pytest` and `tox`, no tests failed.
",Update CONTRIBUTING.rst to include instructions for upgrading pip and setuptools.
24,"Update requirements.txt
",Update requirements.txt: comment out pytest-mock and add comment to explain the reason.
25,"bpo-40222: Mark exception table function in the dis module as private
<!--
Thanks for your contribution!
Please read this comment in its entirety. It's quite important.

# Pull Request title

It should be in the following format:

```
gh-NNNNN: Summary of the changes made
```

Where: gh-NNNNN refers to the GitHub issue number.

Most PRs will require an issue number. Trivial changes, like fixing a typo, do not need an issue.

# Backport Pull Request title

If this is a backport PR (PR made against branches other than `main`),
please ensure that the PR title is in the following format:

```
[X.Y]  (GH-NNNN)
```

Where: [X.Y] is the branch name, e.g. [3.6].

GH-NNNN refers to the PR number from `main`.

-->
",Rename parse_varint and parse_exception_table to _parse_varint and _parse_exception_table.
26,"[rllib] Better document APIs to access policy state
https://github.com/ray-project/ray/issues/2930
",Update documentation for accessing policy state and weights in RLlib.
27,"docs: move extraction up
",Update sidebar position from 3 to 0.05.
28,"Fixed warnings produced by test_record_video.py and test_video_recorder.py
With https://github.com/openai/gym/pull/2654, test_record_video.py and test_video_recorder.py were throwing new warnings
This was caused by the new render_fps value being different from the default backward compatible value. 

Updated the default backward compatible values to be the up to date parameters",Change metadata retrieval for video frames per second and output frames per second to use self.frames_per_sec and self.output_frames_per_sec instead of env.metadata.
29,"Manifest subtitles V
  Boilerplate (own code, improvement/feature/fix) 

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)

### What is the purpose of your *pull request*?
- Improvement
- New feature
- Bug fix



Previously in this series: #247, #515, #948, #2686.

The Go case was caught, again, on issue tracker: , . This PR is *not* expected to resolve the overall tickets, though.

The fix for viu is speculative, as the reporter in the upstream ticket (ytdl-org/youtube-dl#30779) used the direct stream URL instead of the player frontend URL. It is not certain if this will work, as I have no means to test it.

As a side note, the viu extractor contains a somewhat curious FIXME comment, which I did not address here, but I think is worth paying some attention to:

https://github.com/yt-dlp/yt-dlp/blob/4628a3aa751ac0b2161b216662f0e959eb9bd206/yt_dlp/extractor/viu.py#L79-L82
",Added support for extracting subtitles from M3U8 streams in yt_dlp extractor.
30,"added Kairos to Development section
","Added a new API endpoint for Kairos, a face recognition and emotion analysis service, to the list of APIs in README.md."
31,"Simplified Options._get_fields() a bit.
",Remove private fields from the list of fields when recursively calling `_get_fields` in `get_fields`.
32,"Find max function in python
","New file Maths/FindMax.py added, which calculates the maximum value in a list of numbers."
33,"Remove permissions to read Configurations for User and Viewer roles
Only `Admin` or `Op` roles should have permissions to view Configurations.

Previously, Users with `User` or `Viewer` role were able to get/view configurations using
the REST API or in the Webserver. From Airflow 2.0.1, only users with `Admin` or `Op` role would be able
to get/view Configurations.

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
","### Permission to view Airflow Configurations has been removed from `User` and `Viewer` rolePreviously, Users with `User` or `Viewer` role were able to get/view configurations using the REST API or in the Webserver. From Airflow 2.0.1, only users with `Admin` or `Op` role would be able to get/view Configurations.To allow users with other roles to view configuration, add `can read on Configurations` permissions to that role.Note that if `[webserver] expose_config` is set to `False`, the API will throw a `403` response even if the user has role with `can read on Configurations` permission."
34,"Update wording and formatting
Minor changes to https://github.com/psf/black/pull/1294.",Improved documentation and formatting for Black Gallery script.
35,"DOC Update maintainers' guide to use new wheel builder system
#### Reference Issues/PRs

Closes #18776.

#### What does this implement/fix? Explain your changes.

This PR updates the maintainers' guide to use the `Wheel builder` workflow.

#### Any other comments?

CC @ogrisel.","Update documentation for release process, specifically for creating and uploading wheels to PyPI."
36,"optimize parameters for mobile network
","Change the value of AUTORANGE_THREADS from 20 to 10, and the value of AUTORANGE_MAXSIZE from 548576 to 512 * 1024. Also, add a condition to check if mobile is True, and if so, set the value of AUTORANGE_MAXBUFFERSIZE to 20 * 1024 * 1024 / 8."
37,"Fix typo in 'Strings can be tricky sometimes'
Hi üëã ,
thanks for this awesome work. I gave it a partial read and found a typo.
Best",Update to the README.md file: added a note about the change in Constant folding behavior in Python 3.7.
38,"bump pyparsing
",Update version constraint for pyparsing from 2.2 to 2.3.
39,"[pre-commit.ci] pre-commit autoupdate

updates:
- [github.com/codespell-project/codespell: v2.2.4 ‚Üí v2.2.5](https://github.com/codespell-project/codespell/compare/v2.2.4...v2.2.5)
- [github.com/tox-dev/pyproject-fmt: 0.11.2 ‚Üí 0.12.0](https://github.com/tox-dev/pyproject-fmt/compare/0.11.2...0.12.0)
","Update pre-commit configuration to use newer versions of black, codespell, and pyproject-fmt. Add a new section for linear programming with a simplex example."
40,"add Sakura CardCaptor API


- [X] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [X] My addition is ordered alphabetically
- [X] My submission has a useful description
- [X] The description does not have more than 100 characters
- [X] The description does not end with punctuation
- [X] Each table column is padded with one space on either side
- [X] I have searched the repository for any relevant issues or pull requests
- [X] Any category I am creating has the minimum requirement of 3 items
- [X] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update README.md: added new API entry for Sakura CardCaptor.
41,"Add skip_special_tokens checkbox for Dolly model
It is disabled by default for Dolly, GALACTICA, and Open Assistant.

For #1161","### DIFF:Changes were made to the code to add a new option to skip special tokens in the text generation process. This option is available in the settings menu and can be toggled on or off. The code was modified in several places to accommodate this new option, including the `decode` function in `modules/text_generation.py`, the `generate_reply` function in `modules/text_generation.py`, and the `create_settings_menus` function in `server.py`."
42,"Remove Climacell.
Climacell no longer offers a free API.

https://www.climacell.co/weather-api/pricing/

===

Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [x] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x] Your additions are ordered alphabetically
- [x] Your submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column should be padded with one space on either side
- [x] You have searched the repository for any relevant issues or pull requests
- [x] Any category you are creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Removed ClimaCell Micro Weather API endpoint.
43,"add: some more feature packed applications.
Add and update more feature packed applications.
Cleaning the duplicates.
Add environment.yml file for dependencies of the repository.","Remove the calculator program and add a new file `determine_sign.py` to determine the sign of a number. The program can take a number as input and determine whether it is positive, negative, or zero."
44,"new command line argument --cert-passphrase added.
For decrypting the private key that is provided with the --certs command

#### Description

New command line argument added ""--cert-passphrase"", OpenSSL.crypto.load_privatekey already supports the passphrase, and it was only missing the arguments to pass the passphrase.

#### Checklist

 - [x] I have updated tests where applicable.
 - [x] I have added an entry to the CHANGELOG.
",Add support for decrypting private keys with a passphrase.
45,"Update play.py
resolve KeyError from missing key in action dict. use `(dict).get` with a default value of NOP (0).",Update the action selection logic to use a default value of 0 when the pressed keys are not found in the keys_to_action dictionary.
46,"BUG: Categorical.remove_categories(np.nan) fails when underlying dtype is float
Fixes GH #10156. This also makes different null values indistinguishable inside of remove_categories, but they're already indistinguishable in most other contexts:

``` .python
>>> pd.Categorical([], categories=[np.nan, None])
Traceback (most recent call last):
  File """", line 1, in 
  File ""pandas/core/categorical.py"", line 289, in __init__
    categories = self._validate_categories(categories)
  File ""pandas/core/categorical.py"", line 447, in _validate_categories
    raise ValueError('Categorical categories must be unique')
ValueError: Categorical categories must be unique
```
",Bug fixes and improvements in the Categorical data type.
47,"[Workflow] dedupe download on recovery




## Why are these changes needed?



## Related issue number



## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
","### DIFF:Changes to the Ray Workflow codebase.**Recovery Workflow Changes*** The `_recover_workflow_step` function now takes an additional `objectref_cache` parameter, which is used to cache object references during recovery.* The `resolved_object_refs` list is now used to store the resolved object references, which are then used to update the `objectref_cache`.* The `update_count` method in `DebugStorage` now updates the operation counter for `get` operations.**Test Changes*** New tests added to test dedupe downloads, nested workflow recovery, and serialization.* Existing tests updated to use the new `DebugStorage` class.**Storage Changes*** The `DebugStorage` class now uses a temporary directory to store debug information.* The `get_op_counter` method in `DebugStorage` now returns the operation counter for `get` operations.**Miscellaneous Changes*** The `ray.shutdown()` function is now called in the `_alter_storage` function to ensure that the Ray process is shut down before restarting with a new storage instance.* The `os.system(""ray stop --force"")` command is now used to stop the Ray process forcefully."
48,"Fix URL parsing bug
Fixes an error where pathlib incorrectly detects the name for some URLs. Used when passing an authenticated url as the data argument

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Improvement in file name parsing for remote file downloading.

### üìä Key Changes
- Simplified the parsing of file names from URLs by changing the order of operations‚Äîfirst stripping the query parameters and then extracting the name.

### üéØ Purpose & Impact
- **Enhances File Handling**: This tweak ensures that file names are extracted correctly, without any appended URL parameters that could cause errors.
- **Improves Robustness**: The update strengthens the download process, preventing potential file naming issues and ensuring successful downloads for users.
- **User Experience**: Users dealing with remote files can expect a more seamless experience with less risk of encountering file-related errors while using the code.",Update the line `file = Path(urllib.parse.unquote(file)).name.split('?')[0]` to `file = Path(urllib.parse.unquote(file).split('?')[0]).name` to correctly handle URLs with query parameters.
49,"Added 500px
The API is free to use.
","Update to the README.md file: added a new API listing for 500px, a photography community."
50,"[Windows] Working unit tests for certbot-nginx
This PR fixes certbot-nginx and relevant tests to make them succeed on Windows.

Next step will be to enable integration tests through certbot-ci in a future PR.
",### DIFF:Changes to the Certbot Nginx plugin:* Added compatibility with Python's `os` module to ensure correct handling of path normalization.* Improved error handling in the `prepare` method to provide more informative error messages.* Fixed a bug in the `abs_path` method to correctly handle absolute paths.* Updated the `choose_vhosts` method to use the `os` module's `normpath` function to ensure correct path normalization.* Improved error handling in the `deploy_cert` method to provide more informative error messages.* Added a check to ensure that the `server-root` directory is writable before attempting to lock it.* Updated the `util` module to use the `os` module's `geteuid` function to ensure correct handling of user IDs.* Improved error handling in the `parser` method to provide more informative error messages.* Added a check to ensure that the `certbot` package is installed before attempting to use its functions.* Updated the `tests` directory to include new test cases for the `certbot_nginx` plugin.* Improved error handling in the `http_01` test case to provide more informative error messages.* Added a check to ensure that the `certbot_nginx` package is installed before attempting to use its functions.* Updated the `util` module to use the `os` module's `normpath` function to ensure correct path normalization.* Improved error handling in the `tls_sni_01` test case to provide more informative error messages.* Added a check to ensure that the `certbot_nginx` package is installed before attempting to use its functions.* Updated the `tests` directory to include new test cases for the `certbot_nginx` plugin.* Improved error handling in the `util` module to provide more informative error messages.* Added a check to ensure that the `certbot_nginx` package is installed before attempting to use its functions.* Updated the `local-oldest-requirements.txt` file to include the latest version of the `acme` and `certbot` packages.* Improved error handling in the `install_and_test` script to provide more informative error messages.* Added a check to ensure that the `certbot_nginx` package is installed before attempting to use its functions.* Updated the `tox.cover` script to include new test cases for the `certbot_nginx` plugin.* Improved error handling in the `cover` function to provide more informative error messages.* Added a check to ensure that the `certbot_nginx` package is installed before attempting to use its functions.* Updated the `main` function to include new test cases for the `certbot_nginx` plugin.* Improved error handling in the `main` function to provide more informative error messages.
51,"Add CSS for lists
The added styles should improve how HTML lists look in chat and instruct mode, for comparison:

  Old chat CSS on PC
  
  ![1-chat](https://user-images.githubusercontent.com/31524206/230454316-c4940a2c-3bb4-4db4-bcd0-727a68af4f3a.png)
  


  New chat CSS on PC
  
  ![2-chat](https://user-images.githubusercontent.com/31524206/230454893-a56b8280-c6fb-406d-a8ae-4f1bbdf68557.png)




  Old instruct CSS on PC
  
  ![1-instruct](https://user-images.githubusercontent.com/31524206/230455038-465db73f-b727-4b55-8ab3-269dedb0b7ee.png)
  


  New instruct CSS on PC
  
  ![2-instruct](https://user-images.githubusercontent.com/31524206/230455133-9f56c25f-f003-4116-9226-d9b4c1267272.png)
  



  Old chat CSS on mobile
  
  ![1-chat-mobile](https://user-images.githubusercontent.com/31524206/230455450-6195d9d2-370d-44cb-be1a-c274e06d0c16.png)
  


  New chat CSS on mobile
  
  ![2-chat-mobile](https://user-images.githubusercontent.com/31524206/230455550-e2347d41-73ef-41f1-89c2-f72305208159.png)
  



  Old instruct CSS on mobile
  
  ![1-instruct-phone](https://user-images.githubusercontent.com/31524206/230455789-268ab636-0e81-481a-a60c-2db0b65893cf.png)
  


  New instruct CSS on mobile
  
  ![2-instruct-phone](https://user-images.githubusercontent.com/31524206/230455806-9e355d50-558a-4cd8-8cf0-3a15140600b2.png)
  




","Update CSS styles for `.message-body li` and `.message-body li > p` to add margin and display inline, respectively."
52,"Keep blank query values for flow hashing
I've found that some websites use query values that are blank, e.g. foobar.com/index?thing.
The current master branch would hash '/index?thing' and '/index?other' as the same flow for server replay, which is undesirable. This patch causes these blank query values to be preserved and included when hashing.
",Change in `urlparse.parse_qsl` to include blank values in the query string.
53,"Adds NSFW content filter option
This is to fix issue #285 by adding it as an option configurable under settings. I have it disabled by default as that seems to be what the general population likes, but I could see reasons to enable by default.",Add new functionality to filter out NSFW content.
54,"Use self._trial  to generate trial_name for Trainer.
# What does this PR do?

Generate trial name unless the trial is not None, and use `(self._trial or trial)` to generate trial name. 
Because [currently the `optuna` backend give a None trial when using DDP and rank != 0](https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/integrations.py#L193)

Related code:
https://github.com/huggingface/transformers/blob/bd469c40659ce76c81f69c7726759d249b4aef49/src/transformers/integrations.py#L160-L208


Or maybe the documentation should be changed.

https://github.com/huggingface/transformers/blob/bd469c40659ce76c81f69c7726759d249b4aef49/src/transformers/trainer.py#L2318-L2319

Who can review: 

* Trainer: @sgugger  
* optuna HPO: @sywangyi
",Update the trial name in the state based on the trial object when using hyperparameter search.
55,"[3.6] Install dependencies in Travis OSX build (GH-1952)
(cherry picked from commit 167e0fc211c06df)",Update Travis CI configuration to install OpenSSL and xz before running tests.
56,"üìù Add OpenAPI warning to ""Body - Fields"" docs with extra schema extensions
This PR adds a warning in the docs about generating invalid OpenAPI schemas (see #3745) when using additional keys in pydantic `Field` specifications. This warning only effects versions `fastapi>=0.66`, which could be added to the note if helpful.

I would have liked to include a workaround for those who want to keep generating valid schemas, but is currently non-trivial and either involves overriding a whole chain of field info classes, or writing a custom OpenAPI schema method, to undo the effect of `extra = ""allow""` in the `fastapi.openapi.models.Schema` class.
It is a bit of a shame this hasn't been acknowledged as an issue in #3745 but I hope that this warning will be helpful for those who are trying to use generic OpenAPI tools with FastAPI.","Add warning section to body-fields.md, explaining the implications of passing extra keys to Field."
57,"ref(ts): Migrate projectContext test from .jsx to .tsx
",Rename file from `projectContext.spec.jsx` to `projectContext.spec.tsx`.
58,"Add update template_body



## Motivation
CFn get-template after change-set execution is not showing the right template.


## Changes
- Update template-body when change-set is applied
- Added aws validated test cases.

<!-- The following sections are optional, but can be useful! 

## Testing

Description of how to test the changes

## TODO

What's left to do:

- [ ] ...
- [ ] ...

-->

",Update template deployer to set `existing_stack.template_body` to `new_stack.template_body` when updating a stack.
59,"Bump idna from 3.6 to 3.7 in /llama-index-integrations/storage/docstore/llama-index-storage-docstore-elasticsearch
Bumps [idna](https://github.com/kjd/idna) from 3.6 to 3.7.

Release notes
Sourced from idna's releases.

v3.7
What's Changed

Fix issue where specially crafted inputs to encode() could take exceptionally long amount of time to process. [CVE-2024-3651]

Thanks to Guido Vranken for reporting the issue.
Full Changelog: https://github.com/kjd/idna/compare/v3.6...v3.7



Changelog
Sourced from idna's changelog.

3.7 (2024-04-11)
++++++++++++++++

Fix issue where specially crafted inputs to encode() could
take exceptionally long amount of time to process. [CVE-2024-3651]

Thanks to Guido Vranken for reporting the issue.



Commits

1d365e1 Release v3.7
c1b3154 Merge pull request #172 from kjd/optimize-contextj
0394ec7 Merge branch 'master' into optimize-contextj
cd58a23 Merge pull request #152 from elliotwutingfeng/dev
5beb28b More efficient resolution of joiner contexts
1b12148 Update ossf/scorecard-action to v2.3.1
d516b87 Update Github actions/checkout to v4
c095c75 Merge branch 'master' into dev
60a0a4c Fix typo in GitHub Actions workflow key
5918a0e Merge branch 'master' into dev
Additional commits viewable in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=idna&package-manager=pip&previous-version=3.6&new-version=3.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show  ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/run-llama/llama_index/network/alerts).

","Update the version of the ""idna"" package from 3.6 to 3.7."
60,"[fp16/doc] correct initial_scale_power default value
This is a follow up to https://github.com/microsoft/DeepSpeed/pull/2663, fixing the doc's default from 32 to 16.

@tjruwase ",Update default value of `initial_scale_power` from 32 to 16.
61,"Fix space characters problem of Text
Related issue #1017

Code to reproduce the problem
1. character `'\n'`
```py
from manimlib.imports import *

class Test(Scene):
    def construct(self):
        t1 = Text('a\nb').move_to(UP)
        t2 = Text('123').move_to(DOWN)
        self.play(Transform(t1, t2))
        self.wait()
```
![enter](https://user-images.githubusercontent.com/47266984/81162376-ac45c300-8fbf-11ea-97bf-96c3a18eab17.gif)

2. character `'\t'`
```py
from manimlib.imports import *

class Test(Scene):
    def construct(self):
        t1 = Text('a\tb').move_to(UP)
        t2 = Text('123').move_to(DOWN)
        self.play(Transform(t1, t2))
        self.wait()
```
![tab](https://user-images.githubusercontent.com/47266984/81162609-0d6d9680-8fc0-11ea-81a2-348fe0d7a918.gif)

3. character `' '`
```py
from manimlib.imports import *

class Test(Scene):
    def construct(self):
        t1 = Text('a b').move_to(UP)
        t2 = Text('123').move_to(DOWN)
        self.play(Transform(t1, t2))
        self.wait()
```
![space](https://user-images.githubusercontent.com/47266984/81162581-00e93e00-8fc0-11ea-91bf-83b2431ad2f9.gif)
",Change the `apply_space_chars` method to handle space characters correctly.
62,"Add button to manually create dataset events
Use the new create dataset rest api endpoint to add a ""Manually create dataset event"" button in the UI:








---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).
","### Description of Pull Request:This pull request adds a new feature to manually create a dataset event for a specific dataset. The feature includes a modal window that allows users to input extra data (in JSON format) and create a new dataset event. The event is created by making a POST request to the `/api/v1.airflow_api_connexion_endpoints_dataset_endpoint_create_dataset_event` endpoint. The feature also includes a button to trigger the creation of the event, which is displayed in the dataset details page."
63,"Added Eliot logging system.
A logging library for distributed systems: https://github.com/ClusterHQ/eliot
","Added a new item to the ""A curated list of awesome Python frameworks, libraries and software"" section, specifically Eliot, a logging library for complex and distributed systems."
64,"feat: use jpeg instead of jpg, use enums instead of strings
Reasoning: 
1. we're already using jpeg to save a jpg file and should sync the filetype accordingly to support more features.
.jpg is just a compatible form for older OS which only support 3 characters for file extensions.
2. enums allow for less typos in strings and allow the actual value behind the enum option to be changed if needed.

- https://www.adobe.com/creativecloud/file-types/image/raster/jpeg-file.html
- https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#jpeg
- https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#jpeg-2000","### Change in config.py and flags.py:- The `Performance` enum has been replaced with `OutputFormat` enum in the `config.py` and `flags.py` files. The `Performance` enum has been moved to the `flags.py` file and renamed to `OutputFormat`. The `Performance` enum now has three values: `PNG`, `JPEG`, and `WEBP`."
65,"Add Cors support column for EVA
This is my first contribution for an open source project, so I'm trying to keep things as simple as possible.

I've tested this EVA API for validating emails and changed the value on the table for CORS support.
Tested on the browser and using this test cors website. 

Here is a [link for the test ](https://www.test-cors.org/#?client_method=GET&client_credentials=false&server_url=https%3A%2F%2Fapi.eva.pingutil.com%2Femail%3Femail%3Dkibutzzz%40gmail.com&server_enable=true&server_status=200&server_credentials=false&server_tabs=remote)



- [x] My submission is formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x] [N/A] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit


[squash-link]: 
",Update the EVA API documentation to include CORS support.
66,"Document that blackd ignores pyproject.toml
",Update README.md: added note about `blackd` not using `pyproject.toml` configuration.
67,"fix openai agent infinite loop
# Description

Fixes issue where we were using continue instead of breaking from function call

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
",Add two new parameters to the `from_tools` method: `max_function_calls` and `callback_manager`. Update the `reset` method to break the loop when the maximum function calls are exceeded.
68,"fix(fly): Turn links blue
Legal didn't like the links being grey so I'm removing the styling to turn them back to blue.


",Remove unnecessary styles from the `.auth-terms` class.
69,"[recurbate.com] Add recurbate extractor
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

This is a new extractor to support the NSFW website Recurbate.com, by default the website support 1 free download, so a premium subscription is required

Fixes #


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [x] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))


",Add new extractor for Recurbate.com and update existing extractor for RDSIE.
70,"Update
Added another type , `tvshort` to the `movie_types`",Add 'tvshort' to the list of movie types.
71,"fix: Don't remove comments along with parens
<!-- Hello! Thanks for submitting a PR. To help make things go a bit more
     smoothly we would appreciate that you go through this template. -->

### Description

Resolves #3815

### Checklist - did you ...

<!-- If any of the following items aren't relevant for your contribution
     please still tick them so we know you've gone through the checklist.

    All user-facing changes should get an entry. Otherwise, signal to us
    this should get the magical label to silence the CHANGELOG entry check.
    Tests are required for bugfixes and new features. Documentation changes
    are necessary for formatting and most enhancement changes. -->

- [y] Add an entry in `CHANGES.md` if necessary?
- [y] Add / update tests if necessary?
- [-] Add new / update outdated documentation?

<!-- Just as a reminder, everyone in all psf/black spaces including PRs
     must follow the PSF Code of Conduct (link below).

     Finally, once again thanks for your time and effort. If you have any
     feedback in regards to your experience contributing here, please
     let us know!

     Helpful links:

      PSF COC: https://www.python.org/psf/conduct/
      Contributing docs: https://black.readthedocs.io/en/latest/contributing/index.html
      Chat on Python Discord: https://discord.gg/RtVdv86PrH -->
",Fixed a bug where comments were mistakenly removed along with redundant parentheses.
72,"Docs: Add more section labels for referencing
Allows for sections to be linked from other projects using Intersphinx.
",Update documentation for advanced topics in requests library.
73,"3.10 support
",Update the build workflow to include Python 3.10 as a supported version. Remove the README.md file from the gym/envs directory.
74,"[FIX] elasticsearch vector_store: add await for self.client.indices.exist
# Description

- Since client is `AsyncElasticsearchClient` we need to have `await` on `self.client.indices.exist` otherwise it returns a `Coroutine` object

Fixes #11417

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Ran the script provided by the issue author and it works
- [x] I stared at the code and made sure it makes sense
",Update the `indices.exists` method to use `await` for asynchronous execution.
75,"Update pip official website url
",Update link to pip documentation from http://www.pip-installer.org/en/latest/index.html to https://pip.pypa.io/en/stable/installing/.
76,"[Classifier]: TensorBoard logging epoch fix
Fixes TB logging for classification training. @AyushExel 

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Improved logging of training metrics with epoch information in Ultralytics YOLOv5.

### üìä Key Changes
- `classifier.py`: Added epoch number to the `logger.log_metrics` function call.
- `utils/loggers/__init__.py`: Updated the `log_metrics` method to accept an additional `epoch` parameter.

### üéØ Purpose & Impact
- **Purpose**: To enhance the logging functionality by including epoch information, enabling better tracking and visualization of training progress over time.
- **Impact**: Users will now see metrics logged with corresponding epoch numbers, improving the interpretability of training logs in tools like TensorBoard and Weights & Biases. üìà This change enhances debugging and analysis for developers, researchers, and practitioners working with the YOLOv5 model.",Update logger to log metrics with epoch number.
77,"Add Kerberos CVE-2022-33679 (RC4 Is Still Considered Harmful)
","Added a new section to the Active Directory Attack methodology, detailing the CVE-2022-33679 (RC4 Is Still Considered Harmful) attack. This attack performs an encryption downgrade attack by forcing the KDC to use the RC4-MD4 algorithm and then brute-forcing the session key from the AS-REP using a known plaintext attack."
78,"Cleans all DBs on ./breeze stop
When ./breeze stop is run, we run docker-compose down under the
hood - by default with --volumes flag which also removes the
volumes. But the volumes were only defined when you
selected the database.

We want to clean up all the volumes on breeze stop
in order to avoid surprizes when you switch the DB and find
the DB is there.

Otherwise when you switch databases while they are running
stop will delete volumes for only the most recently used
database.

The fix makes sure that all the db
volumes are defined always so they are always all deleted on stop

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
","Remove volumes from backend-sqlite.yml, backend-postgres.yml, and backend-mysql.yml."
79,"Use print() function in both Python 2 and Python 3
Legacy __print__ statements are syntax errors in Python 3 but __print()__ function works as expected in both Python 2 and Python 3.",### DIFF:Changes to the Assembler/assembler.py file.Added a print statement to print the line number when an invalid syntax is encountered.Added a print statement to print the line number when an undefined variable is encountered.Added a print statement to print the line number when an undefined register is encountered.Added a print statement to print the line number when an undefined label is encountered.Added a print statement to print the line number when an unknown subprogram is encountered.Added a print statement to print the line number when a return address is not found on the stack.Added a print statement to print the line number when a register is not found.Added a print statement to print the line number when an argument is not found.The changes are minor and do not affect the functionality of the assembler.
80,"Small PEP8 corrections
","### Description:Changes were made to the facade, memento, and publish_subscribe modules. In facade.py, comments were added to the test sections. In memento.py, an import statement was added to handle exceptions, and a print statement was added to print the exception. In publish_subscribe.py, a comment block was added to the top of the file, referencing a presentation and author."
81,"fix typo
","Update to README.md: Changed ""right-cick"" to ""right-click"" in the instructions for saving a link."
82,"urllib2 is not available on Python 3.
## Pull Request Checklist

- [x] If the change being made is to a [distributed component](https://certbot.eff.org/docs/contributing.html#code-components-and-layout), edit the `master` section of `certbot/CHANGELOG.md` to include a description of the change being made.
- [x] Add or update any documentation as needed to support the changes in this PR.
- [x] Include your name in `AUTHORS.md` if you like.
",Update import statement for `urlretrieve` to use `from urllib.request import urlretrieve` instead of `from urllib import urlretrieve`.
83,"therock.has futures methods, fetchMarkets ordering, string math, leverage, margin
```
2022-01-24T12:19:20.304Z
Node.js: v14.17.0
CCXT v1.69.20
therock.fetchMarkets ()
1145 ms
     id |   symbol | base | quote | settle | baseId | quoteId | settleId | type | spot | margin | future |  swap | option | contract | linear | inverse | taker | maker | contractSize | active | expiry | expiryDatetime | strike | optionType |              precision |                                                                        limits
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 BTCEUR |  BTC/EUR |  BTC |   EUR |        |    BTC |     EUR |          | spot | true |  false |  false | false |  false |    false |        |         | 0.002 | 0.002 |              |   true |        |                |        |            | {""amount"":4,""price"":2} | {""leverage"":{""min"":1},""amount"":{""min"":0.0005},""price"":{""min"":0.01},""cost"":{}}
...
LINKETH | LINK/ETH | LINK |   ETH |        |   LINK |     ETH |          | spot | true |  false |  false | false |  false |    false |        |         | 0.002 | 0.002 |              |   true |        |                |        |            | {""amount"":4,""price"":9} |   {""leverage"":{""min"":1},""amount"":{""min"":0.01},""price"":{""min"":1e-9},""cost"":{}}
 UNIETH |  UNI/ETH |  UNI |   ETH |        |    UNI |     ETH |          | spot | true |  false |  false | false |  false |    false |        |         | 0.002 | 0.002 |              |   true |        |                |        |            | {""amount"":4,""price"":9} |   {""leverage"":{""min"":1},""amount"":{""min"":0.01},""price"":{""min"":1e-9},""cost"":{}}
26 objects
```",Added new properties to the `fetchMarkets` method and updated the `fetchMarkets` method to return the new properties.
84,"Implement support for AutoGPTQ for loading GPTQ quantized models.
This is a quick implementation of PanQiWei /[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) for inference.

This is an alternative to the current GPTQ-for-LLaMA hopefully offering a more universally supported option which is not limited to one platform like linux.

AutoGPTQ supports cuda, triton(on linux) and ~cpu~. Splitting using pre_layer is not supported.

#1263 implements another alternative for GPTQ, but due to relying on triton it is not universal. This pr should be compatible with it though as an option.

Right now requires newer than 0.0.5 version of AutoGPTQ in pypi, so build from source is required at the time of writing.

From my testing appears to be slightly slower than GPTQ-for-LLaMA triton branch and slower still with cuda. I have not compared against cuda versions of GPTQ-for-LLaMA. Probably slower than #1263

But AutoGPTQ is seeing rapid development and likely will have better performance while maintaining compatibility and I think this is the main benefit of this implementation.

Please give feedback and testing is appreciated.

",Update the AutoGPTQForCausalLM import to include BaseQuantizeConfig.
85,"Upgrade hatch to fix installing extras bug
### Description

Fix #4107 by updating hatchling  ~~in a fairly hacky way though.~~

Hatch ~~will~~ used to strip out parenthesis in the extras if it thinks it doesn't need them and wouldn't add them back in. ~~:( We can, however, add a meaningless restriction like `and python_version >= '1'` to force hatch to use our parenthesis. Extremely hacky, and I would understand if we don't want this to be merged until a cleaner hatch fix is upstream. Also happy to come up with another way of essentially just having `true` in the spec, but I couldn't find one with a quick reading of https://peps.python.org/pep-0508/.~~

### Checklist - did you ...

<!-- If any of the following items aren't relevant for your contribution
     please still tick them so we know you've gone through the checklist.

    All user-facing changes should get an entry. Otherwise, signal to us
    this should get the magical label to silence the CHANGELOG entry check.
    Tests are required for bugfixes and new features. Documentation changes
    are necessary for formatting and most enhancement changes. -->

- [x] Add an entry in `CHANGES.md` if necessary?
- [x] Add / update tests if necessary?   I'm not sure it's a great idea to add a test to see that dependency was avoided, happy to add one if desired though!
      I did manually test this by installing my local checkout of the project, confirming that the dependencies for the `d` extra aren't installed:
      
```logs
$ pip install .
Processing /home/brycew/Developer/black
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: typing-extensions>=4.0.1 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (4.9.0)
Requirement already satisfied: tomli>=1.1.0 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (2.0.1)
Requirement already satisfied: mypy-extensions>=0.4.3 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (1.0.0)
Requirement already satisfied: pathspec>=0.9.0 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (0.12.1)
Requirement already satisfied: platformdirs>=2 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (4.1.0)
Requirement already satisfied: click>=8.0.0 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (8.1.7)
Requirement already satisfied: packaging>=22.0 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1763+gdad9f79.d20231212) (23.2)
Building wheels for collected packages: black
  Building wheel for black (pyproject.toml) ... done
  Created wheel for black: filename=black-0.1.dev1763+gdad9f79.d20231212-py3-none-any.whl size=194930 sha256=a511959e5af87f2bc5623744f13eba2bd3c406fd9b522e8a537c8f3d341b1a18
  Stored in directory: /tmp/pip-ephem-wheel-cache-o2gmqktu/wheels/92/d6/de/b610e33302c2b433609d1ae2283761d3ff26bd0fb805d38fe1
Successfully built black
Installing collected packages: black
  Attempting uninstall: black
    Found existing installation: black 0.1.dev1763+gdad9f79
    Uninstalling black-0.1.dev1763+gdad9f79:
      Successfully uninstalled black-0.1.dev1763+gdad9f79
Successfully installed black-0.1.dev1763+gdad9f79.d20231212
```

When run on main, this output includes `Requirement already satisfied: aiohttp>=3.7.4 in /home/brycew/Developer/.venv/lib/python3.10/site-packages (from black==0.1.dev1762+g35ce37d) (3.9.1)`, and all of it's dependencies.

- [x] Add new / update outdated documentation? (nothing changed here)

<!-- Just as a reminder, everyone in all psf/black spaces including PRs
     must follow the PSF Code of Conduct (link below).

     Finally, once again thanks for your time and effort. If you have any
     feedback in regards to your experience contributing here, please
     let us know!

     Helpful links:

      PSF COC: https://www.python.org/psf/conduct/
      Contributing docs: https://black.readthedocs.io/en/latest/contributing/index.html
      Chat on Python Discord: https://discord.gg/RtVdv86PrH -->
","Update dependency requirements in pyproject.toml: hatchling from 1.18.0 to 1.20.0, and update pip install command in before-build."
86,"Adding Prompt lookup decoding
# What does this PR do?

Adds the prompt lookup decoding method from https://github.com/apoorvumang/prompt-lookup-decoding , issue #27722 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","### DIFF:A new `PromptLookupCandidateGenerator` class has been added to the `CandidateGenerator` hierarchy. This class generates candidates by looking up likely continuations in the provided prompt (input_ids) itself. The `update_candidate_strategy` method has been updated to accommodate this new class. Additionally, the `get_candidates` method has been modified to accommodate the new class. The `configuration_utils.py` file has been updated to include a new `prompt_lookup_num_tokens` parameter. The `utils.py` file has been updated to include a new `PromptLookupCandidateGenerator` class. The `test_utils.py` file has been updated to include a new test case for prompt lookup decoding."
87,"changing describe_parameters call to use paginator
##### SUMMARY
updated create_update_parameter() to use a paginator for the describe_parameter method when updating an existing parameter.  I ran into an issue where the parameter I needed to update was not in the first result set of items returned by: client.get_parameter(Name=args['Name'], WithDecryption=True)

See ADDITIONAL INFORMATION for details

##### ISSUE TYPE
- Bugfix Pull Request
##### COMPONENT NAME
aws_ssm_parameter_store

##### ANSIBLE VERSION

```
ansible 2.6.4
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python2.7/site-packages/ansible
  executable location = /bin/ansible
  python version = 2.7.5 (default, Aug  4 2017, 00:39:18) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]

```

##### ADDITIONAL INFORMATION

I ran into an issue where the parameter I needed to update was not in the first result set of items returned by: client.get_parameter(Name=args['Name'], WithDecryption=True)
I got this error when running my playbook:
```

Result:
 ESTABLISH LOCAL CONNECTION FOR USER: root
 EXEC /bin/sh -c 'echo ~root && sleep 0'
 EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239 `"" && echo ansible-tmp-1536863695.84-147938161401239=""` echo /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239 `"" ) && sleep 0'
Using module file /usr/lib/python2.7/site-packages/ansible/modules/cloud/amazon/aws_ssm_parameter_store.py
 PUT /root/.ansible/tmp/ansible-local-30457dmHRd8/tmp_9kYoS TO /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239/aws_ssm_parameter_store.py
 EXEC /bin/sh -c 'chmod u+x /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239/ /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239/aws_ssm_parameter_store.py && sleep 0'
 EXEC /bin/sh -c '/usr/bin/python2 /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239/aws_ssm_parameter_store.py && sleep 0'
 EXEC /bin/sh -c 'rm -f -r /root/.ansible/tmp/ansible-tmp-1536863695.84-147938161401239/ > /dev/null 2>&1 && sleep 0'
The full traceback is:
Traceback (most recent call last):
  File ""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py"", line 253, in 
    main()
  File ""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py"", line 248, in main
    (changed, response) = invocations[state](client, module)
  File ""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py"", line 192, in create_update_parameter
    if describe_existing_parameter['Parameters'][0]['Description'] != args['Description']:
IndexError: list index out of range

fatal: [localhost]: FAILED! => {
    ""changed"": false,
    ""module_stderr"": ""Traceback (most recent call last):\n  File \""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py\"", line 253, in \n    main()\n  File \""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py\"", line 248, in main\n    (changed, response) = invocations[state](client, module)\n  File \""/tmp/ansible_0pStDP/ansible_module_aws_ssm_parameter_store.py\"", line 192, in create_update_parameter\n    if describe_existing_parameter['Parameters'][0]['Description'] != args['Description']:\nIndexError: list index out of range\n"",
    ""module_stdout"": """",
    ""msg"": ""MODULE FAILURE"",
    ""rc"": 1
}
```
The call to client.get_parameter(Name=args['Name'], WithDecryption=True) was actually returning this:

```
 {u'NextToken': u'AAEAAYo5rAFED3zqXPRdM2barhRoPEy7XKiKyQseIg/NfpRoAAAAAFuai7PyhJdo0bspyIm/QCuI/KQWErCGt+O7i4bwVHyJXsX+Z0jYMdddl9TpkJwG+vxArVfXLlu+0rfdy14DbLbK8audCg4cJiThDTRdFhQLCHWptaOTIiblxoJnJfd5KT8yfXsFig/dVTcx9T8oYOFXKF6SdlJ4hlvDmzA/cdGUxTuLGqCXAj0OBmDXsH3jVRm57qEVqkWeTbXzWpZNJvQ6rsAK+UQQKJG4UlgNu3j/0iIlWjgRWr4fd3LaNjTmadFPXUW6sWw8o+tLR0j324YmVCg0s/rSo+2iXnSdh0GgPOAZfmVaBUvH2VkmYe+7yESYlUsYc7Bgzhd+BhE0Cj5h5atle5ipSiJa2/mdtwBJigg7lQiQg2tOfsCvzf9gyvpdqrNvdND37xktB6dOxu+tpUeaJDZ+joqGqdEWLTKE1jfA1ajLCjKXiP/gkUMNrZnXXx/++nmL3ohaWXdV0OfXDgzR2YxQSLTFCPhTm6TcPlK5ibhFYeaC8IX+TtsRTxJD8LMDB5lhbxwXNJO0gLm7LkSFsrTKbaiJhELgChQZBx20zX7t8XCm8pn/Zq23L7wigvnl4Ce0dIKMCqiNk7Vqls2U+90QnHK+X08ulPOS2tF4nzJ0MlTaLyYypWDlr0/Ktn/rp+kCHl1vrYOiwRJuqt8t2zrK+J5faOkleVV0Fx3g5Z4XPrpvYmt8ZlNPGE2zXQjk5insxzLFt+C7vj14xcvql4O2iAK5duFoTQE/L4/ZCw==', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'XXXXXXX', 'HTTPHeaders': {'x-amzn-requestid': 'XXXXXXX', 'date': 'Thu, 13 Sep 2018 16:09:22 GMT', 'content-length': '840', 'content-type': 'application/x-amz-json-1.1'}}, u'Parameters': []})

```
",Update the `describe_parameters` method to use the `get_paginator` method to paginate the results.
88,"build(python): Add autoreload functionality for workers
This adds autoreload functionality for [celery] workers. Uses `django.util.autoreload`.",Add autoreload option to the worker command.
89,"Fix False Positive On 500px
Fix false positive for 500px. The site changed their error text, so Sherlock started reporting false positives.",Update README.md to reflect new version (0.5.2) and update data.json to reflect new error message.
90,"Allows applying dilation by passing negative erosion kernel values. If value is negative, ‚Ä¶
‚Ä¶it turns it into a dilation kernel, which allows facehullandrect to cover more space. Can help to cover double eyebrows. Also could be useful with Masked converter for GAN in @oatssss PR.",Add option to specify erosion kernel size in the masked converter. Positive values apply erosion which reduces the edge of the swapped face. Negative values apply dilation which allows the swapped face to cover more space.
91,"Create decimal to binary
it will convert all decimal numbers to binary",Add a new file `decimal to binary` with a function `decimalToBinary` that converts decimal numbers to binary and prints them. The function uses recursion to convert the number and prints the binary representation.
92,"[doc] Fix typo in asyncio-eventloop documentation
Hi. This proposal is for a change that the python-doc-es team noticed during the translation to Spanish. 

Thanks and regards.",Update documentation for `asyncio-eventloop` to remove redundant information.
93,"onExecutionStart
On execution start, call onExecutionStart on all nodes which have it.

Allows a custom node to initialize itself - for instance, to clear any UI output from the previous run.",Add event listener to execution start event to call onExecutionStart method on each node in the graph.
94,"[WrzutaIE] Add extractor for wrzuta.pl (fixes #3072)
",Added Wrzuta extractor to youtube-dl.
95,"Fix installtion script for OpenSSL in Linux.
Ëøô‰∏™ÈóÆÈ¢òÊàëÂ∑≤ÁªèÂøç‰∫ÜÂ•Ω‰πÖ‰∫Ü, ‰∏ªË¶ÅÁöÑ‰øÆÊîπÊúâ2‰∏™:

1. Fix ArchLinux‰∏ãÁöÑOpenSSLÊòØÂê¶ÂÆâË£ÖÁöÑÂà§Êñ≠, ‰πãÂâçÁöÑËÑöÊú¨‰ºöÈªòËÆ§‰ΩøÁî®Python 3.
2. Âà†Èô§‰∫ÜInteractive ShellÁöÑÂà§Êñ≠, ÊÑüËßâ‰πãÂâçÁöÑÈÇ£‰∏™Âà§Êñ≠‰ºöÂΩ±ÂìçOpenSSLÊ≠£Â∏∏ÁöÑÂÆâË£Ö.

ÊàëÂêØÂä®XX-NetÁöÑÊñπÂºèÊòØ`./start`, ÊØèÊ¨°ÈÉΩ‰∏ç‰ºöÊâßË°åÂÆâË£ÖËÑöÊú¨. ÈÉΩÊèêÁ§∫Êàë`Please install pyOpenSSL`.",Change the condition to check if Python is installed using the `${PYTHON}` command instead of `python`.
96,"Fix error in user stats when loading
there was no check for waiting the data to be fetched, worked locally, not when deployed.",Update the `uswSWRImmutable` hook to use a fallback data object when the API request is still loading.
97,"Allow you to pass in arbitrary rsync options
",Add support for extra rsync options.
98,"Dev
","Update version number in fooocus_version.py from 2.1.839 to 2.1.840. Fix CUDA kernel errors by ensuring the latest version of Fooocus is used, upgrading the Nvidia driver to the latest version, and using CUDA 11 and Xformers if necessary."
99,"Dont require typing-extensions in 3.10
<!-- Hello! Thanks for submitting a PR. To help make things go a bit more
     smoothly we would appreciate that you go through this template. -->

### Description

<!-- Good things to put here include: reasoning for the change (please link
     any relevant issues!), any noteworthy (or hacky) choices to be aware of,
     or what the problem resolved here looked like ... we won't mind a ranty
     story :) -->

As noted on Discord by @ichard26 

My bad for bumping typing-extensions in the Jupyter PR without making it conditional on the Python version, so here's a PR to rectify

### Checklist - did you ...

<!-- If any of the following items aren't relevant for your contribution
     please still tick them so we know you've gone through the checklist.

    All user-facing changes should get an entry. Otherwise, signal to us
    this should get the magical label to silence the CHANGELOG entry check.
    Tests are required for bugfixes and new features. Documentation changes
    are necessary for formatting and most enhancement changes. -->

- [ ] Add a CHANGELOG entry if necessary?
- [ ] Add / update tests if necessary?
- [ ] Add new / update outdated documentation?

<!-- Just as a reminder, everyone in all psf/black spaces including PRs
     must follow the PSF Code of Conduct (link below).

     Finally, once again thanks for your time and effort. If you have any
     feedback in regards to your experience contributing here, please
     let us know!

     Helpful links:

      PSF COC: https://www.python.org/psf/conduct/
      Contributing docs: https://black.readthedocs.io/en/latest/contributing/index.html
      Chat on Python Discord: https://discord.gg/RtVdv86PrH -->
",Update dependency management: - Removed upper version bounds on dependencies (#2718)- `typing-extensions` is no longer a required dependency in Python 3.10+ (#2772)
100,"Ensure that manually creating a DAG run doesn't ""block"" the scheduler
It was possible to ""block"" the scheduler such that it would not schedule or queue tasks for a dag if you triggered a DAG run when the DAG was already at the max active runs.

This approach works around the problem for now, but a better longer term fix for this would be to introduce a ""queued"" state for DagRuns, and then when manually creating dag runs (or clearing) set it to queued, and only have the scheduler set DagRuns to running, nothing else -- this would mean we wouldn't need to examine active runs in the TI part of the scheduler loop, only in DagRun creation part.

Fixes #11582.

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).",
101,"Added support for filter out AWS Kendra search by score confidence
 **Description:** It will add support for filter out kendra search by score confidence which will make result more accurate.
    For example
   ```
retriever = AmazonKendraRetriever(
        index_id=kendra_index_id, top_k=5, region_name=region,
        score_confidence=""HIGH""
    )
```
Result will not include the records which has score confidence ""LOW"" or ""MEDIUM"". 
Relevant docs 
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/kendra/client/query.html
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/kendra/client/retrieve.html

 **Issue:** the issue # it resolve #11801 
**twitter:** [@SmitCode](https://twitter.com/SmitCode)

",Change the type hinting for `ScoreAttributes` to `Optional[dict]` to match the existing type hinting for `DocumentAttributes`.
102,"[tests] make it easier to run tests
Currently to run tests against the local branch and not mistakenly against the preinstalled deepspeed one has to remember to add: `PYTHONPATH=. ` as in:
```
PYTHONPATH=. pytest tests
```
this PR automates this.

Now you just need to run:
```
pytest tests
```
and it'll always test this checkout/branch and not the installed one if any.",Add a new file `tests/conftest.py` with directory-specific settings for pytest tests.
103,"Improve `--help` output
Fixes #1072

- [x] replace `main()` docstring in `--help` output with a more descriptive description of the application
- [x] correct `--clarify` and `--self-heal` option descriptions (from @hmasdev in #1024)
- [x] unify `--improve`, `--clarify` and `--self-heal` help text formats (from @hmasdev in #1024)",Improved help messages and descriptions for CLI commands.
104,"Add Chinese translation for help-fastapi.md
@Dustyposa @RunningIkkyu @phy25 Please take a look.","Add a new help page to the FastAPI documentation, providing various ways to get help, including GitHub issues, Gitter chat, Twitter, and LinkedIn."
105,"Fixed anchors of computer vision sections
If you click on the Python computer vision link, it falsely jumps to the .NET section.",Update computer vision links in README.md.
106,"Con.1 Issue #1905 Return local const
Update for Issue #1905",Update documentation for C++ Core Guidelines: added note about const correctness for returned local variables.
107,"remove tiktoken pin
As suggested in #1713",Update requirements.txt to remove tiktoken version and add new changes. Update tests/test_tokenizer.py to add multilingual support and test cases.
108,"Version 0.57.3
Just up version to 0.57.3 without any other changes. That's because 0.57.3 was just released consisting of 0.57.2, which is already merged into develop, plus a cherrypick that is also already merged into develop.","Update version numbers from 0.57.2 to 0.57.3 in sanity-checks.md, package.json, and setup.py."
109,"fix(alerts): Send null for all environments
Using `''` sets to the default no environment, using null will be more
explicit about no environment",Update environment field to allow null and default to null when environment is cleared from the UI.
110,"Updated Azure Readme, What is an N-tier architecture
",Add N-tier architecture description to Azure README.md.
111,"Fix error when there are no docker volumes present yet
##### SUMMARY


The existing code will attempt to iterate over `None`. A quick `if` statement protects the `for` loop and returns `None`. Return of `None` because there are no volumes.



<!---
If you are fixing an existing issue, please include ""Fixes #nnn"" in your
commit message and your description; but you should still explain what
the change does.
-->

##### ISSUE TYPE
 - Bugfix Pull Request


##### COMPONENT NAME
In the cloud module, in the docker module,  the `docker_volume` command


##### ANSIBLE VERSION

```
ansible 2.4.0 (devel cd32ed1929) last updated 2017/08/14 04:01:47 (GMT +000)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/underway/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /home/underway/underway-server/ansible-fork/lib/ansible
  executable location = /home/underway/underway-server/ansible-fork/bin/ansible
  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]
```


##### ADDITIONAL INFORMATION
<!---
Include additional information to help people understand the change here.
For bugs that don't have a linked bug report, a step-by-step reproduction
of the problem is helpful.
  -->

Attempt to create a volume, where there isn't one on the server yet. Manually do `docker volume ls -q | xargs docker volume rm` if needed

```
- name: Install pip
   easy_install:
    name: pip
    ",Add a check for the 'Volumes' key in the response to handle the case where it is None.
112,"Add vector make tests
Currently `gym.vector.make` does not have any tests
This PR adds a number of tests for the function parameters","Update the Gym's vector environment to allow for asynchronous vectorized environments. This update includes the ability to create asynchronous vectorized environments, the ability to specify the number of environments, and the ability to specify wrappers for the environments."
113,"Add SSL MITM using OpenSSL
","Added new section ""SSL MITM with OpenSSL"" to Network Discovery documentation, describing how to sniff and modify SSL traffic using OpenSSL."
114,"[cherry-pick] #2038
",Update default values of `det_db_unclip_ratio` and add new argument `use_dilation` in `paddleocr.py`.
115,"Fix bug in renegotiating a nonce with the server
If a session runs long enough (without constant activity) then the server can
expire the nonce the session has negotiated. If that happens the session will
get a new 401 response which we were immediately returning to the user. A user
would then have to essentially reinitialize session.auth each time they get an
unexpected 401.

Also, there's no need for setattr calls when we can simply assign the
attribute on the instance.
",Add a new attribute `num_401_calls` to the `Auth` class and update its value in the `handle_redirect` and `handle_401` methods.
116,"Typo fix
",### Removed Cairo test file and added changes to mobject/mobject.py and mobject/numbers.py.
117,"Fix Liabots session
","Add a new class attribute `_cookie_jar` to store the cookie jar and use it when creating the `ClientSession`. Also, update the logic to handle the cookie jar when creating the `ClientSession`."
118,"Add manifest includes for reuse licenses
##### SUMMARY
Add manifest includes for reuse licenses. Fixes #79368 

##### ISSUE TYPE
- Feature Pull Request

##### COMPONENT NAME
```
lib/ansible/galaxy/collection/__init__.py
```

##### ADDITIONAL INFORMATION




```paste below

```",Add additional includes for REUSE license files and ignores for commonly rejected file extensions in galaxy-manifest-reuse-licenses.yml. Update includes and excludes in __init__.py to include additional license files and exclude more file types.
119,"Add `distributed_port` for `deepspeed.initialize`
`deepspeed.initialize` does not involve the `distributed_port` argument, and always uses `TORCH_DISTRIBUTED_DEFAULT_PORT` to initialize the dist env","Add a new parameter `distributed_port` to the `initialize` function, which allows the user to specify the port used for communication during distributed training."
120,"DOC Update `validate_parameter_constraints()` docstring.
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
The docstring for `validate_parameter_constraints()` listing the options available isn't up to date. This add options that were currently missing.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Add new parameter constraints to validate against.
121,"Try to fix Captcha Issue
",### DIFF:Added support for MathJax and highlighted code blocks in the chat UI.
122,"refine parse_outputs in ExecuteNbCode.
ÈáçÊûÑExecuteNbCode‰∏≠ËæìÂá∫Ëß£ÊûêÈÉ®ÂàÜ‰ª£Á†Å„ÄÇÂÆûÁé∞ÂêåÊó∂ËæìÂá∫‰ª£Á†ÅÁâáÊÆµ‰∏≠ÁöÑÊ≠£Â∏∏ÊâìÂç∞ÂÜÖÂÆπÂíåÂºÇÂ∏∏ÂÜÖÂÆπÔºåÁ§∫‰æãÂ¶Ç‰∏ãÔºö
```python
executor = ExecuteNbCode()
code = """"""
import pandas as pd
df = pd.DataFrame({'ID': [1,2,3], 'NAME': ['a', 'b', 'c']})
print(df.columns)  # ÈúÄË¶ÅÊ≠£Â∏∏ËæìÂá∫ÁöÑÂÜÖÂÆπ
print(df['DUMMPY_ID'])  # ‰ºöÂèëÁîüÂºÇÂ∏∏ÁöÑ‰ª£Á†Å
""""""
output, is_success = await executor.run(code)
assert not is_success                 # -> True
assert ""Index(['ID', 'NAME'], dtype='object')"" in output  # -> True
assert ""Executed code failed,"" in output  # -> True
assert ""KeyError: 'DUMMPY_ID'"" in output # -> True
print(output)
#  Index(['ID', 'NAME'], dtype='object')    # ËøôÊòØÈúÄË¶ÅÊ≠£Â∏∏ËæìÂá∫ÁöÑÂÜÖÂÆπÔºåÂâ©‰∏ãÁöÑÊòØÈúÄË¶ÅËæìÂá∫ÁöÑÂºÇÂ∏∏‰ø°ÊÅØ
#, Executed code failed, please reflect the cause of bug and then debug. Truncated to show only last 2000 characters
#-----------------------------------------------------------------
# KeyError                                  Traceback (most recent call last)
# File ~/opt/anaconda3/envs/metagpt/lib/python3.9/site-packages/pandas/core/indexes/base.py:3653, in 
# Index.get_loc(self, key)
# 3652 try:
# -> 3653     return self._engine.get_loc(casted_key)
# 3654 except KeyError as err:
# ....
```",### DIFF:Changes to the `execute_nb_code` module:- Removed the `truncate` function and replaced it with a new implementation in the `run` method.- Added a new `parse_outputs` method to handle parsing of outputs received from notebook execution.- Modified the `run_cell` method to return the parsed outputs instead of a boolean value.- Added a new `show_bytes_figure` method to display bytes figures.- Modified the `run` method to use the new `parse_outputs` method and to handle asynchronous code execution.- Added a new `remove_escape_and_color_codes` function to remove escape characters and color codes from the output.- Modified the `test_truncate` test to use the new `parse_outputs` method.- Added new tests for the `parse_outputs` method and the `show_bytes_figure` method.
123,"Update builder.py
NotImplemented --> NotImplementedError

Re: https://github.com/faif/python-patterns/pull/112#discussion_r51224284
",Change the `build_floor` and `build_size` methods to raise `NotImplementedError` instead of `NotImplemented`.
124,"Catch error if multiprocessing is not available or is unable to determine the number of CPUs
resolves #4113 ",Resolve installation failure and tests crash issues related to multiprocessing.
125,"ENH Use OpenMP effective nthreads helper in sparse manhattan‚Ä¶
Follow up on #14196 ",Add check for the number of OpenMP threads used in the pairwise_fast function.
126,"[MRG+1] add Lasagna to related projects docs
add Lasagne to related projects in docs
",Update documentation to include lasagne and k-modes clustering algorithm.
127,"ref(backup): Lazily load expensive maps
Previously, we were always doing the relatively involved calculations for `get_default_comparators()`, `dependencies()`, and `sorted_dependencies()` at init time. This is wasteful, and slows down the test of anyone who imports these modules. A better solution is to resolve this information lazily when needed, and to cache that result going forward. This ensures that we still only do the work once, but now only when it is actually needed, rather than as soon as possible.","### DIFF:Changes were made to the `sentry` package's backup and comparison functionality. The main changes include:* The `build_default_comparators` function was replaced with a lazy-cached `get_default_comparators` function, which is used to lazily cache the result of the default comparators map.* The `dependencies` function was modified to use a lazy-cached `get_default_comparators` function.* The `validate` function was modified to accept an optional `comparators` argument, which defaults to `None`. If `comparators` is `None`, it is replaced with the result of `get_default_comparators()`.* The `test_good_fresh_install` and `test_good_continuing_sequences` tests were modified to use the `get_default_comparators` function.* The `test_user_pk_mapping` test was modified to use the `get_default_comparators` function.* The `test_validate` test was modified to use the `get_default_comparators` function.These changes aim to improve the performance and functionality of the `sentry` package's backup and comparison functionality."
128,"More robustly stop patches
Inspired by Joona's work at https://github.com/certbot/certbot/pull/7744#discussion_r376559363 and the pattern in [Python docs](https://docs.python.org/3/library/unittest.mock.html#patch-methods-start-and-stop), this PR more robustly stops the `mock.patch` objects that were started during `setUp`.",Update patching of get_utility and add cleanup for get_utilty_patch.
129,"DOCS: fix core api ref build
",Update the directory path from `EXP_DIR` to `CORE_DIR` in the `_document_langchain_core` function.
130,"Bump httpcore to 1.0.5
## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->
Fixes missing handling of EndOfStream errors
https://github.com/encode/httpcore/pull/899

changelog: https://github.com/encode/httpcore/compare/1.0.4...1.0.5

## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [x] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Deprecation (breaking change to happen in the future)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [ ] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: #92734 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [x] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] I have followed the [perfect PR recommendations][perfect-pr]
- [ ] The code has been formatted using Ruff (`ruff format homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/development_checklist/
[manifest-docs]: https://developers.home-assistant.io/docs/creating_integration_manifest/
[quality-scale]: https://developers.home-assistant.io/docs/integration_quality_scale_index/
[docs-repository]: https://github.com/home-assistant/home-assistant.io
[perfect-pr]: https://developers.home-assistant.io/docs/review-process/#creating-the-perfect-pr
",Update httpcore version from 1.0.4 to 1.0.5 in package_constraints.txt and gen_requirements_all.py.
131,"Typo in History
","Bugfixes: Improved proxy handling, fixed exceptions and crashes, and ensured correct pool size for proxy connections."
132,"[YouTube] Fix not finding videos listed under a channel's ""shorts"" subpage. 
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Fixed a bug related to youtube-dl not being able to find and download videos listed under a channel's ""shorts"" subpage with help and guidance from @dirkf. Also tried to improve upon the solution by having the playlist data allow a title to be proposed in the list of playlist items. When running youtube-dl with the --flat-playlist and --dump-json option, the returned JSON data now has the title for each video. 
",Add support for YouTube Shorts.
133,"Added VisPy
This is a pure Python library for high-performance scientific visualization. Based on OpenGL, it is GPU-accelerated.
","Update README.md: added VisPy to the list of awesome Python frameworks, libraries and software."
134,"Added pacman invalid option rule
Changes lowercase options to uppercase for pacman","Update the README.md file to include a new rule for pacman_invalid_option, which replaces lowercase pacman options with uppercase."
135,"Prefer more equal signs before a break when splitting chained assignments
### Description

This PR makes rhs processing prefer more equal signs before breaking the line.

Fixes #4007 

### Checklist - did you ...

- [X] Add an entry in `CHANGES.md` if necessary?
- [X] Add / update tests if necessary?","Changes to the Black code formatter:* Improved handling of chained assignments, preferring more equal signs before a break when splitting.* Additional cases of immediately nested tuples, lists, and dictionaries are now indented less.* Standalone form feed characters at the module level are no longer removed.* Preview style changes: prefer more equal signs before a break when splitting chained assignments."
136,"Corrected 2D relicts in 3D function comments
In Cropping3D and ZeroPadding3D, the comments referred to ""two"" values, which for 3D functions must be ""three"".
### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",Update the documentation for the `ZeroPadding3D` and `Cropping3D` classes to clarify the behavior when a single value is provided for symmetric padding or cropping.
137,"Update model_adapter.py




## Why are these changes needed?
The function `.lower()` makes everything lowercase which makes the comparison to 'TinyLlama' always false.



## Related issue number (if applicable)



## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [x] I've made sure the relevant tests are passing (if applicable).
",Case-insensitive match function for TinyLlama model path.
138,"fix issue 868
fix issue https://github.com/geekan/MetaGPT/issues/868;
according to https://github.com/pandas-dev/pandas/pull/55008, pd.DataFrame.copy() leaks has been fixed by pandas 2.1.1",Update pandas version from 2.0.3 to 2.1.1.
139,"Add NoPhishy to Anti-Malware


- [x] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not have more than 100 characters
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [ ] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Added a new API endpoint for NoPhishy, a service that checks links for known phishing attempts."
140,"Implemented breadth first search for shortest graph path in graph_search.py
Added `find_shortest_path_bfs` to GraphSearch class which uses breadth first search with linear time complexity.  
Updated other method names to show that they are depth first search (for example: `find_path` to `find_path_dfs`).
Included an example for the new method.",Update graph search emulation to include Breadth First Search (BFS) algorithm.
141,"Fix saving of UI defaults to settings.yaml - Fixes #5592
## Checklist:

- [X] I have read the [Contributing guidelines](https://github.com/oobabooga/text-generation-webui/wiki/Contributing-guidelines).
",Update logic for saving extension values in the UI.
142,"Adding AnimeNewsNetwork API
Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [X] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [X] Your additions are ordered alphabetically
- [X] Your submission has a useful description
- [X] The description does not end with punctuation
- [X] Each table column should be padded with one space on either side
- [X] You have searched the repository for any relevant issues or pull requests
- [ ] Any category you are creating has the minimum requirement of 3 items
- [X] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Added new API entry for AnimeNewsNetwork.
143,"unset torch arch list for JIT mode
PyTorch will add `-gencode` args for whatever CCs are in the `TORCH_CUDA_ARCH_LIST` at compile time, this makes sense when pre-compiling but does not make sense in JIT compile mode since we know at runtime what the targets need to be. Our op-builder has already set the intended targets that are needed at runtime. We need to unset the arch list before JIT compiling to avoid CC collisions such as multiple __half  implementations. This PR simply stashes the arch list before JIT compiling and resets it after build.",Add comments to explain the purpose of the new code and reset the arch list after build.
144,"fix token counting for new openai client
# Description

The new openai client changed where to find token counts, causing the token counter to be very bad for OpenAI function calling.

Fixes https://github.com/run-llama/llama_index/issues/8978

## Type of Change

- [x] Bug fix (non-breaking change which fixes an issue)

","Update the ChangeLog.md file to include new features and bug fixes, specifically adding perplexity LLM integration, fixing token counting for new openai client, and fixing a small pydantic bug in postgres vector db. Additionally, update the token_counting.py file to use the new ""usage"" attribute from the response object."
145,"added sources check to not repeat identical sources
Original PR: https://github.com/imartinez/privateGPT/pull/1679
Recreating as the branch I was working on was closed.

I found that after increasing
similarity_top_k: int in vector_store_component.py to something higher, like 10, I would getting some duplicate hits in the sources used.

Instead of just outputting the sources, as you loop through these, inject them into an array. If the item already exists in the array, don't re-add this to the sources_text.


This only affects ""Query Docs""",Improved formatting of sources list in `yield_deltas` function.
146,"Added tests for check command
Added tests for the check command to increase coverage.",Added tests for the `Command` class in the `scrapy.commands.check` module. The tests cover the following scenarios:1. Running the command with options and printing the spider's tested methods.2. Running the command without options and not crawling the spider if it has no tested methods.
147,"Skip ipv6 tests on sudo-enabled travis builds
Travis tests are currently failing because travis migrated their precise builds [1] from their container-based infrastructure to full virtual machines, which apparently don't support IPv6. This PR skips the IPv6 tests on systems where no IPv6 support is present.

[1] We use those so that our PyInstaller inaries are compiled with an old glibc version. Otherwise, they do not work on old systems (and AWS Lambda).",Add support for IPv6 and skip tests on hosts without IPv6 support.
148,"gh-103879: Fix refleak in super specialization



* Issue: gh-103879

",Update the LOAD_SUPER_ATTR_METHOD instruction to decrement the reference count of the result object.
149,"[extractor/facebook] Fix Reels Format
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

Get video format for facebook reels

Fixes #


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at 615f799

### Summary
üé•üõ†Ô∏èüîó

<!--
1.  üé• - This emoji represents the video content and the extractor that is being improved.
2.  üõ†Ô∏è - This emoji represents the fix or the workaround that is applied to the missing playable_url issue.
3.  üîó - This emoji represents the alternative source of the playable_url from the creation_story field.
-->
Improve Facebook extractor by adding a fallback for playable_url. Use the `creation_story` field of the video object to get the `playable_url` if the original field is missing. This fixes some videos not being downloadable.

> _Some videos fail_
> _`playable_url` missing_
> _Fall back to `story`_

### Walkthrough
* Add support for more video types, such as live videos, premiere videos, and videos from groups or pages ([link](https://github.com/yt-dlp/yt-dlp/pull/7564/files?diff=unified&w=0#diff-ca6052d2c84eb4ac45aaf04463a4eaecdc75fb3ae4766e234917d8ab131b60ecL504-R504),                            F0




",
150,"[pre-commit.ci] pre-commit autoupdate

updates:
- [github.com/astral-sh/ruff-pre-commit: v0.3.3 ‚Üí v0.3.4](https://github.com/astral-sh/ruff-pre-commit/compare/v0.3.3...v0.3.4)
",Update Ruff pre-commit hook to version v0.3.4.
151,"load: Optionally disable reading secrets from env
Thank you for contributing to LangChain!

- [ ] **PR title**: ""package: description""
  - Where ""package"" is whichever of langchain, community, core, experimental, etc. is being modified. Use ""docs: ..."" for purely docs changes, ""templates: ..."" for template changes, ""infra: ..."" for CI changes.
  - Example: ""community: add foobar LLM""


- [ ] **PR message**: ***Delete this entire checklist*** and replace with
    - **Description:** a description of the change
    - **Issue:** the issue # it fixes, if applicable
    - **Dependencies:** any dependencies required for this change
    - **Twitter handle:** if your PR gets announced, and you'd like a mention, we'll gladly shout you out!


- [ ] **Add tests and docs**: If you're adding a new integration, please include
  1. a test for the integration, preferably unit tests that do not rely on network access,
  2. an example notebook showing its use. It lives in `docs/docs/integrations` directory.


- [ ] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/

Additional guidelines:
- Make sure optional dependencies are imported within a function.
- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.
- Most PRs should not touch more than one package.
- Changes should be backwards compatible.
- If you are adding something to community, do not re-import it in langchain.

If no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, hwchase17.
",Add new parameter `secrets_from_env` to `load` and `loads` functions and update the logic to use this parameter.
152,"user: Remove superfluous user not found warning
##### SUMMARY

Removes the warning, when `local: true` and the user could not be found.

A note about this warning already exists in the documentation.

Fixes #80267

##### ISSUE TYPE

- Bugfix Pull Request

##### COMPONENT NAME

ansible.builtin.user

##### ADDITIONAL INFORMATION

See #80267.",Remove user not found warning when using 'local: true' in the Ansible 'user' module.
153,"Mark blackd tests with the `blackd` optional marker
This is a follow-up of #2203 that uses a pytest marker instead of a bunch of `skipUnless`.  Similarly to the Python 2 tests, they are running by default and will crash on an unsuspecting contributor with missing dependencies.  This is by design, we WANT contributors to test everything.  Unless we actually don't
and then we can run:

```
pytest --run-optional=no_blackd
```

Relatedly, bump required aiohttp to 3.6.0 at least to get rid of expected failures on Python 3.8 (see 6b5eb7d4651c7333cc3f5df4bf7aa7a1f1ffb45b).",Update aiohttp-cors to version 0.5.2.
154,"Fix help typo
","Update help message for --quiet flag to include a space after ""Implies""."
155,"Removed unneeded list() calls in sorted() argument.
","Update the `sorted` function to use a stable sort, and the `list` function to return a list of keys."
156,"add prob for ra
att",Add a new parameter `prob` to the `RandAugment` class and make it optional with a default value of 0.5.
157,"[Core] ensure uniqueness in spilled file name




## Why are these changes needed?

There are cases that same object is being spilled twice due to failures. This made two spill worker overwrites the same file and causing corruption. The fix is as simple as ensure the uniqueness of the file.

close #26395

## Related issue number



## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Add unique filename generation for spilled objects.
158,"Fixed typo
Changed config-changes in the short help (wrong) to config_changes (right)
",Update the `config-changes` command to `config_changes` in the `letsencrypt/cli.py` file.
159,"docs: extract_info(process=False)


### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [ ] Covered the code with tests (note that PRs without tests will be REJECTED)
- [ ] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

`process` must be True for `download` to work, right? maybe we could make that more clear somehow",Update documentation for `extract_info` method to include type hints and a detailed description of the method's behavior.
160,"[extensions/openai] use instruction templates with chat_completions
The ChatCompletions endpoint is much more reliable when you can instruct the model using the right format, this change loads the correct instruction template for the model if it exists, otherwise falls back to the previous behavior.

It also includes a dummy billing endpoint and some improvements for handling max_new_tokens & length when a large (4k+) context is assumed. ","### DIFF:Changes to the `script.py` file in the `extensions/openai` directory.Added two new response handling methods for `/billing/usage` and `/v1/dashboard/billing/usage` paths.The `do_POST` method now handles chat completions and text completions differently. For chat completions, it generates a prompt and chat messages based on the input. For text completions, it truncates the prompt to fit within the specified token count.The method also handles system messages and chat messages separately, and truncates the chat messages to fit within the specified token count.The `stopping_strings` list is updated to include standard stopping strings.The `req_params` dictionary is updated to include the `truncation_length` and `max_new_tokens` parameters.The `do_POST` method now prints warnings when truncating the prompt or chat messages, and when ignoring the `max_new_tokens` parameter.The `stream_object_type` and `object_type` variables are updated to reflect the type of completion being generated.The `response` variable is updated to include the generated completion."
161,"remove superfluous code
this looks like stray code
get_merged_array is computed recursively on line 701 and appended to the result
the value computed on line 704 is discarded",Remove unnecessary code from get_merged_array method.
162,"Fix broken links
### Description

There are multiple broken links in this project. Here is what I have fixed:

https://github.com/rlaphoenix/VSGAN/blob/master/vsgan/archs/ESRGAN.py --> https://github.com/rlaphoenix/VSGAN/blob/master/vsgan/archs/esrgan.py

https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py --> https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/drop.py

https://kornia-tutorials.readthedocs.io/en/latest/ --> https://kornia.readthedocs.io/en/latest/

This is an updated PR of #1007

### Support my work

These links where found with [link-inspector](https://github.com/justindhillon/link-inspector). If you find this PR useful, give the repo a ‚≠ê",Update documentation links to match the actual repository.
163,"Add Timestamp to SNS messages
This seems to be an official field on SNS messages

See https://docs.aws.amazon.com/en_pv/sns/latest/dg/sns-message-and-json-formats.html","Add a new field 'Timestamp' to the SNS message body, which is a ISO 8601 formatted date."
164,"removed InsaneJournal
Removed InsaneJournal as it returns false positives when giving a it a
username with a period. I was not able to find what the criterias were
for a valid username on InsaneJournal, so the best thing to do is to
remove it

----

Dont mind the branch name. I accidentally committed to the old branch which was used to fix the the IFTTT false positive and dont know how to revert that.",Remove InsaneJournal from the list of supported sites.
165,"Initial mobile support
- Added correct meta tags to index.html - head
- Changed litegraph.core.js - pointerevents_method: ""pointer""  
  This should not negatively affect desktop browsers, since pointer events are supported in most modern browsers. https://caniuse.com/pointer
- Litegraph's double click events are handled even using non-primary pointer.
- Changed setting to not close dialogs on mouseleave.

Current state:

- Can move around canvas
- Right click by tapping with two fingers  
  (Hint: can tap with the second finger later.)
- Can type in conditions
- Double tap to bring search box up  
  (Hint: tap somewhere around top-right on the screen so it will be shown while typing.)
- Tapping input fields opens them like normal, can type in them.

Further work:

- Zooming is not implemented yet, maybe some touch gesture would work...  
  (Zoom by mousewheel still works as usual.)
- Menu is not collapsible, might be in the way","Update meta charset and viewport in index.html, and update LiteGraph core JavaScript file."
166,"Small typo in README.
## Type of changes

- [ ] Bug fix
- [ ] New feature
- [x] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Checklist

- [ ] I've run the latest [black](https://github.com/ambv/black) with default args on new code.
- [ ] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [ ] I've added tests for new code.
- [x] I accept that @willmcgugan may be pedantic in the code review.

## Description

Typo fix in README.
","Update README.md: added ""from"" in ""from Python's logging module""."
167,"feat: add performance sdxl lightning
closes #2382, links to discussion https://github.com/lllyasviel/Fooocus/discussions/2318 & https://github.com/lllyasviel/Fooocus/discussions/2416

Based on https://huggingface.co/ByteDance/SDXL-Lightning/blob/main/sdxl_lightning_4step_lora.safetensors
Also read https://www.felixsanz.dev/articles/sdxl-lightning-quick-look-and-comparison#checkpoint-comparison (https://github.com/lllyasviel/Fooocus/discussions/2318#discussioncomment-8634972 @felixsanz).

Disables negative prompt for even faster generation with cfg = 1, even though it's capable of handling negative prompts with cfg > 1.",Add Lightning mode.
168,"Bump tox from 3.20.1 to 3.21.3
Bumps [tox](https://github.com/tox-dev/tox) from 3.20.1 to 3.21.3.

Changelog
Sourced from tox's changelog.

v3.21.3 (2021-01-28)
Bugfixes

Fix a killed tox (via SIGTERM) leaving the commands subprocesses running by handling it as if it were a KeyboardInterrupt - by dajose #1772

v3.21.2 (2021-01-19)
Bugfixes

Newer coverage tools update the COV_CORE_CONTEXT environment variable, add it to the list of environment variables that can change in our pytest plugin - by gaborbernat. #1854

v3.21.1 (2021-01-13)
Bugfixes

Fix regression that broke using install_command in config replacements - by jayvdb #1777
Fix regression parsing posargs default containing colon. - by jayvdb #1785

Features

Prevent .tox in envlist - by jayvdb #1684

Miscellaneous

Enable building tox with setuptools_scm 4 and 5 by hroncok #1799

v3.21.0 (2021-01-08)
Bugfixes

Fix the false congratulations message that appears when a KeyboardInterrupt occurs during package installation. - by gnikonorov #1453
Fix platform support for install_command. - by jayvdb #1464
Fixed regression in v3.20.0 that caused escaped curly braces in setenv to break usage of the variable elsewhere in tox.ini. - by jayvdb #1690
Prevent {} and require {: is only followed by }. - by jayvdb #1711
Raise MissingSubstitution on access of broken ini setting. - by jayvdb #1716

Features


... (truncated)


Commits

7dbed25 release 3.21.3
15533dd [pre-commit.ci] pre-commit autoupdate (#1872)
8a2bb56 Format markdown
6725807 tox use discord
833b33f Handle sigterm by killing the commands subprocess (#1860)
3d80588 Merge pull request #1858 from tox-dev/release-3.21.2
4116e90 release 3.21.2
1d23293 Newer coverage plugin updates env-vars, ignore in pytest plugin (#1854)
6624a8c Add missing backticks to my username (#1851)
51cd5fd Update README.md
Additional commits viewable in compare view





[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=tox&package-manager=pip&previous-version=3.20.1&new-version=3.21.3)](https://dependabot.com/compatibility-score/?dependency-name=tox&package-manager=pip&previous-version=3.20.1&new-version=3.21.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
If all status checks pass Dependabot will automatically merge this pull request.

[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



",Update dev dependencies to latest versions.
169,"fix lite demo readme
",Update links for downloading Paddle Lite inference library and add instructions for building Paddle Lite from source.
170,"Droplistmenu
The code is to show using tkinter GUI to creat droplist and calender, and using .place() to manage the positions.","New file added: Droplistmenu/GamesCalender.py, a GUI application using tkinter to create a dropdown menu and calendar. The application allows users to select teams and dates, and add them to a list. A README file was also added, describing the purpose of the code."
171,"Refactor a small function
",Update the while loop condition from `while 1` to `while True`.
172,"Fix issue #2077
* Update delete_keys_response_template (jinja2 template)
* Patch _bucket_response_delete_keys from moto
* Add test case for bugfix

",Changes to the S3 service in LocalStack:* Added support for deleting objects with a specific version ID.* Implemented a patch to the `_bucket_response_delete_keys` method to handle the deletion of objects with a specific version ID.* Added a test case for deleting an object with a specific version ID.
173,"CLN: redundant function in `Styler`
simplifies code by removing unnecessary function def.",Update the format_attr function to correctly handle the case where the value is a list.
174,"New GCP Module: gcp_compute_target_http_proxy_facts
##### SUMMARY

New GCP Module: gcp_compute_target_http_proxy_facts
##### ISSUE TYPE
- New Module Pull Request

##### COMPONENT NAME
gcp_compute_target_http_proxy_facts

##### ANSIBLE VERSION
```
2.7
```

##### ADDITIONAL INFORMATION
```
This was generated by [Magic Modules](https://github.com/googlecloudplatform/magic-modules)
```
",Add a new module to gather facts for GCP TargetHttpProxy.
175,"Broaden GPTQ-for-LLaMA branch support
Conditionalized the keyword arguments passed to GPTQ-for-LLaMA's `quant.make_quant()` so we can support more branches simultaneously. Pretty simple, we just test which kwargs `quant.make_quant()` currently has available and don't try to send any invalid ones.

Tested (with varying launch.py arguments) on:
* Oobabooga's old CUDA fork: [9659310499cc7a0ea5498c1beb47bb228d65d178](https://github.com/oobabooga/GPTQ-for-LLaMa/tree/9659310499cc7a0ea5498c1beb47bb228d65d178)
* Current CUDA branch: [610fdae6588c2b17bcf2726cacaaf795cd45077e](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/610fdae6588c2b17bcf2726cacaaf795cd45077e)
* Current triton branch: [6800a08c7a3d8d682499df5d6d96666253af8ed1](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/6800a08c7a3d8d682499df5d6d96666253af8ed1)

I suspect it would also work with the older non-groupsize versions too, but I don't think I have any old quantizations lying around to test it.

Following up #785

Just to be clear, this still won't make a model quantized by an excessively different version of GPTQ-for-LLaMA load, so e.g. you still need the old CUDA GPTQ-for-LLaMA in your repositories directory to use the 2-week-oldish quantizations most users probably have at this point. Likewise, if you want to try the new triton branch, you'll need to grab some fresh requantizations.
This PR is just so you can use different GPTQ-for-LLaMA versions without having to _also_ fiddle with text-generation-webui code to get things cooperating.",Add import statement for inspect module and modify the make_quant function call to include all the keyword arguments.
176,"[MRG+1] Raise warning in scikit-learn/sklearn/linear_model/cd_fast.pyx for cases when the main loop exits without reaching the desired tolerance
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10813.

#### What does this implement/fix? Explain your changes.
This pull request adds `ConvergenceWarnings` to the `enet_coordinate_descent*` solvers found in scikit-learn/sklearn/linear_model/cd_fast.pyx for cases when the main loop exits without reaching the desired tolerance.

#### Any other comments?
Tests have been included in both sklearn/linear_model/tests/test_coordinate_descent.py and sklearn/linear_model/tests/test_sparse_coordinate_descent.py

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Change value of ConvergenceWarning messages to include more information about the convergence status.
177,"[hotfix]fixed p2p process send stuck
",Add a new condition to handle the case where `send_split` is `True` and append the tensor to `object_send_list`.
178,"Fixing parameter type for obtain_certificate's domains parameter
",Update type hint for `domains` from `set` to `list` in `obtain_certificate` method.
179,"fixed tests
We had removed the username_unclaimed from data.json but forgotten to remove it from the tests",Remove check for username_unclaimed in coverage_total_check method.
180,"update minimum requirements
Set minimum requirements to latest versions. In particular Werkzeug 2.3.","Update minimum requirements to the latest versions: Werkzeug>=2.3.0, Jinja2>3.1.2, itsdangerous>=2.1.2, click>=8.1.3."
181,"manacher's algorithm to find palindromic string
manacher's algorithm to find palindromic string in linear time complexity",Implement Manacher's algorithm to find the longest palindromic substring in linear time.
182,"[DigitalConcertHall] Support for Films
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

The PR adds support for films to an existing extractor that previously only supported concerts.

Fixes #7184


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at 33df3da

### Summary
üé•üéºüõ†Ô∏è

<!--
1.  üé• - This emoji represents the addition of support for `film` videos, which are a different type of content from `concert` videos and may have different formats and metadata.
2.  üéº - This emoji represents the extraction of `chapters` for `concert` videos, which are a useful feature for navigating and identifying the different musical pieces and performers in a concert.
3.  üõ†Ô∏è - This emoji represents the update of the extractor code and the addition of a new test case, which are both necessary tasks for implementing and verifying the changes.
-->
Improve the Digital Concert Hall extractor by adding support for different video types and more metadata. Update tests accordingly.

> _`film` or `concert`, we don't care, we just want to rock_
> _We scrape the hall for every bit of metadata we can unlock_
> _We split the `concert` into `chapters`, we don't miss a note_
> _We test our code with `film` videos, we make the extractor float_

### Walkthrough
*  Update `_VALID_URL` regex to support `film` and `concert` types of videos ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL14-R14))
*  Add a new test case for a `film` type video with expected metadata ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcR43-R55))
   * Extract `type_` group from URL and use it to request appropriate JSON data from API ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL106-R123))
   * Handle different structures of `videos` list for `film` and `concert` videos ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL106-R123))
   * Pass `type_` parameter to `_entries` method ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL123-R141))
   * Accept `type_` parameter and use it to conditionally extract `chapters` field from `cuepoints` key of each video item ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL78-R91), [link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL133-R147))
   * Return a list of entries with video metadata and formats ([link](https://github.com/yt-dlp/yt-dlp/pull/7202/files?diff=unified&w=0#diff-ec781d6032769743854814b17eb49f18a75456f429a07ff12bb2dcf494bd20bcL78-R91))




",Change the type of video from 'concert' to 'film' in the extractor.
183,"Emergency Fix: SyntaxError: invalid syntax - ÊúçÂä°Âô®Á´Ø Python ËØ≠Ê≥ï BUG Á¥ßÊÄ•‰øÆÂ§çÔºÅ
This will cause Fatal problem in GAE server !!! Result in GAE_Exception 605 'status:500'


Google Log Report:


11:33:20.864
 (/base/data/home/runtimes/python27_experiment/python27_lib/versions/1/google/appengine/runtime/wsgi.py:263)
Traceback (most recent call last):
  File ""/base/data/home/runtimes/python27_experiment/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 240, in Handle
    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
  File ""/base/data/home/runtimes/python27_experiment/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler
    handler, path, err = LoadObject(self._handler)
  File ""/base/data/home/runtimes/python27_experiment/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 85, in LoadObject
    obj = __import__(path[0])
  File ""/base/data/home/apps/s~radar1-179201/1.403929351434169704/gae.py"", line 124
    data = 'HTTP/1.1 %d %s\r\n%s\r\n\r\n%s' %
                                             ^
SyntaxError: invalid syntax",Change in the `format_response` function: removed backslash (`\`) from the string formatting.
184,"Add pikaur AUR manager to Arch Linux's commands
See [pikaur repo](https://github.com/actionless/pikaur) and [AUR package](https://aur.archlinux.org/packages/pikaur)
",Update the README.md to include pikaur as a package manager option.
185,"Create model card for asafaya/bert-large-arabic
",Add new file README.md to model_cards/asafaya/bert-large-arabic directory with Arabic BERT Large model documentation.
186,"bitmart error mapping
",Add new error code '51011' to the list of error codes in the bitmart.js file.
187,"[xattrs] '--xattrs' option that writes metadata to extended attributes (implemented as a postprocessor)
I think you guys are going to like this final version of my xattr writer.

I listened to your previous objections, and rewrote most of it. It's now a postprocessor (`XAttrMetadataPP`), and it will only load the python `pyxattr` module (or search the path for `setfattr`) if the postprocessor is enabled.

Plus, it can be enabled with the `--xattrs` option.
","### DIFF:Changes to the YouTube-DL project.Added support for writing metadata to video files using Dublin Core and XDG standards.Updated the FFmpegPostProcessor class to include a new method for writing metadata to video files.Added a new class, XAttrMetadataPP, for writing metadata to video files using extended attributes.Updated the __init__.py file to include the new XAttrMetadataPP class.Updated the utils.py file to include a new function, hyphenate_date, for converting dates in 'YYYYMMDD' format to 'YYYY-MM-DD' format.No changes to the README.md file."
188,"add /path/to to gen_label doc
",Update PaddleOCR model deployment documentation and add instructions for installing paddlehub.
189,"Make yalexs_ble matcher more specific


## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->
Related issue #78051 (will not solve it since that seems to be a driver problem)


## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Deprecation (breaking change to happen in the future)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [ ] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [ ] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

The integration reached or maintains the following [Integration Quality Scale][quality-scale]:
<!--
  The Integration Quality Scale scores an integration on the code quality
  and user experience. Each level of the quality scale consists of a list
  of requirements. We highly recommend getting your integration scored!
-->

- [ ] No score or internal
- [ ] ü•à Silver
- [ ] ü•á Gold
- [ ] üèÜ Platinum

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html
[quality-scale]: https://developers.home-assistant.io/docs/en/next/integration_quality_scale_index.html
[docs-repository]: https://github.com/home-assistant/home-assistant.io
",Update manifest.json to include service_uuid for YaleXs BLE component and update generated/bluetooth.py to include service_uuid for YaleXs BLE domain.
190,"Fixed #35091 -- Allowed GeoIP2 querying using IPv4Address/IPv6Address.
ticket-35091.",Change query types to allow IPv4 and IPv6 addresses.
191,"Fix duplicate code on linear_model.rst
#### Reference Issues/PRs

No PR found with this search keyword

```
is:open label:module:linear_model label:Documentation 
```

#### What does this implement/fix? Explain your changes.

Remove `.fit()` function called twice in example code in ""Ordinary Least Squares"" section",Remove two lines of code from the documentation of LinearRegression.
192,"[MRG+1] Fixed SelectKBest corner case: k=0
Without this fix k = 0 will have the same behavior as k = 'all' based on how
the array indexing was written.

I've included a test that demonstrates proper behaviour.
",Add a new test case for selecting zero features in SelectKBest.
193,"feat: rebase to geekan:main
feat: rebase to geekan:main","### DIFF:Changes to the MetaGPT documentation and codebase.**Documentation:*** Updated the ROADMAP.md file to reflect changes in the MetaGPT development roadmap.* Added new tasks and features, including support for incremental development, human confirmation and modification, and more.* Updated the evaluation section to include new evaluation metrics and methods.**Code:*** Updated the `write_code_review.py` file to include a retry mechanism for writing code reviews.* Updated the `faiss_store.py` file to include an asynchronous search method.* Updated the `token_counter.py` file to include new token counters and models.* Updated the `setup.py` file to reflect changes in the MetaGPT version and description.* Updated the `conftest.py` file to include logging and mocking functionality.**Changes:*** The MetaGPT development roadmap has been updated to include new features and tasks.* The codebase has been updated to include new functionality and improvements.* The documentation has been updated to reflect changes in the MetaGPT development roadmap."
194,"Use Weverse preview endpoint if no auth provided
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

Change Weverse extractor to try and download videos using the preview endpoint if no credentials are provided.

Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at 302a6c7

### Summary
üîíüåêüõ†Ô∏è

<!--
1.  üîí - This emoji represents the improved handling of authentication and access rights, as the extractor now checks for the presence and validity of an access token before making requests to the API, and gracefully handles errors related to expired or revoked tokens, insufficient permissions, or missing content.
2.  üåê - This emoji represents the added support for alternative endpoints that provide previews of the posts for anonymous or unauthorized users, as the extractor now falls back to these endpoints when the access token is missing or invalid, or when the post is not accessible to the authenticated user. These endpoints are also used to extract some metadata that is not available from the main API, such as the number of comments and likes on a post.
3.  üõ†Ô∏è - This emoji represents the general refactoring and improvement of the code quality and readability, as the extractor now uses more consistent and descriptive variable names, follows the PEP 8 style guide, and uses helper functions and classes to avoid repetition and complexity.
-->
Refactor `weverse` extractor to improve authentication and access handling, and add preview support.

> _The `weverse` extractor was due_
> _For a refactor to make it more true_
> _To the access rights logic_
> _And the endpoints biologic_
> _That provide previews for me and for you_

### Walkthrough
* Move token check and login error to `_call_api` and `_call_post_api` functions ([link](https://github.com/yt-dlp/yt-dlp/pull/7924/files?diff=unified&w=0#diff-267a5b7aa508e0099ef609aad0ce38e0371f77515bdf56f4ad244143aaf59d60L73-R75))
* Use alternative endpoints for previews without authentication or access rights ([link](https://github.com/yt-dlp/yt-dlp/pull/7924/files?diff=unified&w=0#diff-267a5b7aa508e0099ef609aad0ce38e0371f77515bdf56f4ad244143aaf59d60L104-R112))




",Add check for session token expiration and account access before calling API.
195,"Exclude broken typing-extensions version + fix import
### Description

re. import, the ipynb code was assuming that typing-extensions would always be available, but that's not the case! There's an environment marker on the requirement meaning it won't get installed on 3.10 or higher. The test suite didn't catch this issue since aiohttp pulls in typing-extensions unconditionally.

### Checklist - did you ...

- [x] Add a CHANGELOG entry if necessary?
- [x] Add / update tests if necessary?
- [x] Add new / update outdated documentation?

<!-- Just as a reminder, everyone in all psf/black spaces including PRs
     must follow the PSF Code of Conduct (link below).

     Finally, once again thanks for your time and effort. If you have any
     feedback in regards to your experience contributing here, please
     let us know!

     Helpful links:

      PSF COC: https://www.python.org/psf/conduct/
      Contributing docs: https://black.readthedocs.io/en/latest/contributing/index.html
      Chat on Python Discord: https://discord.gg/RtVdv86PrH -->
",Exclude typing-extensions version 3.10.0.1 due to it being broken on Python 3.10 (#2460).
196,"Backport PR #43597 on branch 1.3.x (BUG: .cat changing dtype inplace)
Backport PR #43597: BUG: .cat changing dtype inplace","Fixed regressions in pandas:*   Fixed regression in `merge` with integer and `NaN` keys failing with `outer` merge.*   Fixed performance regression in `MultiIndex.equals`.*   Fixed regression in `Series.cat.reorder_categories` failing to update the categories on the `Series`.*   Fixed regression in `Series.cat.categories` setter failing to update the categories on the `Series`.*   Fixed regression in `DataFrame.corr` raising `ValueError` with `method=""spearman""` on 32-bit platforms."
197,"Capitalization and test to see if Travis CI is turned on
 #313 has prepared this repo for free, automated flake8 testing on each pull request but it must be enabled by @3b1b logging into https://travis-ci.com/profile via GitHub id and flipping the repository switch on.",Update documentation to reflect Python 2.7 support removal.
198,"Add reminder to local-oldest-requirements.txt.
We've updated the local-oldest-requirements files a couple times now to get ""oldest"" tests to pass but forgot to update the corresponding setup.py files. This adds a reminder for us to do that.",Updated package versions in local-oldest-requirements.txt files across various directories.
199,"[fix] Set default json dumps setting to ensure_ascii=False
# Description

Set json dumps setting to ensure_ascii=False in metadata_extractors. If not, the content that goes into openai would be wrong.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
",Change encoding of node metadata from ASCII to non-ASCII characters.
200,"Bug fix for `asyncio_mod` in `LabelledRagDataset`
# Description

Quick fix on a bug when `show_progress` is `False` and `asyncio_mod`'s `gather` method doesn't have the `desc` param.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] I stared at the code and made sure it makes sense",Add optional progress tracking for batch predictions.
201,"Print which directory gpt-engineer is using
Makes it clearer where we are actually running, especially if we're run without an explicit path as it defaults to `projects/example`",Add print statement to indicate the running directory of gpt-engineer.
202,"update py inference to 2.0 and delete fluid
","### DIFF:Changes were made to the PP-OCR inference tools, specifically to the `det_sast_loss.py`, `predict_cls.py`, `predict_det.py`, `predict_rec.py`, and `utility.py` files. The changes include:* Removing the `fluid` module and replacing it with `paddle` in several places.* Updating the `create_predictor` function to use the `inference` module instead of `fluid`.* Removing the `use_zero_copy_run` argument and related code.* Updating the `AnalysisConfig` and `create_paddle_predictor` functions to use the `inference` module.* Removing the `fluid.core.PaddleTensor` and `fluid.fluid.core.PaddlePredictor` classes.* Updating the `get_input_tensor` and `get_output_tensor` functions to use the `get_input_handle` and `get_output_handle` functions from the `inference` module.These changes are likely due to the migration of the PP-OCR inference tools from PaddlePaddle 1.x to PaddlePaddle 2.x, which removed the `fluid` module and introduced the `inference` module."
203,"Fixed #29017 -- Updated BaseCommand.leave_locale_alone doc per refs #24073.
https://code.djangoproject.com/ticket/29017",Update documentation for `BaseCommand.leave_locale_alone` attribute to clarify its behavior when translations are deactivated.
204,"Victoria open data


- [X] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [X] My addition is ordered alphabetically
- [X] My submission has a useful description
- [X] The description does not have more than 100 characters
- [X] The description does not end with punctuation
- [X] Each table column is padded with one space on either side
- [X] I have searched the repository for any relevant issues or pull requests
- [X] Any category I am creating has the minimum requirement of 3 items
- [X] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Added new government data source: Open Government, Victoria State Government."
205,"pivoting: add english version of the article
",Update links to Network Pivoting Techniques with added country flags and updated link.
206,"Add renew_hook to options stored in the renewal config, #3394
renew_hook are called for each lineage, so it makes sense to store it and run it for each. Discussion on the issue probably still applies for pre/post hooks though. Discussion was in  #3394",Update the `renewal.py` file in the `certbot` directory to include `renew_hook` in the `STR_CONFIG_ITEMS` list.
207,"Fixing Tinder false positive
changed tinder URL to one that accurately gives results","Update the ""url"" from ""https://www.gotinder.com/@{}"" to ""https://www.tinder.com/@{}""."
208,"Update thirdparty.md
","Update the third-party tool list in the documentation to remove the ""AI StudioÈ°πÁõÆ"" for electric meter reading and numbering recognition and replace it with ""ÂûÇÁ±ªÂú∫ÊôØË∞É‰ºò"" for the same purpose."
209,"Add option to set notification sound volume
## Description

* a simple description of what you're trying to accomplish
  * This PR adds the option to the settings for changing the volume of the optional notification sound that can be played after image generation. This can be useful if the volume of the used notification sound should be lowered without the need to modify the sound file self.

* a summary of changes in code
  * The option for changing the volume value was added to the shared options in percent
  * Before playing the notification sound, the volume will be adjusted based on the set value in the options

## Screenshots/videos:
![grafik](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/118578287/82b79dbd-e131-4b44-9c0a-ead230c3e7d0)


## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Update notification sound playback to include volume control and play the notification sound after image generation.
210,"[OK.RU] Raise error in case of paid videos.
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/rg3/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [ ] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [x] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information
In case of paid video raise proper error.

",Add support for paid videos on Odnoklassniki.
211,"Update README.md
added a missing key ingredient: SICP","Update to README.md: Added a new book recommendation, ""Structure and interpretation of Computer Programs"", to the list of inspiring books."
212,"feat: add Fastly CDN.
",Add support for Fastly as a CDN provider.
213,"Sort by confidence and remove excess boxes without explicit if
It is related to #10502.
Signed-off-by: Yonghye Kwon 

<!--
Thank you for submitting a YOLOv5 üöÄ Pull Request! We want to make contributing to YOLOv5 as easy and transparent as possible. A few tips to get you started:

- Search existing YOLOv5 [PRs](https://github.com/ultralytics/yolov5/pull) to see if a similar PR already exists.
- Link this PR to a YOLOv5 [issue](https://github.com/ultralytics/yolov5/issues) to help us understand what bug fix or feature is being implemented.
- Provide before and after profiling/inference/training results to help us quantify the improvement your PR provides (if applicable).

Please see our ‚úÖ [Contributing Guide](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md) for more details.
-->


## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Optimization of Non-Max Suppression (NMS) in YOLOv5 object detection.

### üìä Key Changes
- Simplified the sorting of detection boxes based on confidence score.
- Removed excess boxes beyond a maximum threshold (`max_nms`) in one line of code.

### üéØ Purpose & Impact
- **Purpose:** To streamline the box sorting process within NMS and improve code readability.
- **Impact:** This change can potentially speed up the NMS step in object detection, leading to faster inference times. It also makes the codebase easier to understand and maintain for developers. Users can expect slightly quicker results without compromising accuracy when detecting objects in images. üöÄ",Change the implementation of non-maximum suppression to sort boxes by confidence and remove excess boxes.
214,"CVE-2007-4559 Patch
# Patching CVE-2007-4559

Hi, we are security researchers from the Advanced Research Center at [Trellix](https://www.trellix.com). We have began a campaign to patch a widespread bug named CVE-2007-4559. CVE-2007-4559 is a 15 year old bug in the Python tarfile package. By using extract() or extractall() on a tarfile object without sanitizing input, a maliciously crafted .tar file could perform a directory path traversal attack. We found at least one unsantized extractall() in your codebase and are providing a patch for you via pull request. The patch essentially checks to see if all tarfile members will be extracted safely and throws an exception otherwise. We encourage you to use this patch or your own solution to secure against CVE-2007-4559. Further technical information about the vulnerability can be found in this [blog](https://www.trellix.com/en-us/about/newsroom/stories/research/tarfile-exploiting-the-world.html).

If you have further questions you may contact us through this projects lead researcher [Kasimir Schulz](mailto:kasimir.schulz@trellix.com).
","Update the `tar.extractall` method to use `safe_extract` instead, which checks for path traversal attacks."
215,"raw now returns changed: true
##### ISSUE TYPE
- Feature Pull Request
##### COMPONENT NAME

plugins/action/raw.py
##### SUMMARY

raw now returns `changed: true` for consistency w/ shell/command/script ""non-idempotent"" modules.

Updated tests, changelog.
","### Change in CHANGELOG.md: Added consistency to `raw` tasks, changed behavior of `changed` to `True` by default."
216,"Fix syntax error in lock action
I missed this in the last review üòî Looks like the job is failing with a syntax error because of the missing `with` clause.",Add `with` keyword to the `uses` step.
217,"Deploy Windows wheels to Amazon S3
## Why are these changes needed?

We need to upload Windows wheels.

## Related issue number

#631

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",Add AWS credentials configuration and upload wheels to S3.
218,"CLN: Remove redundant mixin to TimedeltaIndex
we already pass-through the relevant attributes",Remove TimelikeOps mixin from TimedeltaIndex class.
219,"fix(hybrid-cloud): Properly generate discord_request in Discord request parser
Fixes [HC-TEST-CONTROL-M7](https://sentry-st.sentry.io/issues/4681906125/?project=4505211735834624).

I've adjusted the tests to assert on `parser.get_integration_from_request()` after `parser.get_response()` to properly reflect the actual state that the parser will run with.",Improved caching of discord request.
220,"[runtime env] Parse local conda/pip requirements files before sending runtime env to Ray Client Server




## Why are these changes needed?
Validate the runtime env before sending it to the Ray Client Server.  Validation rewrites the `pip` or `conda` field if it contains a local path to a requirements file, replacing it with a Python object containing the file contents (List in the case of a pip requirements.txt, dict in the case of a conda environment.yml). This prevents errors when the validation is run again on the remote nodes.  

In the future we should refactor jobconfig.py so a validated runtime env is passed in once at the beginning, instead of running validation every time we serialize the JobConfig.



## Related issue number


Closes #20876 
## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests (Will add release test once this is merged.  Can't do it beforehand because `conda` runtime_env requires the Ray wheel to exist on AWS, so I can't manually run the release test using Ray built from source.)
   - [ ] This PR is not tested :(
",### DIFF:Changes to the Ray Python library.The changes include:* Added a `set_runtime_env` method to `JobConfig` with a `validate` parameter.* Added a `chdir` context manager to `ray/_private/test_utils.py`.* Updated `ray/_private/test_utils.py` to use the `chdir` context manager.* Updated `ray/_private/runtime_env.py` to use the `chdir` context manager.* Added a test for the `chdir` context manager in `ray/tests/test_runtime_env_complicated.py`.* Updated `ray/tests/test_runtime_env_conda.py` to use the `chdir` context manager.* Updated `ray/tests/test_runtime_env_working_dir_2.py` to use the `chdir` context manager.* Updated `ray/util/client/worker.py` to use the `set_runtime_env` method with `validate=True`.
221,"fix erroneous failures in docker_compose due to deprecation warnings ‚Ä¶
‚Ä¶from docker (#60961)

##### SUMMARY

Consistently capture stdout and stderr to prevent any output from causing erroneous failures

fixes #60961
##### ISSUE TYPE

- Bugfix Pull Request

##### COMPONENT NAME

docker_compose
##### ADDITIONAL INFORMATION




",Fix deprecation warning in Docker Compose.
222,"REF: avoid unnecessary raise in DataFrameGroupBy._cython_agg_general
Untangling these layered try/excepts is turning out to be an exceptional PITA, so splitting it into extra-small pieces.",Improved handling of groupby aggregation with numeric_only and empty groups.
223,"Update autogen.py
fix duplicate module name for callbacks module

Callback() should be 
`keras.callbacks.Callback()`
but autogen.py generated
`keras.callbacks.callbacks.Callback()`

Classes in callbacks module had the same issue.
Added callback in `post_process_signature` method in autogen.py

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",Add support for callbacks in signature post-processing.
224,"fix obj insert_nodes
insert_nodes would raise errors about unserializable objects",Add checks for IndexNode instances in the insert_nodes method to handle their serialization and deserialization.
225,"Add loss scale guard to avoid inf loop
@jeffra @sdtblck 

Brings https://github.com/EleutherAI/DeeperSpeed/commit/a6748a420be10240ff86baa65b0e6ab5fcbddf03 into upstream DeepSpeed. 

If the loss scale reaches the minimum value and we attempt to decrease it again, raise an error to avoid infinite loop.",Add a new parameter `raise_error_at_min_scale` to the `DynamicLossScaler` class and update the `update_scale` method to raise an error when the current loss scale reaches the minimum scale.
226,"Made observation spaces Dict iterable
So far, the Dict in gym.spaces was crashing when one tries to iterate through it. This happened because it had a __getitem__() method defined, but no __iter__(). Hence:
```
for key in Dict():
```
lead to 
```
for key in Dict():
  File ""/opt/anaconda/lib/python3.7/site-packages/gym/spaces/dict.py"", line 64, in __getitem__
    return self.spaces[key]
KeyError: 0
```

I fixed this by adding a __iter__ method to Dict. Now it is iterable.",Add iteration functionality to the Dict space in Gym.
227,"Address #2786 - Fix code style in Project Euler Problem 76 
Signed-off-by: joan.rosellr 

### **Describe your change:**



* [ ] Add an algorithm?
* [x] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### **Checklist:**
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [x] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
",Change the function signature and docstring in `partition.py` to `solution.py` and add a default value for the input parameter.
228,"Fix notebook that 'cannot convert' via nbdoc_build
",Update Python version from 3.10.9 to 3.11.2.
229,"Check if we need to upgrade deps before rebuilding everything
Due to the order of checkes in selective_ci_checks.sh, we were never
eagerly upgrading deps on a Pull Reuqest, because we exited before
getting to `check_if_setup_files_changed`.

Previoulsy the output was this

```
Get changed files
Check if everything should be run

  Changed files matching the ^.github/workflows/|^Dockerfile|^scripts|^setup.py|^setup.cfg pattern:

  Dockerfile
  Dockerfile.ci
  setup.py

  Important environment files changed. Running everything
  ...
  image-build=true
  upgrade-to-newer-dependencies=false
```

And then it exited. By simply changing the order we set the right flag
_first_ and then exit.
",Update check_if_setup_files_changed function to check for changes in setup.py/cfg instead of generic setup files.
230,"Fix typo in tutorial
","Update documentation: Changed ""chose a"" to ""choose a"" in introduction.rst."
231,"Update 4 Digit Number Combinations.py
 Updated the code and appended the strings to the list, if the user needs to access it.",Remove unnecessary comments and print statements.
232,"Skip async tests on old python versions
This should fix the tests! :)",Add skipif markers for Python versions less than 3.8 in test_readfile.py.
233,"Support inference with OFT networks
## Description

This PR adds support for inference of OFT networks trained with kohya-ss [sd-scripts](https://github.com/kohya-ss/sd-scripts). The implementation is based on kohya's implementation here: https://github.com/kohya-ss/sd-scripts/blob/main/networks/oft.py

~~This is a draft PR because of these major issues:~~
* ~~The network remains affects all generations after loading even when supposed to be unloaded~~
* ~~It noticeably slows down inference~~

~~The current implementation replaces sd_module's forward method with a custom forward method, which I believe is the cause of the network continuing to affect future generations and the speed of inference.~~

### ~~Any suggestions on how to fix these issues is greatly appreciated!~~

Related links:
OFT project page: https://oft.wyliu.com
OFT official repository: https://github.com/Zeju1997/oft

## Screenshots/videos:


## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
","### DIFF:The pull request adds a new module type for LoRa's OFT (Orthogonal Factorization Transform) and COFT (Constrained Orthogonal Factorization Transform) in the LyCORIS framework. The module type is named `ModuleTypeOFT` and is used to create a network module that applies the OFT/COFT transformation to the input data.The module type is implemented in the `network_oft.py` file and includes two main functions: `create_module` and `calc_updown`. The `create_module` function creates a new network module instance based on the input data and weights, while the `calc_updown` function applies the OFT/COFT transformation to the input data.The pull request also adds a new network type for OFT/COFT in the `networks.py` file, which is used to create a new network instance that applies the OFT/COFT transformation to the input data.The changes include:* Adding a new module type for LoRa's OFT/COFT* Implementing the `create_module` and `calc_updown` functions in the `network_oft.py` file* Adding a new network type for OFT/COFT in the `networks.py` file* Updating the `load_network` function in the `networks.py` file to handle the new network typeThe pull request is a significant update to the LyCORIS framework and adds a new module type for LoRa's OFT/COFT. The changes are well-documented and include a clear description of the new module type and its functions."
234,"Fix incorrect statement id in Lambda add-permission API
Fixed random statement id in order to use the provided by the requests #1788.

Updated tests and responses when the policies don't exists anymore.
",Add a statement ID to the lambda permission and update the test to match the new behavior.
235,"Update quickstart doc and set version in pyproject.toml to 0.2.0
",Update private-gpt to version 0.2.0 and modify quickstart instructions to use poetry run command.
236,"Remove extra asterisk in example in C.60
See #1429",Fix typo in return statement.
237,"Allow _acme-challenge as a zone
Like described here:
https://github.com/lukas2511/dehydrated/wiki/example-dns-01-nsupdate-script

Not using this patch may be an issue if the parent zone has been (where a wildcard certificate has been requested.) signed by DNSSEC.

Please consider this also for inclusion before dns-01 will be allowed for wildcards.","Changes to the DNS RFC2136 plugin:* Removed the `domain` parameter from the `add_txt_record` and `del_txt_record` methods, as it is no longer needed.* Updated the `_find_domain` method to use the `record_name` parameter instead of `domain_name`.* Removed the `domain` parameter from the `_query_soa` method, as it is no longer needed.* Updated the test cases to reflect the changes."
238,"fixed broken badge link
",Update badges in README files to point to new workflow.
239,"Update Breadth_First_Search.py
Fixed the fact that all nodes except source are marked as visited twice",Add a line to mark a node as visited in the Breadth First Search algorithm.
240,"Update docs/cli-help.txt -- typo and formatting
'areusing' -> 'are using'",Update documentation for DNS Made Easy plugin to match the CLI option.
241,"fix concurrency issues of cfn and sns tests
This PR introduces concurrency control for two tests that [started failing/flaking](https://app.circleci.com/pipelines/github/localstack/localstack/5973/workflows/ddd64c56-1f61-4bfe-900a-b069bf9a28bf/jobs/33718/tests#failed-test-0) after the ASF migration.
",### Description:Update test cases for CloudFormation and SNS topics to include new functionality and improve test coverage.
242,"binance.createOrder market buy string math
```
% binance createOrder ADA/USDT market buy 23 0.45
2022-09-07T01:56:25.564Z
Node.js: v18.4.0
CCXT v1.93.9
(node:9030) ExperimentalWarning: The Fetch API is an experimental feature. This feature could change at any time
(Use `node --trace-warnings ...` to show where the warning was created)
binance.createOrder (ADA/USDT, market, buy, 23, 0.45)
2022-09-07T01:56:27.681Z iteration 0 passed in 676 ms

{
  info: {
    symbol: 'ADAUSDT',
    orderId: '3621981315',
    orderListId: '-1',
    clientOrderId: 'x-R4BD3S82d99ffb7e94d1468ca53816',
    transactTime: '1662515787820',
    price: '0.00000000',
    origQty: '22.50000000',
    executedQty: '22.50000000',
    cummulativeQuoteQty: '10.33650000',
    status: 'FILLED',
    timeInForce: 'GTC',
    type: 'MARKET',
    side: 'BUY',
    fills: [
      {
        price: '0.45940000',
        qty: '22.50000000',
        commission: '0.02250000',
        commissionAsset: 'ADA',
        tradeId: '406917760'
      }
    ]
  },
  id: '3621981315',
  clientOrderId: 'x-R4BD3S82d99ffb7e94d1468ca53816',
  timestamp: 1662515787820,
  datetime: '2022-09-07T01:56:27.820Z',
  lastTradeTimestamp: undefined,
  symbol: 'ADA/USDT',
  type: 'market',
  timeInForce: 'GTC',
  postOnly: false,
  reduceOnly: undefined,
  side: 'buy',
  price: 0.4594,
  stopPrice: undefined,
  amount: 22.5,
  cost: 10.3365,
  average: 0.4594,
  filled: 22.5,
  remaining: 0,
  status: 'closed',
  fee: { currency: 'ADA', cost: 0.0225 },
  trades: [
    {
      info: {
        price: '0.45940000',
        qty: '22.50000000',
        commission: '0.02250000',
        commissionAsset: 'ADA',
        tradeId: '406917760'
      },
      timestamp: undefined,
      datetime: undefined,
      symbol: 'ADA/USDT',
      id: '406917760',
      order: '3621981315',
      type: 'market',
      side: 'buy',
      takerOrMaker: undefined,
      price: 0.4594,
      amount: 22.5,
      cost: 10.3365,
      fee: { cost: 0.0225, currency: 'ADA' },
      fees: [ { currency: 'ADA', cost: '0.02250000' } ]
    }
  ],
  fees: [ { currency: 'ADA', cost: 0.0225 } ]
}
2022-09-07T01:56:27.681Z iteration 1 passed in 676 ms
```",Change calculation of `quoteOrderQty` to use `Precise.stringMul` for multiplication of `amount` and `price`.
243,"Fix typo discovered by codespell
___codespell --quiet-level=2___
```
./python-patterns/patterns/behavioral/publish_subscribe.py:68: subscirbed ==> subscribed
./python-patterns/patterns/structural/front_controller.py:30: cant ==> can't
./python-patterns/patterns/structural/front_controller.py:72: cant ==> can't
```",### Description:Improved documentation and code comments in the publish-subscribe and front controller patterns.
244,"[highlighter] Add complex numbers pattern to our highlighter's `number` matching
## Type of changes

- [x] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Checklist

- [x] I've run the latest [black](https://github.com/psf/black) with default args on new code.
- [x] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [x] I've added tests for new code.
- [x] I accept that @willmcgugan may be pedantic in the code review.

## Description

closes #2214 

The fix in action:

#### In IPython:
![Screenshot from 2022-04-25 12-24-00](https://user-images.githubusercontent.com/722388/165079687-6e497cb1-91ad-48e1-868f-f0deb7f0c5d5.png)

#### In a Jupyter notebook:
![Screenshot from 2022-04-25 12-28-04](https://user-images.githubusercontent.com/722388/165080326-d576b0be-5b2e-4a8b-b6a1-52c1fa1a71f6.png)

#### In plain Python 
![Screenshot from 2022-04-25 12-29-03](https://user-images.githubusercontent.com/722388/165080507-08913ca4-b8d4-483c-b8af-31b53a93f33c.png)

",Fixed markup escaping issue and added support for complex numbers in the highlighter.
245,"[youku] Update cccode
Update cccode to 0510 for youku.py

Get from:

http://g.alicdn.com/player/ykplayer/0.5.28/youku-player.min.js

{""0505"":""interior"",""050F"":""interior"",""0501"":""interior"",""0502"":""interior"",""0503"":""interior"",""0510"":""adshow"",""0512"":""BDskin"",""0590"":""BDskin""}

Before the fix:
```
you-get --debug http://v.youku.com/v_show/id_XMzU5NjkxNTM1Ng==.html?spm=a2h0z.8244218.2371631.5
[DEBUG] get_content: https://ups.youku.com/ups/get.json?vid=XMzU5NjkxNTM1Ng==&ccode=0502&client_ip=192.168.1.1&utid=%2BoR%2BE7ve%2BDsCARi5QmohYOQw&client_ts=1526175486&ckey=DIl58SLFxFNndSV1GFNnMQVYkx1PP5tKe1siZu/86PR1u/Wh1Ptd%2BWOZsHHWxysSfAOhNJpdVWsdVJNsfJ8Sxd8WKVvNfAS8aS8fAOzYARzPyPc3JvtnPHjTdKfESTdnuTW6ZPvk2pNDh4uFzotgdMEFkzQ5wZVXl2Pf1/Y6hLK0OnCNxBj3%2Bnb0v72gZ6b0td%2BWOZsHHWxysSo/0y9D2K42SaB8Y/%2BaD2K42SaB8Y/%2BahU%2BWOZsHcrxysooUeND
you-get: Áî®Êà∑Ë¥¶Êà∑ÂºÇÂ∏∏„ÄÅËØ∑ÈáçÊñ∞ÁôªÂΩï
```

After the fix:

```
you-get --debug http://v.youku.com/v_show/id_XMzU5NjkxNTM1Ng==.html?spm=a2h0z.8244218.2371631.5
[DEBUG] get_content: https://ups.youku.com/ups/get.json?vid=XMzU5NjkxNTM1Ng==&ccode=0510&client_ip=192.168.1.1&utid=IYV%2BE0QbrVcCARi5Qmqd6hjL&client_ts=1526175525&ckey=DIl58SLFxFNndSV1GFNnMQVYkx1PP5tKe1siZu/86PR1u/Wh1Ptd%2BWOZsHHWxysSfAOhNJpdVWsdVJNsfJ8Sxd8WKVvNfAS8aS8fAOzYARzPyPc3JvtnPHjTdKfESTdnuTW6ZPvk2pNDh4uFzotgdMEFkzQ5wZVXl2Pf1/Y6hLK0OnCNxBj3%2Bnb0v72gZ6b0td%2BWOZsHHWxysSo/0y9D2K42SaB8Y/%2BaD2K42SaB8Y/%2BahU%2BWOZsHcrxysooUeND
site:                ‰ºòÈÖ∑ (Youku)
title:               Ë¥ßÂ∏ÅÊàò‰∫âÔºà‰∏ãÔºâÔºöÂ§¥Âè∑Áé©ÂÆ∂
stream:
    - format:        mp4hd3v2
      container:     mp4
      video-profile: 1080P
      size:          567.0 MiB (594580265 bytes)
      m3u8_url:      http://pl-ali.youku.com/playlist/m3u8?vid=XMzU5NjkxNTM1Ng%3D%3D&type=hd3&ups_client_netip=18b9426a&utid=IYV%2BE0QbrVcCARi5Qmqd6hjL&ccode=0510&psid=b336626aa0973701c5248c01cfa2f3f8&duration=3071&expire=18000&drm_type=1&drm_device=7&ups_ts=1526175526&onOff=0&encr=0&ups_key=bcfaf87bad590f7a77bfa4ac5ffc8793
    # download-with: you-get --format=mp4hd3v2 [URL]
```",Change value of ccode from '0502' to '0510'
246,"bpo-37759:  Significant edits to Whatsnew 3.8
Work in progress.  Please add suggestions here as you find them.


https://bugs.python.org/issue37759

",
247,"Updated with virtualenv for Atom.io
","Update README.md: added virtualenv-api, virtual, and linter-flake8 to the list of awesome Python frameworks, libraries and software."
248,"Fix mypy error in web_programming/reddit.py
### Describe your change:

In recent PRs such as #9160, mypy is failing in the pre-commit check because of the following error:
```
web_programming/reddit.py:36: error: Missing named argument ""response"" for ""HTTPError""  [call-arg]
```
This happens even though contributors aren't modifying `web_programming/reddit.py`. I'm not sure why this mypy error is appearing now, but this appears to fix it.

* [ ] Add an algorithm?
* [x] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [ ] All new Python files are placed inside an existing directory.
* [ ] All filenames are in all lowercase characters with no spaces or dashes.
* [ ] All functions and variable names follow Python naming conventions.
* [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [ ] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.
* [ ] If this pull request resolves one or more open issues then the description above includes the issue number(s) with a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue): ""Fixes #ISSUE-NUMBER"".
","Add three new algorithms to the list of algorithms in the DIRECTORY.md file: Smith Waterman, Subset Generation, and Sum Of Subset. Also, modify the Reddit.py file to raise an HTTPError with the response object when the response status code is 429."
249,"Fix various typos
Fixes #169.

Note that one of the corrections was incorrect: ""ue""->""due"". I opened [codespell#1413](https://github.com/codespell-project/codespell/issues/1413) to track that issue.",### DIFF:Changes to the cheat.sh project's documentation and code.* Updated the CONTRIBUTING.md file to include new steps for contributing to the project.* Modified the README.md file to include information about the `cht.sh` client's configuration file and how to override its location.* Updated the lib/adapter/adapter.py file to include a new method for getting the local repository location.* Modified the lib/adapter/cmd.py file to include a new command for creating a new cheat sheet in an upstream repository.* Updated the lib/adapter/git_adapter.py file to include a new method for getting the list of files in a cheat sheet directory.* Modified the lib/config.py file to include new configuration parameters for the `CHEATSH_PATH_CONFIG` environment variable.* Updated the lib/fetch.py file to include new methods for fetching and updating cheat sheets.* Modified the share/cht.sh.txt file to include new installation instructions.* Updated the share/intro.txt file to include new information about using the cheat sheets server.* Modified the tests/results/16 file to include new test results.* Updated the tests/results/25 file to include new test results.* Modified the tests/results/6 file to include new test results.* Updated the tests/results/8 file to include new test results.
250,"Fix command-line formatting
Fix formatting for a bash command in ``docs/patterns/packages.rst``.",Add code block to the documentation explaining how to run the application with the `flask` command.
251,"Fix-up standalone binary docs.
",Update README.md and methods.yml to reflect correct installation process for single binary executables.
252,"More doc updates
",Added documentation for authentication and quickstart guides in the Requests library.
253,"Fix AutoModel tests
Auto model tests were not kept up to date. This patches the following two tests:
```
FAILED tests/test_modeling_auto.py::AutoModelTest::test_model_from_pretrained
FAILED tests/test_modeling_common.py::ModelUtilsTest::test_model_from_pretrained
```",Update test cases for model loading and configuration.
254,"Fix llama2 provider, Update TOC in readme
",Update README.md and docker-compose.yml files to reflect changes in the project.
255,"Allow disabling docs UIs by disabling OpenAPI
:sparkles: Allow disabling docs UIs by disabling OpenAPI and add docs and tests for it.",Conditional OpenAPI configuration and disabling OpenAPI schema and documentation user interfaces.
256,"fix(suspect-spans): All columns must be explicitly specified
Previously, some columns were still being added automatically with a default of
adding every column. This change ensures that all the desired columns must be
explicitly specified or it won't be returned.",Add fields to query and test for percentileArray and sumArray aggregations.
257,"Extract port number from authority before IDNA decode 
#### Description

Before calling `.decode(""idna"")` first extract the port number. Attempt to fix issue #4409

If the port is present it can lead to a UnicodeError exception causing net.http.url.parse_authority to incorrectly return False.

The existing test passes because `.decode(""idna"")` accepts a port number some of the time:

```
>>> b""xn--aaa-pla.example:80"".decode(""idna"")
'√§aaa.example:80'
```

However this results in a UnicodeError exception:

```
>>> b""xn--r8jz45g.xn--zckzah:80"".decode(""idna"")
Traceback (most recent call last):
  File ""/usr/lib/python3.6/encodings/punycode.py"", line 207, in decode
    res = punycode_decode(input, errors)
  File ""/usr/lib/python3.6/encodings/punycode.py"", line 194, in punycode_decode
    return insertion_sort(base, extended, errors)
  File ""/usr/lib/python3.6/encodings/punycode.py"", line 165, in insertion_sort
    bias, errors)
  File ""/usr/lib/python3.6/encodings/punycode.py"", line 146, in decode_generalized_number
    % extended[extpos])
UnicodeError: Invalid extended code point '8'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/encodings/idna.py"", line 214, in decode
    result.append(ToUnicode(label))
  File ""/usr/lib/python3.6/encodings/idna.py"", line 131, in ToUnicode
    result = label1.decode(""punycode"")
UnicodeError: decoding with 'punycode' codec failed (UnicodeError: Invalid extended code point '8')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File """", line 1, in 
UnicodeError: decoding with 'idna' codec failed (UnicodeError: decoding with 'punycode' codec failed (UnicodeError: Invalid extended code point '8'))
```

#### Checklist

 - [x] I have updated tests where applicable.
 - [x] I have added an entry to the CHANGELOG.
",Fixed IDNA host 'Bad HTTP request line' error and added support for internationalized domain names in URLs.
258,"Small styling tweaks
Originally from #513

- button hover style
- ensure context menu is always above everything
","Update styles for buttons in web/style.css: added hover effects and cursor pointer for buttons, and added a new class ""litecontextmenu"" with z-index."
259,"properly handling stop value when it can be a list or a str




## Why are these changes needed?

The OpenAI `stop` parameter can be either a string or a list of strings.  Right now the vllm worker assumes its always a string and causes some fatal error deep in the vllm engine when a list is passed.  This properly handles both str and list cases.

## Related issue number (if applicable)



## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [x] I've made sure the relevant tests are passing (if applicable).

Manually testing this and comparing against vllm's openai api server.  With this fix I now see comparable speeds.",Update handling of `stop_str` to correctly handle both string and list inputs.
260,"Fixed `from utils import create_email, check_email` in `gpt4free/uses‚Ä¶
‚Ä¶less/__init__.py`

### **Before**:
```
from utils import create_email, check_email
```


### **Now**:
```
from .utils import create_email, check_email
```


### **Change**:
`Added dot before module name`",Update import statement in `__init__.py` to use relative import from `utils` module.
261,"Count the tokens/second when using --debug 
## Why are these changes needed?
We would like to count the tokens/second when debugging a served model on the cli.
Result using `llama-7b`:



## Related issue number (if applicable)
Closes #1268 

## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [ ] I've made sure the relevant tests are passing (if applicable).
",Add import of time module and calculate tokens per second in chat loop.
262,"Invalud argument format
Other functions other than that function have spaces between argument",Update the `step` method to include `u` as a parameter.
263,"primer: Add `--no-diff` option
- Allow runs with no code diff output
- This is handy for reducing output to see which file is erroring

Test:
- Edit config for 'channels' to expect no changes and run with `--no-diff` and see no diff output
- Fix unittests to handle new argument",Add option to disable showing source file changes in black output.
264,"Add fail2ban filter.
A simple filter for fail2ban, which is a powerful log monitoring tool.

Please put the shadowsocks.conf into your filter.d directory, and
using `filter = shadowsocks` to use the filter.
",Add new file `shadowsocks.conf` in `utils/fail2ban` directory with basic configuration for fail2ban.
265,"Api Key Leaks: Add Trivy to tools section
This will add [Trivy](https://github.com/aquasecurity/trivy) to the tools section of the topic `Api Key Leaks`. This is currently my go-to tool to scan docker images or repositories for secrets and therefore should be mentioned. As I could not see it mentioned somewhere, I picked this topic as it seemed to fit best. But it might also fit for other topics such as scanning for vulnerable dependencies (CVE) used in a projects or weaknesses in regards of SBOM/IaC. If you see a better fit for Trivy at other topics or might want to add it redundantly multiple times I guess we could add it to this MR.
Let me know what you think. :)

(Also changed the phrasing for two other tools as their main repository uses the upper/lowecase as changed which should not require a separate MR)",Update API Key Leaks/README.md to include new tools for finding and exploiting API key leaks.
266,"Revert/Fix vicuna prompt




## Why are these changes needed?

Fixing the vicuna prompt after the airoboros model PR changed the prompt



## Related issue number (if applicable)

https://github.com/lm-sys/FastChat/pull/1877


## Checks

- [ ] I've run `format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed.
- [ ] I've made sure the relevant tests are passing (if applicable).
",Update conversation template description to be polite.
267,"New `LetterBox(size)` `CenterCrop(size)`, `ToTensor()` transforms (#9213)
YOLOv5 LetterBox class for image preprocessing, i.e. T.Compose([T.ToTensor(), LetterBox(size)])

Signed-off-by: Glenn Jocher 

@AyushExel 


## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Enhancements to image preprocessing in YOLOv5 with new transformation classes and modified dataloader behavior.

### üìä Key Changes
- üìù Introduced `LetterBox`, `CenterCrop`, and `ToTensor` custom preprocessing classes in `augmentations.py`.
- üëÅÔ∏è‚Äçüó®Ô∏è Modified the behavior of default transformations to use new custom classes instead of torchvision transforms.
- üîÑ Updated `dataloaders.py` to incorporate the new transformation classes and streamlined image color conversion when transforms are applied.
- üóÇÔ∏è Cache improvements for storing and retrieving processed images, reducing I/O operations and potentially speeding up training.

### üéØ Purpose & Impact
- üí° **Purpose**: To provide YOLOv5 with more flexible image preprocessing options, specifically tailored to the YOLO architecture, and improve performance through better caching mechanisms.
- üöÄ **Impact**: Expect increased efficiency in data loading, a potential boost in training speed, and easier customization of image preprocessing steps for YOLOv5 users.","Changes to the image preprocessing and augmentation pipeline in the YOLOv5 model. The changes include the addition of new classes `LetterBox`, `CenterCrop`, and `ToTensor` to the `albumentations` library, which are used to preprocess images before training the model. The `LetterBox` class is used to pad images to a fixed size, the `CenterCrop` class is used to crop images to a fixed size, and the `ToTensor` class is used to convert images to tensors. The changes also include the modification of the `Dataloader` class to use the new classes."
268,"F.2 corrected auto to void
Corrected an auto return type to void",Change function `print` from taking `auto& output` and returning `void` to taking `auto& output` and returning `auto&`.
269,"[youku] Fix list extraction.(close #15065)
### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---",Added two new URLs to the YoukuShowIE extractor for handling ongoing playlists and playlists without data-id values.
270,"Add support for Flickr
Sample videos:
- https://www.flickr.com/photos/brianpittman/22076440276/in/pool-timelapsevideos/
- https://www.flickr.com/photos/130585389@N04/21085642933/in/pool-timelapsevideos/



[](https://reviewable.io/reviews/soimort/you-get/697)


",Added support for downloading videos from Flickr.com.
271,"Add IP Fast API to Development


- [x] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not have more than 100 characters
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update README.md: added new API documentation for ip-fast.com.
272,"PERF: Check identity first before comparing the objects
 - [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

This is a regeression in #41952. Checking the identity first avoids long comparison of e.g. tuples - checking the id first is also how `PyObject_RichCompareBool(a, b, Py_EQ)` works.

Here are results from asv:

```
       before           after         ratio
     [1ff6970c]       [b8285515]
-      11.0¬±0.1ms      7.09¬±0.06ms     0.64  algos.isin.IsInWithLongTupples.time_isin

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.

```
",Update to the `isin` method in `isin.py` to include a test case with long tuples.
273,"Gracefully handle user does not exist in Inference
Much less hacky hack than the one we had before, the inference log should also be much cleaner now.",Add user existence check before creating a chat.
274,"update deepspeed to run with the most recent triton 2.1.0
fixes to make deeepspeed to run with the most recent triton, [2.1.0](https://pypi.org/project/triton/2.1.0/)
Specifically, the changes are to reflect change in triton 2.1 that tl.libdevice and triton.testing.allclose are no longer available.",Change the approximation of erf function in gelu to use math.erf instead of libdevice.erf.
275,"‚¨ÜÔ∏è Upgrade version of Starlette to `>= 0.36.3`
‚¨ÜÔ∏è Upgrade version of Starlette to `>= 0.36.3`","Update dependency ""starlette"" from version "">=0.35.0,<0.36.0"" to "">=0.36.3,<0.37.0""."
276,"Fix S3 S3_SKIP_SIGNATURE_VALIDATION
Due to changes in https://github.com/localstack/localstack/commit/3fe01cf6699a4fb71153b582715b4ec21560e4e1 `S3_SKIP_SIGNATURE_VALIDATION` flag is not working anymore, here is the proposed fix

**Please refer to the contribution guidelines in the README when submitting PRs.**
",Add a check for signature validity after calculating it.
277,"[AIP-49] Completes the Breeze OTel integration and adds a banner to the UI 
Completes the Breeze OpenTelemetry integration and adds a banner to the UI indicating it is currently experimental.

(Final?) PR to add some groundwork to allow OTel support.  Follow-up to https://github.com/apache/airflow/pull/29521 

After getting a working working PoC, the Breeze integration needed a tweak to work correctly.  Changes have been tested against a working* proof-of-concept which can be found [here](https://github.com/ferruzzi/airflow/tree/ferruzzi/otel/poc) for now if anyone wants to try them out.

*So far only a subset of metrics are working in that POC, but the ones that ARE working are working end to end.

Major changes:
- Corrects the OpenTelemetry Collector port to 8889 instead of 8888
- Adds OTel values to the Airflow Config 
- Adds the required new Environment Variables tot he integration's docker-compose file to set those config values 
- If Otel is enabled, adds a banner to the UI showing that OTel support is currently experimental 
- Edits to scripts/ci/docker-compose/otel-collector-config.yml to get it working as expected and trim some fat

Minor Changes:
- Rephrasing some docs
- Cleans up some unnecessary port forwarding in the integration's docker-compose file
- Cleans up some extensions in the OTel collector config file that ended up being unnecessary","Here is the description of the pull request:**Changes to OpenTelemetry metrics**This pull request updates the OpenTelemetry metrics configuration in Airflow. The changes include:* Enabling OpenTelemetry metrics by default* Configuring the OpenTelemetry Collector to listen on port 28889* Configuring Prometheus to scrape metrics from the OpenTelemetry Collector* Updating the Grafana dashboard to display OpenTelemetry metrics* Adding support for OpenTelemetry metrics in the Airflow web interface* Updating the documentation to reflect the changes**Other changes*** Updated the `CONTRIBUTING.rst` file to include information about OpenTelemetry metrics* Updated the `INSTALL` file to include instructions for installing OpenTelemetry* Updated the `config_templates` directory to include OpenTelemetry metrics configuration* Updated the `docs` directory to include information about OpenTelemetry metrics* Updated the `scripts` directory to include scripts for testing OpenTelemetry metrics* Updated the `setup.py` file to include OpenTelemetry as a dependency**Impact**This pull request updates the OpenTelemetry metrics configuration in Airflow, making it easier to use OpenTelemetry metrics in Airflow. The changes also update the documentation and other related files to reflect the changes."
278,"MAINT: Remove combineAdd and combineMult
Deprecated in 0.17.0.

xref #10735",Deprecation of combineAdd and combineMult methods in pandas DataFrame.
279,"Pass frameskip and repeat_action_probability to AtariEnv EzPickle
AtariEnv currently does not pass `frameskip` or `repeat_action_probability` to the EzPickle constructor. This leads to environments restored from pickle not being initialised correctly:

```
import gym, pickle
env = gym.make('PongNoFrameskip-v4')
print(env.unwrapped.frameskip)
```
```
1
```
```
env2 = pickle.loads(pickle.dumps(env))
print(env2.unwrapped.frameskip)
```
```
(2, 5)
```

This pull request fixes it.",Add `frameskip` and `repeat_action_probability` parameters to the `__init__` method.
280,"Skoda: add FW versions for Karoq 2021
route: 2c4292a5cd10536c|2022-07-24--09-44-25
",Update the range of supported ≈†koda Karoq models to 2019-2021.
281,"Update profile image on login
Fixes #2196 (not only for discord, but all providers)

Instead of doing the big mess of having to refresh images in the background, we update the images when the user signs in, from the profile we get from the provider.

This has the side effect that if you have 2 different images on 2 different login providers, your image will always change depending on the last provider you logged in with, but I think this is ok.

Also, consolidate all of the updates that are applied to the user to one database call.


",### DIFF:Update user menu avatar image to use `session.user.image` instead of `session.user.image!`. Remove `discord_avatar_refresh` file and update `auth` API to use `session.user.image` instead of `token.picture`.
282,"[MRG+1] Disable SpiderState extension if no JOBDIR set
",Add check for spider state configuration and handle NotConfigured exception.
283,"Add Blitapp


- [ ] My submission is formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [ ] My addition is ordered alphabetically
- [ ] My submission has a useful description
- [ ] The description does not end with punctuation
- [ ] Each table column is padded with one space on either side
- [ ] I have searched the repository for any relevant issues or pull requests
- [ ] Any category I am creating has the minimum requirement of 3 items
- [ ] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update to the README.md file: added a new API documentation entry for Blitapp.
284,"add lang filter option to message endpoints
# add lang filter option to message endpoints
Fixes issues #896

As descriped in the issues, I added a lang filter option to:
- `api/v1/messages`
- `api/v1/messages/cursor`
- `api/v1/users/{user-id}/messages`
- `api/v1/users/{user-id}/messages/cursor`

I saw that `query_messages_ordered_by_created_date()` is also used in `/api/v1/frontend_users/{auth_method}/{username}/messages` and ``/api/v1/frontend_users/{auth_method}/{username}/messages/cursor`. However, I did not add the lang filter to this endpoint since the issues did not specify this endpoint. So, I was unsure if this was desired or not. Let me know if you also want it there, it is an easy fix.",Add support for filtering messages by language.
285,"Keep image extension for xkcd comics
It might be best to just keep the file extension, to prevent confusion. Especially on Windows where how the file is opened depends on the file extension. ",Remove the last 4 characters from comic_name.
286,"Fix: --sd_model in ""Prompts from file or textbox"" script is not working
## Description
Fixes bug #8079 
The Features page lists ""sd_model"" as a valid parameter for the ""Prompts from file or textbox"" script, but the implementation was never finished.
This patch enables the use of the ""sd_model"" parameter as originally intended.

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] All tests passed, but my code was not included in the [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
- [x] The code has been manually tested, both for valid and invalid input. No issues were observed.",Update import statement to include `sd_models` and add a new function `process_model_tag` to get the name of the closest checkpoint match.
287,"add discord reader
","### DIFF:This pull request adds support for reading Discord channels and loading their content into a list of documents. The `DiscordReader` class is added to the `gpt_index` package, which allows users to read conversations from channels. The reader uses the `discord.py` library to interact with the Discord API and retrieve messages. The `load_data` method of the `DiscordReader` class is used to read a list of channels and load their content into a list of documents. The documents contain the channel content and extra information about the channel."
288,"Bypass shape inference in deconv2d and use the output shape provided by the user
Following the discussions in #3540 and #3824 this pull request removes the shape inference part of the `Deconv2d` layer and use the output shape provided by the user.

Because the user must specify an output shape and the shape inference introduced does not work in all cases, the code is more concise and works in cases where the user wants to introduce operations where a precise output shape is needed.

@yaringal is this reasonnable?
",### DIFF:Changes to the Variational Autoencoder (VAE) model:* Increased the number of convolutional filters from 32 to 64 in the `variational_autoencoder_deconv.py` file.* Changed the batch size from 16 to 100.* Modified the architecture of the decoder to include more convolutional layers and upsampling layers.* Added a new layer `decoder_mean_squash` to squash the output of the decoder.* Updated the `vae_loss` function to use the new architecture.* Modified the `generator` model to use the new architecture.* Updated the `Deconvolution2D` layer to use the correct output shape.These changes aim to improve the performance of the VAE model by increasing the capacity of the model and using a more complex architecture.
289,"fix #12492
",Add conditional logic to set console color when writing to console in log method.
290,"COMPAT: Support fastparquet 0.7.1
- [ ] closes #42588
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Also adds support for ``use_nullable_dtypes`` keyword. Technically, its not our enhancement its fastparquet's, so I think can backport. IMO, users shouldn't have to wait 6 months for 1.4 to get this new feature.
",Change the version constraint for fastparquet to allow version 0.7.1 and above.
291,"Fix issue #3766.
‰πãÂâçÁöÑ‰∏§‰∏™Ê£ÄÊµãIPv6ÁöÑÁΩëÂùÄÊó†Ê≥ïÊ≠£Â∏∏ÊâìÂºÄÔºåÈÅÇÊõøÊç¢‰∏∫Ôºö

- ÂÖ≠Áª¥Á©∫Èó¥: `bt.neu6.edu.cn`
- DragonLabÂÆûÈ™åÂÆ§: `www.cernet2.net`

ÊõøÊç¢ÁöÑÁΩëÂùÄÊª°Ë∂≥Êù°‰ª∂: DNSËß£ÊûêÂêé‰ªÖËøîÂõûIPv6Âú∞ÂùÄ.","Change the list of IPv6 hosts in the `check_ipv6` function from [""www.6rank.edu.cn"", ""v6.testmyipv6.com"", ] to [""bt.neu6.edu.cn"", ""v6.ipv6-test.com"", ""ipv6.test-ipv6.jp""]."
292,"chore: update aifs
### Describe the changes you have made:

aifs APIs had changes and added capabilities, so bumping to latest: `poetry add aifs@latest`

### Reference any relevant issues (e.g. ""Fixes #000""):

### Pre-Submission Checklist (optional but appreciated):

- [ ] I have included relevant documentation updates (stored in /docs)
- [x] I have read `docs/CONTRIBUTING.md`
- [x] I have read `docs/ROADMAP.md`

### OS Tests (optional but appreciated):

- [ ] Tested on Windows
- [x] Tested on MacOS
- [ ] Tested on Linux
",Update aifs package from version 0.0.5 to 0.0.7.
293,"[CI/cleanup/restrict_strip_Marks] 

Boilerplate: own code/#36238, bug fix/improvement
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/) except for code from PR #32638 for which this or the below was already asserted 
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature



---

### Description of your *pull request* and other information

1. Resubmit youtube consent fix with better log message
2. Ensure _apt_ is updated for Ubuntu runners
3. Fix CI not selecting correct default settings for `push` and `pull_request` events
4. Add 3.12 to standard CPythons and generalise pre-release build steps for a future beta (3.next)
5. Improve filename handling for `--restrict-filenames`: ignore any character in Unicode categories Mark and Other instead of mapping it to `_`.",Update CI workflow to support Python beta version.
294,"Fixes #31441 -- Fixed admin TabularInline problems
Fixes the first two issues of [Ticket 31441](https://code.djangoproject.com/ticket/31441#comment:3).",### DIFF:Changes were made to the Django admin's inline formsets to improve the display of errors. The changes include:* Adding a red border around form fields with errors in stacked and tabular inlines.* Showing/hiding inline formsets based on the presence of errors.* Updating the JavaScript code to handle the new inline formset behavior.These changes aim to provide a more user-friendly experience when working with inline formsets in the Django admin.
295,"Update README.md
Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [x ] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x ] Your additions are ordered alphabetically
- [ x] Your submission has a useful description
- [ x] The description does not end with punctuation
- [ x] Each table column should be padded with one space on either side
- [x ] You have searched the repository for any relevant issues or pull requests
- [x ] Any category you are creating has the minimum requirement of 3 items
- [x ] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Update to the README.md file: added a new API source, Financial Modeling Prep, with a link to its website."
296,"session_data.get_url -> server_util.get_url
`session_data.py` contains a function, `get_url()`, that has nothing to do with the SessionData class. This PR moves it to `server_util.py`.","### DIFF:Changes were made to the Streamlit library to improve the functionality of the `get_url` function in the `server_util` module. The function now correctly generates the URL for the Streamlit server, taking into account the development mode and the browser server port. Additionally, the `get_url` function was moved from the `session_data` module to the `server_util` module."
297,"Format azure_rm_snapshot document
##### SUMMARY

Format azure_rm_snapshot document.


##### ISSUE TYPE

- Docs Pull Request

##### COMPONENT NAME

azure_rm_snapshot
##### ADDITIONAL INFORMATION




```paste below

```
",Update Azure Snapshot module to fix documentation issues.
298,"[chatgpt] fix lora save bug
## üìå Checklist before creating the PR

- [ ] I have created an issue for this PR for traceability
- [ ] The title follows the standard format: `[doc/gemini/tensor/...]: A concise description`
- [ ] I have added relevant tags if possible for us to better distinguish different PRs


## üö® Issue number

> Link this PR to your issue with words like fixed to automatically close the linked issue upon merge
>
> e.g. `fixed #1234`, `closed #1234`, `resolved #1234`



## üìù What does this PR do?

> Summarize your work here.
> if you have any plots/diagrams/screenshots/tables, please attach them here.
fix the bug that we do not merge lora weight before save ckpt;
fix the bug that lora_eval has conflict with gemini 


## üí• Checklist before requesting a review

- [ ] I have linked my PR to an issue ([instruction](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue))
- [ ] My issue clearly describes the problem/feature/proposal, with diagrams/charts/table/code if possible
- [ ] I have performed a self-review of my code
- [ ] I have added thorough tests.
- [ ] I have added docstrings for all the functions/methods I implemented

## ‚≠êÔ∏è Do you enjoy contributing to Colossal-AI?

- [ ] üåù Yes, I do.
- [ ] üåö No, I don't.

Tell us more if you don't enjoy contributing to Colossal-AI.
","Changes to the Lora model and its usage in the trainer strategies. The changes include adding and removing attributes and methods in the Lora model, and modifying the save_model method in the trainer strategies to merge Lora weights into weights and evaluate the Lora model."
299,"Bump the github-actions group with 2 updates
Bumps the github-actions group with 2 updates: [install-pinned/ruff](https://github.com/install-pinned/ruff) and [actions/cache](https://github.com/actions/cache).

Updates `install-pinned/ruff` from 4ec70113ad8b512f13948b8d9855ac59675535d2 to fe472defb50a6a2c00ea3a3982534e86e69991e8

Commits

fe472de update README.md (ruff 0.1.15)
cf75457 update pins (ruff 0.1.15)
a5367ea update README.md (ruff 0.1.14)
38ee0f4 update pins (ruff 0.1.14)
a8e213b update README.md (ruff 0.1.13)
21180b3 update pins (ruff 0.1.13)
58b3f74 update README.md (ruff 0.1.12)
4234903 update pins (ruff 0.1.12)
8c12fc8 update README.md (ruff 0.1.11)
dbde053 update pins (ruff 0.1.11)
See full diff in compare view




Updates `actions/cache` from 3 to 4

Release notes
Sourced from actions/cache's releases.

v4.0.0
What's Changed

Update action to node20 by @‚Äãtakost in actions/cache#1284
feat: save-always flag by @‚Äãto-s in actions/cache#1242

New Contributors

@‚Äãtakost made their first contribution in actions/cache#1284
@‚Äãto-s made their first contribution in actions/cache#1242

Full Changelog: https://github.com/actions/cache/compare/v3...v4.0.0
v3.3.3
What's Changed

Cache v3.3.3 by @‚Äãrobherley in actions/cache#1302

New Contributors

@‚Äãrobherley made their first contribution in actions/cache#1302

Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.3
v3.3.2
What's Changed

Fixed readme with new segment timeout values by @‚Äãkotewar in actions/cache#1133
Readme fixes by @‚Äãkotewar in actions/cache#1134
Updated description of the lookup-only input for main action by @‚Äãkotewar in actions/cache#1130
Change two new actions mention as quoted text by @‚Äãbishal-pdMSFT in actions/cache#1131
Update Cross-OS Caching tips by @‚Äãpdotl in actions/cache#1122
Bazel example (Take #2Ô∏è‚É£) by @‚Äãvorburger in actions/cache#1132
Remove actions to add new PRs and issues to a project board by @‚Äãjorendorff in actions/cache#1187
Consume latest toolkit and fix dangling promise bug by @‚Äãchkimes in actions/cache#1217
Bump action version to 3.3.2 by @‚Äãbethanyj28 in actions/cache#1236

New Contributors

@‚Äãvorburger made their first contribution in actions/cache#1132
@‚Äãjorendorff made their first contribution in actions/cache#1187
@‚Äãchkimes made their first contribution in actions/cache#1217
@‚Äãbethanyj28 made their first contribution in actions/cache#1236

Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.2
v3.3.1
What's Changed

Reduced download segment size to 128 MB and timeout to 10 minutes by @‚Äãkotewar in actions/cache#1129

Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.1
v3.3.0
What's Changed

Bug: Permission is missing in cache delete example by @‚Äãkotokaze in actions/cache#1123



... (truncated)


Changelog
Sourced from actions/cache's changelog.

Releases
3.0.0

Updated minimum runner version support from node 12 -&gt; node 16

3.0.1

Added support for caching from GHES 3.5.
Fixed download issue for files &gt; 2GB during restore.

3.0.2

Added support for dynamic cache size cap on GHES.

3.0.3

Fixed avoiding empty cache save when no files are available for caching. (issue)

3.0.4

Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntu-latest. (issue)

3.0.5

Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)

3.0.6

Fixed #809 - zstd -d: no such file or directory error
Fixed #833 - cache doesn't work with github workspace directory

3.0.7

Fixed #810 - download stuck issue. A new timeout is introduced in the download process to abort the download if it gets stuck and doesn't finish within an hour.

3.0.8

Fix zstd not working for windows on gnu tar in issues #888 and #891.
Allowing users to provide a custom timeout as input for aborting download of a cache segment using an environment variable SEGMENT_DOWNLOAD_TIMEOUT_MINS. Default is 60 minutes.

3.0.9

Enhanced the warning message for cache unavailablity in case of GHES.

3.0.10

Fix a bug with sorting inputs.
Update definition for restore-keys in README.md



... (truncated)


Commits

13aacd8 Merge pull request #1242 from to-s/main
53b35c5 Merge branch 'main' into main
65b8989 Merge pull request #1284 from takost/update-to-node-20
d0be34d Fix dist
66cf064 Merge branch 'main' into update-to-node-20
1326563 Merge branch 'main' into main
e718767 Fix format
0122982 Apply workaround for earlyExit
3185ecf Update &quot;only-&quot; actions to node20
25618a0 Bump version
Additional commits viewable in compare view





Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show  ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)
- `@dependabot ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)
- `@dependabot ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)
- `@dependabot unignore ` will remove all of the ignore conditions of the specified dependency
- `@dependabot unignore  ` will remove the ignore condition of the specified dependency and ignore conditions


",Update actions/checkout and actions/cache to v4.
300,"add workflow to execute unit tests
","Add a new GitHub Actions workflow file `.github/workflows/pytest-action.yml` to automate testing with Pytest. The workflow is triggered on push and pull request events to the `main` branch. It installs dependencies, runs tests using Pytest, and uses Python 3.10.11. Additionally, update the `requirements.txt` file to include the `pytest` package."
301,"üåê Add Chinese translation for Tutorial - Security - OAuth2 with Password (and hashing), Bearer with JWT tokens
@maoyibo @yanever @RunningIkkyu Please take a look.","### OAuth2 with JWTThis tutorial introduces the concept of JSON Web Tokens (JWT) and how to use them in a FastAPI application. It covers the basics of JWT, including how to generate and verify tokens, and how to use them to authenticate users."
302,"Adding Tellonym.me
Fully tested and working","Add new ""Tellonym.me"" object to data.json with additional properties."
303,"[CGTN] Add new extractor
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Explanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.
",Add a new extractor for CGTN news videos.
304,"[parallelism docs] Megatron-Deepspeed info
This PR adds:
- BigScience fork of Megatron-Deepspeed
- Super important paper on Megatron-Deepspeed

@sgugger ","Update documentation for parallelism: added Megatron-Deepspeed from BigScience as a new implementation, and added a new important paper."
305,"Update install readme


<!--- ***** Template: Car bug fix *****

**Description** [](A description of the bug and the fix. Also link any relevant issues.)

**Verification** [](Explain how you tested this bug fix.)

**Route**
Route: [a route with the bug fix]

-->

<!--- ***** Template: Bug fix *****

**Description** [](A description of the bug and the fix. Also link any relevant issues.)

**Verification** [](Explain how you tested this bug fix.)

-->

<!--- ***** Template: Car port *****

**Checklist**
- [ ] added to README
- [ ] test route added to [test_routes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/test/test_routes.py)
- [ ] route with openpilot:
- [ ] route with stock system:

-->

<!--- ***** Template: Refactor *****

**Description** [](A description of the refactor, including the goals it accomplishes.)

**Verification** [](Explain how you tested the refactor for regressions.)

-->
","Update README.md to clarify that openpilot can run on a PC without special hardware or a car, and provide more information on tools for plotting logs, replaying drives, and watching camera streams."
306,"Remove ancient .gitignores
I found these files while working on other stuff. `certbot/.gitignore` comes from [a commit from 2012](https://github.com/certbot/certbot/commit/d18c7f6eee94b1d5288140e22166a740bd9514cd) and there are no `.crt` files under `certbot/`. `certbot-dns-route53/.gitignore` comes from https://github.com/certbot/certbot/pull/4692 when we took over this plugin and it was also not ignoring any files.",Delete.gitignore files in certbot-dns-route53 and certbot directories.
307,"Add new option to generate subtitles by a specific number of words
*Updated according jonwook code review

Added a new word option called **--max_words_per_line** that will generate subtitles setting a maximum limit of words per segment. This could sound similar to **--max_line_width** option, but the results are more pleasent for readers IMHO. Here a couple of comparisons using .SRT files:

![max_word_per_line](https://github.com/openai/whisper/assets/8297398/3ccd320f-8a4d-4502-ba05-4553125d0f71)
Notice that **--max_words_per_line** works as an upper bound of words, but still it will respect the segments in the way that end of sentences can have less words if the remaining number of words in a segment is lower than the **max_words_per_line** value.
i.e. _Segment = [word1, word2, word3, word4, word5] and max_words_per_line = 3
=>Result = [word1, word2, word3] and [word4, word5]_
This is not the behaviour we can see using **--max_line_width** that can leave bigger gaps of time when joining end and beginning of segments:

![comparison](https://github.com/openai/whisper/assets/8297398/bdd209c0-42bd-4dda-b136-487cde31b76e)

Subtitles generated with **--max_words_per_line** look similar of what we can see in Shorts, Reels and other short duration videos.

This is my first contribution, so feel free of changing/comment/improve anything.

### Additional notes

- The use  of **--max_line_width** will disable the effects of **--max_words_per_line**.
- Manually tested using Python and cli and checked results in .srt and .vtt files (.txt. and .tsv files won't be affected).
","### DIFF:Changes to the Whisper transcription library:* Added a new argument `--max_words_per_line` to the `cli` function, which allows users to specify the maximum number of words per line in the output.* Modified the `write_result` method in the `ResultWriter` class to handle the new `--max_words_per_line` argument.* Updated the `iterate_result` method in the `SubtitlesWriter` class to take into account the new `--max_words_per_line` argument.* Added a new `max_words_per_line` parameter to the `iterate_result` method in the `SubtitlesWriter` class.* Updated the `WriteVTT` and `WriteSRT` classes to use the new `max_words_per_line` parameter.* Added a new `max_words_per_line` parameter to the `write_result` method in the `WriteVTT` and `WriteSRT` classes.* Updated the `WriteTSV` and `WriteJSON` classes to use the new `max_words_per_line` parameter.* Added a new `max_words_per_line` parameter to the `write_result` method in the `WriteTSV` and `WriteJSON` classes."
308,"CP.44 improve example and add more accurate discussion
",Improved documentation for C++ Core Guidelines: added example to illustrate the issue with unnamed local objects.
309,"Up version to 1.20.0
",Update package.json and setup.py to reflect new version 1.20.0. Update cache_data_api.py to ignore mypy complaints. Update script_runner.py to compile code.
310,"BUG: Fix aligning a DataFrame with a Series with MultiIndex #46001
When aligning a DataFrame to a Series we are using Series.reindex() to
broadcast the Series data to the new index.  That introduces NaNs when the new
index rows are not identical to the existing ones, which is not the case when
we introduce a new MultiIndex level.

In this patch we use the same approach as for aligning a Series to another
Series. That means that we have to replicate a part of Series._reindex_indexer
as DataFrame does not have it.

- [x] closes #46001 
  and #43321
- [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature
- [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).
- [x] Added an entry in the latest `doc/source/whatsnew/v1.5.0.rst`.
","Here is the description of the pull request:Changes to the `align` method in pandas: The `align` method now correctly handles the alignment of a pandas Series with a pandas DataFrame when the DataFrame has a MultiIndex and the Series has a different index level. This change fixes issues where the alignment process would fail or produce incorrect results. The method now correctly handles cases where the Series index level is not present in the DataFrame, and also correctly handles cases where the Series index level is present in the DataFrame but has different values."
311,"Add validation for header name
Following up on #6083, this refactors our header validation function from 2.11.0 to consider the header name. We'd originally avoided adding header name validation because we wanted to limit the change scope to header splitting with new lines. Since then the standard library has made similar changes to ours and now raises a ValueError in `http.client`. This gives us inconsistent errors depending on which portion of the header you provide a bad value.

```python
>>> requests.get(""https://httpbin.org/get"", headers={"":bad"": ""header""})
Traceback (most recent call last):
  File """", line 1, in 
  File ""/Users/nateprewitt/Work/OpenSource/requests/requests/api.py"", line 73, in get
    return request(""get"", url, params=params, **kwargs)
  File ""/Users/nateprewitt/Work/OpenSource/requests/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/Users/nateprewitt/Work/OpenSource/requests/requests/sessions.py"", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/nateprewitt/Work/OpenSource/requests/requests/sessions.py"", line 701, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/nateprewitt/Work/OpenSource/requests/requests/adapters.py"", line 489, in send
    resp = conn.urlopen(
  File ""/Users/nateprewitt/Work/OpenSource/urllib3/src/urllib3/connectionpool.py"", line 727, in urlopen
    httplib_response = self._make_request(
  File ""/Users/nateprewitt/Work/OpenSource/urllib3/src/urllib3/connectionpool.py"", line 433, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/Users/nateprewitt/Work/OpenSource/urllib3/src/urllib3/connection.py"", line 309, in request
    super().request(method, url, body=body, headers=headers)
  File ""/Users/nateprewitt/.pyenv/versions/3.10.4/lib/python3.10/http/client.py"", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Users/nateprewitt/.pyenv/versions/3.10.4/lib/python3.10/http/client.py"", line 1323, in _send_request
    self.putheader(hdr, value)
  File ""/Users/nateprewitt/Work/OpenSource/urllib3/src/urllib3/connection.py"", line 274, in putheader
    super().putheader(header, *values)
  File ""/Users/nateprewitt/.pyenv/versions/3.10.4/lib/python3.10/http/client.py"", line 1250, in putheader
    raise ValueError('Invalid header name %r' % (header,))
ValueError: Invalid header name b':bad'
```

This PR ports the [validation scheme](https://github.com/python/cpython/blob/6b9122483f1f26afb0c41bd676f9754ffe726e18/Lib/http/client.py#L139) from http.client for header names and ensures we raise an `InvalidHeader` error consistently in all supported versions.","### DIFF:Changes to the Requests library's internal utilities and header validation.The changes include:* Added regular expressions to validate header names and values.* Introduced a `HEADER_VALIDATORS` dictionary to store the regular expressions for validating header names and values.* Modified the `check_header_validity` function to use the `HEADER_VALIDATORS` dictionary.* Added a `_validate_header_part` function to validate individual header parts.* Modified the `get_auth_from_url` function to use the `_validate_header_part` function.* Added tests for header validation, including tests for invalid header values, leading whitespace, and return characters."
312,"Restructure for text parsing. Faster, more stable, more maintainable
#187 ",Change implementation of the A2 OEIS sequence in Python.
313,"fix sqs message group visibility when sending messages



## Motivation

Even after the initial implementation of FIFO message groups in #8238, users have reported odd behavior when using SQS FIFO queues related to message group visibility specifically that consumers are triggered although the message group is expected to be invisible #8724. This also affected lambdas triggered with fifo queues #7036.

This PR fixes the mechanism we were using to reset message group visibility, which was too general. It assumed that every message put into the message group reset its visibility, when really it should just be messages that were previously invisible, not new ones.


## Changes

* message group visibility is no longer reset when new messages are sent to the queue

Fixes
* #8724
* #7036 (tested with #7037)

<!-- The following sections are optional, but can be useful! 

## Testing

Description of how to test the changes

## TODO

What's left to do:

- [ ] ...
- [ ] ...

-->

",Add visibility timeout check for FIFO queues.
314,"bpo-33944: note about the intended use of code in .pth files



https://bugs.python.org/issue33944



Automerge-Triggered-By: @matrixise",Added a note about the intended use of code in.pth files.
315,"TST: nose.SkipTest on RemoteDataErrors in tests for io.data.Options
Prevents tests from failing if data isn't able to be downloaded from Yahoo Finance.
",Fixed Yahoo data download failure in tests.
316,"pencil2: Fix awkward pronoun usage
","### Security Improvements and Tutorial UpdatesThis pull request includes several security-related updates and improvements to the documentation. The main changes are:* An update to the HTTP Basic Auth tutorial to better explain the concept of timing attacks and how to mitigate them using `secrets.compare_digest()`.* An addition to the background tasks tutorial to explain the concept of operations that need to happen after a request, but that the client doesn't have to wait for the operation to complete before receiving the response.* An update to the error handling tutorial to provide a better example of how to handle errors in a FastAPI application.* An addition to the first steps of the security tutorial to explain the concept of FastAPI handling the API and authentication.* An update to the OAuth2 JWT tutorial to explain the concept of signed tokens and how they can be used to verify the authenticity of a user."
317,"bpo-35059: Enhance _PyObject_AssertFailed()
_PyObject_AssertFailed():

* exchange expr and msg arguments
* expr and func arguments can now be NULL


https://bugs.python.org/issue35059

",Change the implementation of _PyObject_AssertFailed to use the msg parameter.
318,"Restructure for text parsing. Faster, more stable, more maintainable
#187 ",Change implementation of the A2 OEIS sequence in Python.
319,"Future proof octal number: 0755 --> 0o755
",Change permission bits of directory creation from 0755 to 0o755.
320,"Bisenet-FP - Improved weights for masker
The following pull-request allows for the use of custom trained weights for the Bisenet-FP Mask.

The dataset of 40k faces has been built from a wide variety of posed and in the wild images. These have been manually annotated over several months to handle:
- many varieties of real obstructions
- difficult/varied conditions + poses
- glasses (mask out frames, keep lenses)
- dark sunglasses (mask out frames + lenses)
- multiple targets within a single extracted image.

This dataset will continue to be grown over time and will be used to create new and more improved maskers. Whilst this solution is by no means perfect, it is vastly improved over any existing masking solutions.

NB: This Pull Request does not include the actual weights file. This will be going to Patreons first for a timed exclusive, but the weights will eventually be made available for use in the main code.

### Glasses + Sunglasses:
![gls](https://user-images.githubusercontent.com/36920800/160387537-1ce15fda-4da8-41c8-9d74-57249b756d4c.gif)

### Ignoring the non-target face: 
![fac](https://user-images.githubusercontent.com/36920800/160387642-6bdd4caa-c7bb-4f4d-860a-8d987498830a.gif)

### Multiple obstruction types:
![obs](https://user-images.githubusercontent.com/36920800/160387649-0ad78edb-4dc0-42c2-b455-68ee8a5322a3.gif)

",Changes to the BiSeNet Face Parsing model:- Added support for 'faceswap' trained weights.- Updated model segment indices for 'faceswap' weights.- Updated model mean and standard deviation for 'faceswap' weights.- Added a new option to include or exclude glasses in the face mask.- Updated the model's ability to handle glasses in the face mask for 'faceswap' weights.- Improved the model's ability to handle dark sunglasses and just frame masking for 'faceswap' weights.
321,"use asf gateway as default
Disables the edge proxy by default in favor of #5243 ",Change the value of LEGACY_EDGE_PROXY from not being set to being set.
322,"Mark config schema as removed in Verisure
<!--
  You are amazing! Thanks for contributing to our project!
  Please, DO NOT DELETE ANY TEXT from this template! (unless instructed).
-->
## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->

The Verisure configuration schema is not deprecated, it is actually removed.
This adjusts the marker for it.

## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [x] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [x] The code change is tested and works locally.
- [x] Local tests pass. **Your PR cannot be merged unless tests pass**
- [x] There is no commented out code in this PR.
- [x] I have followed the [development checklist][dev-checklist]
- [x] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

The integration reached or maintains the following [Integration Quality Scale][quality-scale]:
<!--
  The Integration Quality Scale scores an integration on the code quality
  and user experience. Each level of the quality scale consists of a list
  of requirements. We highly recommend getting your integration scored!
-->

- [x] No score or internal
- [ ] ü•à Silver
- [ ] ü•á Gold
- [ ] üèÜ Platinum

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [x] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html
[quality-scale]: https://developers.home-assistant.io/docs/en/next/integration_quality_scale_index.html
[docs-repository]: https://github.com/home-assistant/home-assistant.io
",Update CONFIG_SCHEMA to use cv.removed instead of cv.deprecated.
323,"ReplicaSets Information
Kubernetes ReplicaSets","Update README.md file with new content related to Kubernetes. The file now includes information about kubectl, pods, deployments, and replica sets."
324,"Fix MPD BaseURL handling when root-relative URL
### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [X] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Hi,

I have come across an MPD manifest that includes a BaseURL attribute set to a root-relative URL. For example:
```
/x/y/
```

The current processing of BaseURL assumes the URL is either absolute (e.g. http://example.com/x/y/z) or relative (x/y/z). Root-relative URLs end up generating an incorrect fragment_base_url, failing to download content.",Update logic for handling relative URLs in `extract_Initialization` function.
325,"revert instructor code and fix a bug in anthropic ds parsing
",Update the load_anthroopic_rlhf function to use the correct class name.
326,"PEP8
",### DIFF:Changes to the Let's Encrypt client codebase.The `Client` class has been updated to include a `server_url` attribute and a `validate_csr_key_cli` method. The `authenticate` method now accepts an `eula` parameter and has been updated to handle the case where the CSR does not contain one of the specified names. The `optimize_config` method now accepts a `redirect` parameter and has been updated to handle the case where the redirect is None. The `interactive_challenge` class has been updated to include a `get_display_string` method and a `perform` method that displays the string sent by the CA. The `main` function has been updated to include a `revoke` option and to call the `authenticate` method with the `eula` parameter.
327,"[Core] Fix test_logging_to_driver


## Why are these changes needed?

Fixes test_logging_to_driver

## Related issue number



## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",Change the assertion of output_lines to err_lines in test_advanced_3.py.
328,"[Gemini] hotfix the unittest bugs
We have to use the original logic to search the chunk size. Otherwise, the unittest will hang.",Change the condition in `is_empty` method to check if the length of `param_visited_order` is greater than 0.
329,"Add runtime requirements
Simply copies `kornia`, `streamlit-drawable-canvas`, and `invisible-watermark` from the conda-style `environment.yaml` to the pip-style `requirements.txt`.

Tested with the following:

```
    git clone https://github.com/Stability-AI/stablediffusion/
    cd stablediffusion
    python -m venv venv
    source venv/bin/activate
    pip install -U pip
    pip install -r requirements.txt
    python scripts/txt2img.py --prompt ""our galaxy itself contains a hundred billion stars"" --ckpt ../768-v-ema.ckpt --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768  --n_samples 1
```
on Ubuntu 22 with Python 3.8, or similar, on a machine with CUDA already installed.
","Update requirements.txt: added kornia, invisible-watermark, and streamlit-drawable-canvas, removed e."
330,"Backport PR #43166 on branch 1.3.x (Pin aiobotocore to get ci passing)
Backport PR #43166: Pin aiobotocore to get ci passing",Add aiobotocore<=1.3.3 as a dependency in multiple files.
331,"Detect downstream deps in tests
This is meant to detect issues such as #919. It makes sure we run tests without any downstream dependencies installed, e.g. `letsencrypt_apache` is not installed when `letsencrypt` is being tested.

It also improves verbosity of the tests, so it's a lot easier to detect problems (especially busy waiting loops) when investigating Travis build logs output.
",Update tox.ini to improve test commands and add verbosity to nosetests.
332,"Fixed error handling in train.py
Fixed the error handling in train.py so it doesn't swallow tracelogs.

This is an improvement to follow Python best practices of raising any exceptions that aren't handled so that tracelogs don't get lost.",Change exception handling in `processThread` method from printing the exception to raising it.
333,"Added `--nsfw` flag in documentation
Added `--nsfw` tag in `Usage` prompt with the description matching the `help` prompt of the parser's --nsfw flag",Add option to include checking of NSFW sites from default list.
334,"Fix `pretty` cyclic reference handling
## Type of changes

- [x] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Checklist

- [x] I've run the latest [black](https://github.com/psf/black) with default args on new code.
- [x] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [x] I've added tests for new code.
- [x] I accept that @willmcgugan may be pedantic in the code review.

## Description

Previously cyclic references were only handled for container and dataclass types, but not namedtuple, attrs, or custom types. This fixes that, and expands the tests to cover these cases.
","### Change handling of cyclic references in pretty printerThe pretty printer now correctly handles cyclic references in objects, replacing them with ""..."" to prevent infinite recursion. This change affects the `rich/pretty.py` file and the `tests/test_pretty.py` file."
335,"Added certbot-dns-rfc2136 to list of changed modules in CHANGELOG
Added certbot-dns-rfc2136 to list of changed modules in CHANGELOG
",Update changelog to include certbot-dns-rfc2136 package.
336,"[NFC] update news link
",Update links from Medium to HPC-AI Tech blog.
337,"Added Wayfair to the list of companies using Apache Airflow
Added Wayfair to INTHEWILD.md
",Add Wayfair to the list of companies using Airflow.
338,"Add webview docs and examples, Set webview as default
","### Description of Pull RequestThis pull request updates the README.md file to reflect changes in the G4F (Generalized Fourier Transform) project. The main changes include:* Improved documentation for the provider status list and updates* Enhanced compatibility and error handling* Added support for local models and potential development* Improved the Bing wrapper with a new approach* Updated the web interface with a new GUI and improved performance* Added support for multiple platforms, including Linux* Improved the web server and added a new command-line interface* Updated the requirements.txt file to include new dependencies* Added support for SSL/TLS encryption in the web server* Improved the GUI client with a new file input and image upload feature* Added support for camera and image input in the GUI client* Improved the API with new endpoints and updated documentation* Added support for multiple providers, including OpenAI and Bing* Improved the stability and performance of the G4F projectThese changes aim to improve the overall functionality and usability of the G4F project, making it more robust and user-friendly."
339,"[core] Deflake test_object_manager::test_object_transfer_during_oom




## Why are these changes needed?

This flaky test checks that it isn't possible to store more objects than the available object store memory, but it looks like it may have been written before object spilling was enabled. Now with spilling and fallback to object allocation on disk, the test should be modified.

## Related issue number

Closes #40446.

## Checks

- [ ] I've signed off every commit(by using the -s flag, i.e., `git commit -s`) in this PR.
- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
    - [ ] I've added any new APIs to the API Reference. For example, if I added a 
           method in Tune, I've added it in `doc/source/tune/api/` under the 
           corresponding `.rst` file.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Remove test_object_transfer_during_oom test case.
340,"TST: Added test to check that the freqstr attribute of the index is p‚Ä¶
‚Ä¶reserved after a shift operation. Run black and flake8.

- [x] closes #21275
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (omitted, test only)
",Add new test cases for `shift` method with `freqstr` argument.
341,"Fix code typos for C.145
Also added the ""extends B"" to D
",Change the definition of `f` and `use` function to use `int` return type and override keyword.
342,"[openload] Fix extraction.
### Before submitting a _pull request_ make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/rg3/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)
### What is the purpose of your _pull request_?
- [x] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---

They changed ""decryption"" code a little. Here is current one. 

``` js
var y = $(""#yJ7n4OuuU7"").text();
var magic = y.slice(-1).charCodeAt(0);
y = y.split(String.fromCharCode(magic - 1)).join(""  "");
y = y.split(y.slice(-1)).join(String.fromCharCode(magic - 1));
y = y.split(""   "").join(String.fromCharCode(magic));
var s = [];
for (var i = 0; i < y.length; i++) {
  var j = y.charCodeAt(i);
  if ((j >= 33) && (j <= 126)) {
    s[i] = String.fromCharCode(33 + ((j + 14) % 94));
  } else {
    s[i] = String.fromCharCode(j);
  }
}
var tmp = s.join("""");
var str = tmp.substring(0, tmp.length - 1) + String.fromCharCode(tmp.slice(-1).charCodeAt(0) + 2);
```

They've added (funny code if you ask me) 

``` js
var magic = y.slice(-1).charCodeAt(0);
y = y.split(String.fromCharCode(magic - 1)).join(""  "");
y = y.split(y.slice(-1)).join(String.fromCharCode(magic - 1));
y = y.split(""   "").join(String.fromCharCode(magic));
```

I wonder what they are trying to achieve by doing such stupid obfuscation. 

I adopted those changes, let's see how long it will be working this time :)

Fixes #10408
",Add logic to handle encrypted data extraction in Openload extractor.
343,"Add LEXUS_ESH_TSS2 to improved longitudinal tune
2021 Lexus ES 300h.  Tested improved longitudinal tune parameters on master-ci for better following, stopping and resuming behind lead vehicles.

Route:  3a5c5101bd71ad5d|2021-05-24--15-05-04

Examples around timestamps 15:06:45 and 15:14:00",Improved longitudinal tuning for specific car models.
344,"fix path to script with nginx prerequisites
The path is copied from `.. include` directive below.
","Update file path in contributing guide from ""tests/integration/nginx.sh"" to ""letsencrypt-nginx/tests/boulder-integration.sh""."
345,"Single quotes are messing with the command.
",Update SQLite injection example to use double quotes instead of concatenation.
346,"Fix typo: unoffical -> unofficial
",Update to unofficial installation method for The Fuck.
347,"fix some typo
att",Update language support for multi-language recognition.
348,"eos_interfaces: Added unit testcases for eos_interfaces
##### SUMMARY
Added unit testcases for the resource module eos_interfaces


##### ISSUE TYPE

- Feature Pull Request

##### COMPONENT NAME
/home/gosriniv/ansible/test/units/modules/network/eos/test_eos_interfaces.py



",### DIFF:Changes to the Ansible module for Eos interfaces. The module now supports the following features:- Merged configuration: The module can merge the configuration with the existing configuration on the device.- Replaced configuration: The module can replace the existing configuration on the device with the new configuration.- Deleted configuration: The module can delete the existing configuration on the device.- Forced speed: The module can set the speed of the interface to a forced value.- Auto speed: The module can set the speed of the interface to auto.- Half speed: The module can set the speed of the interface to half.- Overridden configuration: The module can override the existing configuration on the device with the new configuration.The module also includes tests for the above features.
349,"Set lora target modules via command line arguments
Now it possible to set the modules directly from the command line

for example
`--lora_target_modules ""q_proj"", ""k_proj"", ""v_proj"", ""down_proj"", ""gate_proj"", ""up_proj""`",Update the `target_modules` parameter in `LoraConfig` from a hardcoded list to a dynamic value based on `lora_args.lora_target_modules`.
350,"Update reference to SF.7 to match text
Looks like the rule was changed in 768e4620 and most references were updated but this one was missed. Since the meaning is quite different I've updated this text to match the rule.

Note #1667 in its current form does correctly update this text so supersedes this change when merged.","Update note text to remove ""at global scope"" phrase."
351,"Implement simplified Box representation
This pull request aims to simplify the string representation of Box spaces. 
If the lower (or analogously upper) limit is a multiple of the all-ones vector, we only print the multiplicand, instead of the entire vector. This was discussed in issue #2497.

The method `_short_repr` will return a possibly shortened string representation of an array, as outlined above. I will add a short comment to the method later.

Best,
Markus",Add functionality to create a shortened string representation of a numpy array in the `Box` class.
352,"HTTPAdapter.send() timeout defaults
This PR allows to use urllib3 ``Timeout`` objects as ``timeout`` argument in ``HTTPAdapter.send()``.

Besides it omits the ``timeout`` argument when calling ``conn.urlopen()`` if ``HTTPAdapter.send()`` is called without a timeout. This allows setting a default timeout at connection pool level:
`adapter.poolmanager.connection_pool_kw['timeout'] = urllib3.Timeout(...)`",Update timeout type to include urllib3 Timeout object.
353,"Add myself to CONTRIBUTORS.md
## Type of changes

- [ ] Bug fix
- [ ] New feature
- [x] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Checklist

- [ ] I've run the latest [black](https://github.com/ambv/black) with default args on new code.
- [X] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [ ] I've added tests for new code.
- [X] I accept that @willmcgugan may be pedantic in the code review.

## Description

Ref to: [this](https://github.com/willmcgugan/rich/pull/207#issuecomment-674491635)

I have put it in alphabetical order by surname as you requested in the file, even though I think putting the owner at the top should make more sense",Add Hedy Li as a contributor to the Rich project.
354,"Model card: T5-base fine-tuned on QuaRTz
# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to the it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors which may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

 albert, bert, XLM: @LysandreJik
 GPT2: @LysandreJik, @patrickvonplaten
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @patrickvonplaten, @TevenLeScao
 Blenderbot, Bart, Marian, Pegasus: @sshleifer
 T5: @patrickvonplaten
 Rag: @patrickvonplaten, @lhoestq
 EncoderDecoder: @patrickvonplaten
 Longformer, Reformer: @patrickvonplaten
 TransfoXL, XLNet: @TevenLeScao, @patrickvonplaten
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 FSTM: @stas00
 -->
",Add a fine-tuned T5-base model on QuaRTz dataset for question-answering downstream task.
355,"Match the HTTPbis on HTTP 301.
See the discussion on issue #1704.
",Update the logic for handling redirects in the `resolve_redirects` method to correctly handle 301 and 302 responses.
356,"fixbug: role init with is_human=True was not work
bugfix:  role init with is_human=True was not work",Add a new attribute `is_human` to the `Role` class and use it to conditionally set the `llm` attribute to an instance of `HumanProvider` when `is_human` is `True`.
357,"Fix #6628 - JSONDecodeError are not deserializable
See issue #6628 for full bug-report

-----

requests.exceptions.JSONDecodeError are not deserializable: calling `pickle.dumps` followed by `pickle.loads` will trigger an error.

This is particularly a problem in a process pool, as an attempt to decode json on an invalid json document will result in the entire process pool crashing.

This is due to the MRO of the `requests.exceptions.JSONDecodeError` class: the `__reduce__` method called when pickling an instance is not the one from the JSON library parent: two out of three args expected for instantiation will be dropped, and the instance can't be deserialised.

By specifying in the class which parent `__reduce__` method should be called, the bug is fixed as all args are carried over in the resulting pickled bytes.",Add serialization and deserialization support for JSONDecodeError and InvalidJSONError exceptions.
358,"langchain-community: Dappier chat model integration
**Description:** 

This PR adds [Dappier](https://dappier.com/) for the chat model. It supports generate, async generate, and batch functionalities. We added unit and integration tests as well as a notebook with more details about our chat model.


**Dependencies:** 
    No extra dependencies are needed.","### DIFF:Change the way Dappier AI is integrated into the chat model. This change adds a new chat model that uses the Dappier API to generate responses. The chat model is initialized with the desired configuration, including the Dappier endpoint, model, and API key. The model is then used to generate responses to user queries.The chat model is tested using unit tests and integration tests. The unit tests check the formatting of the messages sent to the Dappier API, while the integration tests check the generation of responses using the chat model."
359,"Adds shrug to the category ""Japanese Emoticons""
¬Ø\\\_(„ÉÑ)_/¬Ø this guy",Add a new emoji to the end of the list.
360,"Adds wemake-python-styleguide
`wemake-python-styleguide` is the strictest python linter out there.
It was partially inspired by this project. And we have implemented almost rules as automatic checks from the examples. References: https://github.com/wemake-services/wemake-python-styleguide/search?q=wtfpython&type=Issues

It is a `flake8` plugin with some extra goodies, it can be used as `flake8 .` or like so:
![Github Action](https://raw.githubusercontent.com/wemake-services/wemake-python-styleguide/master/docs/_static/reviewdog.png)

Link: https://github.com/wemake-services/wemake-python-styleguide
Docs: https://wemake-python-stylegui.de/en/latest/",Update to README.md: added a new link to wemake-python-styleguide.
361,"üåê Add Japanese translation for Tutorial - Cookie Parameters
This PR translates the tutorial/cookie-params.md to Japanese.
Please review tutorial/cookie-params.md.

~~Issue #1571 generates a new directory to support Japanese.
PR #1656 generates a new directory for tutorial.
I will fix dependency after #1571 & #1656 are merged.~~","Add a new tutorial on cookie parameters, including importing the Cookie class, declaring cookie parameters, and a summary of the cookie concept."
362,"Add teddy-bear class back to first 1000 classes of imagenet22k_ms_synsets (line 851, index 850)
Working on fix for #2140 

Need to investigate further.... look at 12k & 22k map files too.",Add new synset n04399382.
363,"turn 2 prints to py2/py3 compatible syntax
as my very modest contribution towards python3
",Change print statements to include parentheses and format strings.
364,"Update utils/datasets.py to support .webp files
Simply added 'webp' as an image format to the img_formats array so that webp image files can be used as training data.

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Support for WEBP image format added to YOLOv5.

### üìä Key Changes
- The list of acceptable image formats/suffixes now includes 'webp'.

### üéØ Purpose & Impact
- **Purpose**: To allow YOLOv5 to utilize WEBP images, which are popular due to their efficient compression.
- **Impact**: Users can now train YOLOv5 models with '.webp' images, potentially reducing disk usage and improving load times. üöÄ",Update image formats to include 'webp' in the list of acceptable image suffixes.
365,"Added HAR generation as inline script
As promised yesterday, this one goes open source ;)
",### DIFF:This pull request adds a new HAR log extractor script to MITMProxy. The script uses the harparser library to generate a HAR log object. The HAR log object is then serialized to JSON and compressed. The script also includes a `print_attributes` helper method to print all attributes of an object and their values. The script has been tested with MITMProxy and has been found to work correctly.
366,"Don't block users with the warning thread.
Resolves #1349 ",### Description of Pull Request:This pull request includes several changes to the HTTPie project. The main changes are:* Added support for session persistence of repeated headers with the same name.* Changed `httpie plugins` to the new `httpie cli` namespace as `httpie cli plugins` (`httpie plugins` continues to work as a hidden alias).* Fixed redundant creation of `Content-Length` header on `OPTIONS` requests.* Fixed blocking of warning thread on some use cases.* Added support for sending `Secure` cookies to the `localhost` (and `.local` suffixed domains).* Fixed the vulnerability that caused exposure of cookies on redirects to third party hosts.* Updated the `httpie/uploads.py` file to include a new daemon thread for handling stdin data.* Updated the `tests/test_uploads.py` file to include new tests for chunked JSON and raw data uploads.* Added a new test for reading from stdin with a warning threshold.* Added a new test for reading from stdin with a blocking exit.* Updated the `TestMultipartFormDataFileUpload` class to include a new test for non-existent file raises parse error.
367,"Array Equilibrium-Index
","Add a new Python script to calculate the equilibrium index of an array. The script takes an array as input and returns the equilibrium index. If no equilibrium index is found, it returns -1."
368,"Correct some type annotations
<!--
Before contributing (PLEASE READ!)

‚ö†Ô∏è If your contribution is more than a few lines of code, then prior to starting to code on it please post in the issue saying you want to volunteer, then wait for a positive response. And if there is no issue for it yet, create it first.

This helps make sure:

  1. Two people aren't working on the same thing
  2. This is something Streamlit's maintainers believe should be implemented/fixed
  3. Any API, UI, or deeper architectural changes that need to be implemented have been fully thought through by Streamlit's maintainers
  4. Your time is well spent!

More information in our wiki: https://github.com/streamlit/streamlit/wiki/Contributing
-->

## üìö Context

Corrects type annotations and adds some missing annotations.

- What kind of change does this PR introduce?

  - [X] Bugfix
  - [ ] Feature
  - [ ] Refactoring
  - [ ] Other, please describe:

## üß† Description of Changes

- Replace `str` with `Any` where appropriate in input widgets
- Add missing annotations (e.g. `format_func : Callable[[Any], str] = str`)

  - [ ] This is a breaking API change
  - [X] This is a visible (user-facing) change

NOTE: These changes could break user's mypy tests if they rely on the existing annotations (e.g. by using `--strict` and `# type: ignore`).  This is true for basically any changes to type annotations, so IDK if this counts as a ""breaking API change"".

No behavior changes.

## üß™ Testing Done

- [ ] Screenshots included
- [ ] Added/Updated unit tests
- [ ] Added/Updated e2e tests

## üåê References

_Does this depend on other work, documents, or tickets?_

- **Issue**: Closes #XXXX

---

**Contribution License Agreement**

By submitting this pull request you agree that all contributions to this project are made under the Apache 2.0 license.
","### Update to type hints and docstrings for Streamlit widgetsThis pull request updates the type hints and docstrings for several Streamlit widgets, including `multiselect`, `number_input`, `radio`, `select_slider`, and `selectbox`. The changes include:* Updating type hints for function parameters and return types* Adding docstrings for each function to provide a brief description of what the function does* Updating the `format_func` parameter in `multiselect` and `selectbox` to use a more general type hint (`Callable[[Any], str]`) instead of a specific type hint (`str`)* Updating the `value` parameter in `number_input` and `select_slider` to use a more general type hint (`Any`) instead of a specific type hint (`str` or `int`)* Updating the `options` parameter in `multiselect`, `radio`, `select_slider`, and `selectbox` to use a more general type hint (`Sequence[V]`) instead of a specific type hint (`Sequence`, `numpy.ndarray`, `pandas.Series`, `pandas.DataFrame`, or `pandas.Index`)"
369,"Improve docstring re-indentation handling
This addresses a few crashers, namely:

* producing non-equivalent code due to mangling escaped newlines,

* invalid hugging quote characters in the docstring body to the docstring outer triple quotes (causing a quadruple quote which is a syntax error),

* lack of handling for docstrings that start on the same line as the `def`, and

* invalid stripping of outer triple quotes when the docstring contained a string prefix.

As a bonus, tests now also run when string normalization is disabled.","Changes to Black's docstring handling:* Improved docstring re-indentation for multiline docstrings.* Added a check for docstrings with backslash newline escapes, ignoring them because changing indentation changes the AST representation of the code.* Added tests for docstring handling in the `tests/data/docstring.py` file.* Updated the `tests/test_black.py` file to include tests for docstring handling."
370,"Added some doctests in binary_tree_traversal.py
### Describe your change:

Added doctests for the functions: level_order, get_nodes_from_left_to_right, get_nodes_from_right_to_left, and zigzag.

* [ ] Add an algorithm?
* [ ] Fix a bug or typo in an existing algorithm?
* [x] Add or change doctests? -- Note: Please avoid changing both code and tests in a single pull request.
* [ ] Documentation change?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.
* [x] If this pull request resolves one or more open issues then the description above includes the issue number(s) with a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue): ""Fixes #ISSUE-NUMBER"".
",Added docstrings and test cases for binary tree traversals.
371,"Add docker info for ClearML remote execution
<!--
Thank you for submitting a YOLOv5 üöÄ Pull Request! We want to make contributing to YOLOv5 as easy and transparent as possible. A few tips to get you started:

- Search existing YOLOv5 [PRs](https://github.com/ultralytics/yolov5/pull) to see if a similar PR already exists.
- Link this PR to a YOLOv5 [issue](https://github.com/ultralytics/yolov5/issues) to help us understand what bug fix or feature is being implemented.
- Provide before and after profiling/inference/training results to help us quantify the improvement your PR provides (if applicable).

Please see our ‚úÖ [Contributing Guide](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md) for more details.
-->


# Adding a default docker image to ClearML

This PR simply adds some information to the ClearML Task: the default docker image to use. When the task is later cloned and remotely executed, the remote worker (ClearML agent) will use the default YOLOv5 docker image to run the code inside of, which enhanced the out-of-the-box experience of new users.

## Open Question
Currently, I've added `latest` as the docker tag, which is already much better than the nothing we have now, but is there a way we can specifiy the exact correct docker image version tag? Or are you planning on making `latest` always backwards compatible? 

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Improved integration with ClearML by ensuring remote experiments are runnable using a predefined Docker image.

### üìä Key Changes
- Added code to specify a base Docker image (`ultralytics/yolov5:latest`) for remote ClearML tasks.
- Included additional Docker arguments (`--ipc=host -e=""CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL=1""`) to the task configuration.
- Inserted a script (`pip install clearml`) to set up the Docker environment for ClearML.

### üéØ Purpose & Impact
- üõ† **Enhanced Reproducibility**: Ensures remote tasks run in a consistent environment, reducing discrepancies between local and remote runs.
- üöÄ **Streamlined Set-Up**: Automates the environment setup for ClearML, making it simpler to start remote experiments.
- üìà **Potential Impact**: Benefits users leveraging ClearML for distributed experiments, potentially leading to wider adoption and a smoother experience.",Add new functionality to the `__init__` method of `clearml_utils.py` to set the base Docker image and provide additional arguments for remote agent execution.
372,"[youtube] Add support for downloading top lists (fixes #1868)
It needs to know the channel and the title of the list, because the ids change every time you browse the channels and are attached to a 'VISITOR_INFO1_LIVE' cookie.

It's not the ideal approach, but it's the only method I've found. If anyone has a better idea, please share it.
",Add support for downloading YouTube.com top lists.
373,"Add back crutch of incorrect responses to ChatBase
70% of responses content order to contact support instead of answer. It must not be treated as normal answer and raise ValueError.",Add a list of incorrect responses to the ChatBase class and check for these responses in the create_async_generator method.
374,"Updates starlette dependency to 0.13.6 due to vulnerability in 0.13.4
The project has Starlett dependency of version 0.13.4 which has a directory traversal vulnerability for windows machines. I was made aware of this through: https://snyk.io/vuln/SNYK-PYTHON-STARLETTE-573266

This upgrade would mostly be for the fix to this vulnerability that was introduced in Starlette 0.13.5. This upgrade would be to version 0.13.6 because version 0.13.6 fixes a breaking change related to how 0.13.5 was initially implemented.

Here is a link to the Starlett issue that was fixed for more details: https://github.com/encode/starlette/issues/981",Update Starlette version from 0.13.4 to 0.13.6.
375,"[netease] Add warnings for users outside of China
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Add warnings for users outside of China, add more diagnostic information, and give suggested advice.",Change in error handling for geo-restricted videos.
376,"Added_Comments
I've Added Some Comment Lines To Make Sure That Beginner Python Developers Understand How Everything Is Working!
Hope You Like It!üòÑ","Add comments and descriptions to the code, and modify the GUI to include a button to roll the dice."
377,"Update path name on xpu-max1100.yml, add badge in README
- Fixes xpu-max1100 not running on PR because of incorrect yml name.",Add support for Intel XPU and update the README.md file to include a badge for the new workflow.
378,"langchain-mistralai: add missing _combine_llm_outputs implementation in ChatMistralAI
# Description
Implementing `_combine_llm_outputs` to `ChatMistralAI` to override the default implementation in `BaseChatModel` returning `{}`. The implementation is inspired by the one in `ChatOpenAI` from package `langchain-openai`.
# Issue
None
# Dependencies
None
# Twitter handle
None",Added functionality to combine LLM outputs and added tests for combined LLM outputs.
379,"Flip conditional in session.send()
Previously we checked that the `request` being sent was an instance of a
PreparedRequest. If a user somehow created a PreparedRequest using a different
Requests library instance, this check makes the request un-sendable.

(This happened recently - unbeknownst to me, my server was running an outdated
version of pip, vulnerable to this issue - pypa/pip#1489, which creates
multiple subdirectories (src/requests, src/requests/requests) when you rerun
pip install --target. So the PreparedRequest was being created in one version
of the library and compared against the other version of the library, and
throwing this exception, even though they were both PreparedRequest instances!)

It would probably be preferable to check the object's behavior (instead of
its type), but a PreparedRequest has a lot of behavior, and it wouldn't be
really feasible or allow us to provide a helpful error message to check all
of it here. Instead flip the conditional to guard against the user sending an
unprepared Request, which should still give us most of the benefits of the
better error message.

Fixes #3102
",Fixes for PreparedRequest and non-prepared request handling in requests library.
380,"[docs] batch prediction guide, strict mode
","### DIFF:Changes were made to the documentation of Ray Data's batch inference feature. The changes include:* Added a new section on default data formats, which explains how Ray Data infers the batch format based on the input dataset.* Updated the examples to use the `ds.default_batch_format()` method to specify the batch format.* Added a new example that demonstrates how to use the `ds.map_batches()` method with a custom batch format.* Updated the documentation to reflect the changes in the `ds.map_batches()` method.* Added a new section on troubleshooting, which provides tips on how to debug issues with batch inference.### Description:Changes were made to the documentation of Ray Data's batch inference feature. The changes include the addition of a new section on default data formats, updated examples, and a new section on troubleshooting."
381,"Changed sticky bit string from 'sS' to 'tT'
",Change's' to 't' in permission calculation.
382,"Magical fix for #152 (M2Crypto install dependency order)
",Remove M2Crypto from install_requires and add a comment explaining the reason.
383,"Adding Not So Standard Deviations blog post by Roger D. Peng and Hilary Parker
","Added two new links to the ""Podcasts"" section: ""Not So Standard Deviations"" and ""Learning Machines 101""."
384,"Better cookie docs on sessions.
Suggested as part of #1488.
",Update documentation for `cookies` attribute in `requests/sessions.py` to include a description of its behavior.
385,"Fix ""RuntimeError: dictionary changed size during iteration"" in the ""expire"" function of ""mitmproxy/certs.py""
Fix #4057 

I changed iteration to list comprehension in order to prevent ""RuntimeError: dictionary changed size during iteration"" in the ""expire"" function of ""mitmproxy/certs.py""

",Update CHANGELOG to include new feature and bug fix. Fix certificate runtime error during expire cleanup.
386,"[ie/nintendo] Fix Nintendo Direct extraction
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

This rewrite redirects Nintendo Direct extraction to the GraphQL-API endpoint.


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))


",
387,"Add AdamW to the supported optimizers
As a follow up to https://github.com/microsoft/DeepSpeed/pull/670 this PR documents that AdamW is natively supported.
",Update documentation to include AdamW optimizer.
388,"Document and initialise Response.request
In response to #2066. Weirdly, we weren't initialising the `.request` property in the constructor. Nothing terrible there, but we need to do it to get it documented, so let's do it.
",Update the `__init__` method to include a new attribute `request` and add a docstring explaining its purpose.
389,"[RLlib] Trajectory view API: enable by default for ES and ARS


This PR is based on #11747 (which needs to be merged first!)

It enables the trajectory view API by default for: ES and ARS (on tf, tfe, tf2, and torch).



## Why are these changes needed?



## Related issue number



## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
","Changes to use the new ""trajectory view API"" to collect samples and produce model- and policy inputs."
390,"Fix 2 typos
In manim cairo-backend, line 127 and 131,
Fixing typo: ""ipmlemented"" to ""implemented"" ; ""understanable"" to ""understandable""",Update comments in interpolate_submobject and get_sub_alpha methods.
391,"Fix awkward wording.
","Update the wording of the ""Reason"" section to clarify the purpose of a destructor."
392,"Correct order of overflowing_tokens for slow tokenizer
# What does this PR do?
 When using a slow tokenizer (LayoutLM, Bert, Alberta, etc.), the `overflowing_tokens` were obtained in the wrong order. I have made the necessary changes that will produce the `overflowing_tokens` in the correct order.
## Tasks summary - 
- [x] making sure overflowing tokens are returned in the correct order for all `truncation_strategy` for a sequences of input ids.
- [x] if a pair of sequences of input ids (or batch of pairs) is provided, an error should be raised for the `truncation_strategy=True` or `longest_first` stating _""Not possible to return overflowing tokens for pair of sequences with the `longest_first`.Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.""_
- [x] Replaced the deprecated method `encode_plus` to regular `__call__` method in `test\test_tokenization_common.py`.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



This PR fixes the issue [ huggingface/transformers#13148 ](https://github.com/huggingface/transformers/issues/13148 ) 
Fixes huggingface#13148



## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),Pull Request section? Yes üëçüèª
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Yes, [ huggingface/transformers#13148 ](https://github.com/huggingface/transformers/issues/13148 ) 
- [x] Did you write any new necessary tests?Yes üëçüèª  , Required tests are added in `tests/test_tokenization_common.py`
- [x] Did you make sure to update the documentation with your changes? 


Anyone in the community is free to review the PR once the tests have passed.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->
@patrickvonplaten  @NielsRogge @LysandreJik @n1t0 @SaulLu ",Change value of truncation strategy from `longest_first` to `only_second` or `only_first` to return overflowing tokens.
393,"Use internal imports in Bluetooth update coordinator to avoid future circular imports

## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->
We should not import from the top level / public api here, instead we should import the internal path since future refactoring will create circular imports otherwise


## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Deprecation (breaking change to happen in the future)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [x] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [x] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] I have followed the [perfect PR recommendations][perfect-pr]
- [ ] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/development_checklist/
[manifest-docs]: https://developers.home-assistant.io/docs/creating_integration_manifest/
[quality-scale]: https://developers.home-assistant.io/docs/integration_quality_scale_index/
[docs-repository]: https://github.com/home-assistant/home-assistant.io
[perfect-pr]: https://developers.home-assistant.io/docs/review-process/#creating-the-perfect-pr
",Update import statements and add new imports from `api` and `match` modules.
394,"fix
",Update version from 2.0.92 to 2.0.93 and adjust MPS hack to use fp16 correctly.
395,"a+=b vs a=a+b : clarify the difference is not universal
Made the text say it depends on class, and rest of explanation talk of lists, not objects.","Clarified the behavior of `a += b` and `a = a + b` in the context of lists, highlighting the difference in how they modify the original object."
396,"[downloader/http] Make retrying more lenient
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [X] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [X] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [X] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [X] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [X] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Makes the http downloader much more lenient on what it retries on, to survive any sort of brief network interruption etc. (given enough retries for such). 

We are thinking it is better to retry on more even though some may be unrecoverable, vs retrying on few, known and likely to be recoverable errors but missing many other cases.

Resolves https://github.com/yt-dlp/yt-dlp/issues/3056, https://github.com/yt-dlp/yt-dlp/issues/2071, https://github.com/yt-dlp/yt-dlp/issues/3034, https://github.com/yt-dlp/yt-dlp/issues/2969 and prob many upstream

Might also help sorting exceptions in #2861 simpler.
",Update import statements in yt_dlp/downloader/http.py: - Removed import of errno and socket modules. - Added import of ssl and compat_http_client modules.
397,"0.107.3
- Fix netatmo webhook registration issue ([@cgtobi] - [#32994]) ([netatmo docs])
- Fix discovery issue with netatmo climate devices ([@cgtobi] - [#33040]) ([netatmo docs])
- Fix packages for schemas without a default ([@frenck] - [#33045]) ([person docs])

[#32994]: https://github.com/home-assistant/core/pull/32994
[#33040]: https://github.com/home-assistant/core/pull/33040
[#33045]: https://github.com/home-assistant/core/pull/33045
[@cgtobi]: https://github.com/cgtobi
[@frenck]: https://github.com/frenck
[netatmo docs]: https://www.home-assistant.io/integrations/netatmo/
[person docs]: https://www.home-assistant.io/integrations/person/",### DIFF:Added new features to the Netatmo component:- Added support for device IDs- Added support for cloud integration- Added support for cloudhook URL- Added support for presence detection- Updated camera and climate components- Updated config flow to include new features- Updated const.py to include new constants- Updated tests for new features
398,"Typo in a comment
","Update comment in autoblack.yml workflow to use ""compliant"" instead of ""complient""."
399,"Fix requests.packages not having package attributes
Fixes #4104","Add Ryan Pineo to the list of contributors in AUTHORS.rst. Update the requests/packages.py file to import packages using locals() instead of __import__. Add a new tests/test_packages.py file with tests for accessing urllib3, idna, and chardet attributes."
400,"Add a reference paper for Adagrad
Add the reference paper for the Adagrad optimizer: [Adaptive Subgradient Methods for
Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).
",Update documentation for Adagrad optimizer with a reference to the original paper.
401,"Flag that DNS plugins are distributed separately from Certbot
#8470 


","The pull request adds notes to the documentation of various DNS plugins for Certbot, including Cloudflare, CloudXNS, DigitalOcean, DNSimple, DNS Made Easy, Gehirn, Google Cloud DNS, Linode, LuaDNS, NS1, OVH, and Sakura Cloud. The notes explain that the plugins are not installed by default and can be installed by heading to the Certbot instructions website, choosing the system, and selecting the Wildcard tab."
402,"Remove unneeded mypy dependencies
Black no longer uses typed-ast or the dataclasses backport, so these should both be unnecessary now!",Remove types-dataclasses and types-typed-ast from the list of additional dependencies in.pre-commit-config.yaml.
403,"Connect AppVeyor to the certbot git repository
Hello !

Since my first PR for cerbot windows compatibility has been merged (see #6296), it is time to industrialize a continuous integration process for windows.

**Principle**

The goal here is to do the same than Travis, but on Windows: to execute all regression tests on various Windows plateform at each push on master, and each push on a PR for master.

This PR and the next are focusing on this:
1) Create a connection to AppVeyor (this PR)
2) Launch as much as possible existing tests on AppVeyor (see adferrand/certbot#8 for a preview).

**About AppVeyor**

First, why AppVeyor ? AppVeyor is really similar to Travis in its approach: it connects to GitHub nicely, configure the build pipeline through a yaml file, allows execution matrix against several environment descriptors (plateform, python version and so on). It is free for GitHub projects, like Travis.

But AppVeyor is built on top of Windows VM clusters, so the build environment is Windows Server 2012 R2, or Windows Server 2016. So to sum up, it is the Travis for Windows.

Furthermore, the VMs offers a complete Visual Studio environment, so it is the royal path to build MSI installers of Certbot on Windows platforms.

**About this PR**

Now, this PR does not a lot of things: it declares a ""dummy"" `appveyor.yml` at the root of certbot repository: the built declared cannot fail, it just prints ""Hello World"" before marking the build as succeeded.

What really matters here, is to connect AppVeyor to the certbot github repository, and it is the real work of this PR, that I cannot do obsiously ... 

**The process to connect AppVeyor to GitHub**

The process is really similar to Travis. **It must be done after the current PR is merged to master!** (Otherwise unexpected behavior will occur due to the lack of `appveyor.yml` file on the master branch).

A developper with access to the certbot account need to go to https://www.appveyor.com/, click Sign-In, then select GitHub as the provider to create the account. Once the OAuth authentication is established, AppVeyor will scan for public repositories hold by the certbot GitHub account. From AppVeyor administration page, you will need to go to `Projects`, `New Project`, and select `certbot` in the drop-down list of GitHub category.

Once done, AppVeyor is connected to the certbot project repository. Any PR on master will need by default to pass both Travis CI pipelines and AppVeyor pipelines. As said before, and until my next PR, this will have no impact as the integrated dummy `appveyor.yml` cannot fail.

**Next step**

When this PR is merged and AppVeyor is connected to GitHub, I will create the next PR, that will then toggle the real CI pipeline for Windows that I prepared here: adferrand/certbot#8

**Help**

Of course I am at your disposition for any help needed on this PR and on AppVeyor.

Regards,
Adrien Ferrand","New file added: appveyor.yml, containing AppVeyor CI pipeline configuration."
404,"Send the most detailed user credentials to the backend
Fixes #827 

This adds a new helper method that checks what linked accounts a user has.  If a linked account exists, we use those credentials when communicating with the backend rather than the web's local credentials.","### DIFF:Changes to the Oasst API contract tests and the OasstApiClient class to use a `BackendUserCore` object instead of a `JWT` token. The `BackendUserCore` object contains the user's ID, display name, and auth method. This change allows for more specific and strongly-typed user data to be passed to the Oasst API."
405,"Allow runtime modifications of HTTP flow filters for server replays
#### Description
Fixes #4506
`mitmproxy` during server-replay mode, calculates the hashes of flows from input files based on user defined filters and uses them to compare against hashes of incoming requests to serve the corresponding stored response by matching the hash. However, during runtime, if the user changes any of the filters, `mitmproxy` fails to recalculate the hashes of input flows and hence doesn't return the intended response. This PR fixes this issue by recomputing the hashes for every flow whenever a filter(option) used for computing hashes is changed.

#### Checklist

 - [x] I have updated tests where applicable.
 - [x] I have added an entry to the CHANGELOG.",Allow runtime modifications of HTTP flow filters for server replays.
406,"MNT use api.openml.org URLs for fetch_openml
#### Reference Issues/PRs

According to https://github.com/openml/OpenML/issues/1135#issuecomment-1504114194, this is the preferred URL.

#### What does this implement/fix? Explain your changes.

This uses `api.openml.org` rather than `openml.org` in `fetch_openml`. This adapts the tests as well.

To take the example of a single URL, [`https://openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1`](https://openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1) redirects to [`https://www.openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1`](https://www.openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1) (`openml.org` -> `www.openml.org`) which redirects to [`https://api.openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1`](https://api.openml.org/api/v1/json/data/list/data_name/titanic/limit/2/data_version/1) (`www.openml.org` -> `api.openml.org`)

#### Any other comments

I have added the ""no changelog needed"" label, let me know it you think a changelog entry is needed
","Update OpenML API prefix from ""https://openml.org/"" to ""https://api.openml.org/"" in _openml.py and test_openml.py."
407,"fix lambda function URLs when they don't exist or the service is not loaded



## Motivation

I noticed an error when calling non-existing lambda function URLs. The code we had was apparently faulty, and I got this error:

```
2024-04-11T17:20:40.966 ERROR --- [et.reactor-0] l.aws.handlers.logging     : exception during call chain
Traceback (most recent call last):
  File ""/opt/code/localstack/.venv/lib/python3.11/site-packages/rolo/gateway/chain.py"", line 166, in handle
    handler(self, self.context, response)
  File ""/opt/code/localstack/.venv/lib/python3.11/site-packages/rolo/gateway/handlers.py"", line 27, in __call__
    router_response = self.router.dispatch(context.request)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/code/localstack/.venv/lib/python3.11/site-packages/rolo/router.py"", line 378, in dispatch
    return self.dispatcher(request, handler, args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/code/localstack/.venv/lib/python3.11/site-packages/rolo/dispatcher.py"", line 71, in _dispatch
    result = endpoint(request, **args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/code/localstack/localstack/services/lambda_/urlrouter.py"", line 79, in handle_lambda_url_invocation
    match = FULL_FN_ARN_PATTERN.search(lambda_url_config.function_arn).groupdict()
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
```

After writing a test, I further ran into a problem that we've had in the past, which is that, services that have a data plane are not properly loaded based on calling URLs of the data plane, even though we could.

Previously localstack would behave like this when calling a lambda function URL without lambda being loaded first (e.g., by calling `awslocal lambda list-functions`.

```
 % curl http://a38wes0ubqxh7ipckjdl8ilptt40uiha.lambda-url.us-east-1.localhost.localstack.cloud:4566
{""__type"": ""InternalError"", ""message"": ""exception while calling lambda with unknown operation: Traceback (most recent call last):\n  File \""/opt/code/localstack/localstack/aws/protocol/parser.py\"", line 557, in parse\n    operation, uri_params = self._operation_router.match(request)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \""/opt/code/localstack/localstack/aws/protocol/op_router.py\"", line 321, in match\n    rule, args = matcher.match(path, method=method, return_rule=True)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \""/opt/code/localstack/.venv/lib/python3.11/site-packages/werkzeug/routing/map.py\"", line 631, in match\n    raise NotFound() from None\nwerkzeug.exceptions.NotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \""/opt/code/localstack/.venv/lib/python3.11/site-packages/rolo/gateway/chain.py\"", line 166, in handle\n    handler(self, self.context, response)\n  File \""/opt/code/localstack/localstack/aws/handlers/service.py\"", line 63, in __call__\n    return self.parse_and_enrich(context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \""/opt/code/localstack/localstack/aws/handlers/service.py\"", line 67, in parse_and_enrich\n    operation, instance = parser.parse(context.request)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \""/opt/code/localstack/localstack/aws/protocol/parser.py\"", line 172, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \""/opt/code/localstack/localstack/aws/protocol/parser.py\"", line 559, in parse\n    raise OperationNotFoundParserError(\nlocalstack.aws.protocol.parser.OperationNotFoundParserError: Unable to find operation for request to service lambda: GET /\n""}%  
```

In this situation:
* the service is identified as ""lambda"" because of the hostname matching rule (`.lambda-url.`)
* the service hasn't been loaded yet, so the edge routes for lambda (i.e., the data plane) haven't been added yet either
* therefore, the request moves on to the service request parser, which fails because no operation is associated with the request.

bad and ugly!

instead, localstack now correctly loads the lambda service:

```
curl http://a38wes0ubqxh7ipckjdl8ilptt40uiha.lambda-url.us-east-1.localhost.localstack.cloud:4566
{""Message"": null}
```

I had to
* adapt the service loader so it correctly loads services even when there is no api operation set (for the life of me i cannot remember why i thought it would be useful to check individual operations rather than just whole services).
* add a special type of service loader that only checks the hostname rules



## Changes

* A selected set of services are now properly loaded when calling data plane URLs. Specfically, URLs of APIGW (`.execute-api.`), S3 website (`.s3-website.`) (cc @bentsku) and lambda (`.lambda-url.`)
* Calling non-existing lambda function URLs now returns the correct HTTP response

<!-- The following sections are optional, but can be useful! 

## Testing

Description of how to test the changes

## TODO

What's left to do:

- [ ] ...
- [ ] ...

-->

",### DIFF:Changes to the LocalStack AWS app.py file:* Added a new class `ServiceLoaderForDataPlane` to load services based on hostname indicators for the AWS data plane.* Modified the `ServiceLoader` class to use the new `ServiceLoaderForDataPlane` class when loading services.* Added a new method `determine_aws_service_model_for_data_plane` to the `protocol/service_router.py` file to determine the AWS service model for data plane requests.* Modified the `register_routes` method in the `lambda_/urlrouter.py` file to handle lambda URL invocations and return a 403 error if the URL does not exist.Changes to the tests/aws/services/lambda_/test_lambda.py file:* Added a new test case `test_lambda_url_non_existing_url` to test the behavior when a lambda URL does not exist.* Modified the `test_lambda_url_invalid_invoke_mode` test case to validate the response when an invalid invoke mode is used.Changes to the tests/aws/services/lambda_/test_lambda.validation.json file:* Added a new test case `test_lambda_url_non_existing_url` to the validation file.
408,"[Datasets] Add a test case for spark-on-ray




## Why are these changes needed?
Thanks to the previous discussion on this issue, I make a test case for it.


## Related issue number
[issue #21980 ](https://github.com/ray-project/ray/issues/21980)


## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Add a new method `to_torch` to convert a dataset to a PyTorch dataset.
409,"Fix cuDNN tests
Move `times = []` from the outer to the inner loop (because the speed-up is calculated for each inner loop)
Delete `clear_session()` which is redundant with `keras_test` (thus the outer loop should be transformed to `pytest.mark.parametrize`)
Reduce the example size under the 3x speed-up is satisfied (the sample size 10000 is too large) ",Added parameterization for LSTM and GRU recurrent neural network types in the cudnn_rnn_timing test.
410,"[UI] Stream unicode properly
Currently, the console could only stream English ASCII words, separated by space. This PR enables streaming of general unicode in rich style.",Improved rich text output and better text streaming quality for non-ASCII content in the command line interface.
411,"Some preliminary documentation updates to mention renew verb
This isn't really detailed but @bmw pointed out that we really do want to point to the renew verb in all of our documentation now that it's there, because people will start looking! So this is a first cut at doing that so that something useful will be there when people look.
","Changes to the Let's Encrypt documentation:* Added information about the standalone plugin and its use cases.* Clarified the webroot plugin's behavior when obtaining certificates for multiple domains.* Added a note about the importance of serving files from hidden directories when using the webroot plugin.* Updated the manual plugin section to reflect its ability to obtain certificates without relying on other server software.* Added a section on the renewal process, including the ability to renew certificates using the `letsencrypt renew` command.* Clarified the behavior of the `--force-renew` flag and its use cases.* Provided more information about the `letsencrypt certonly` command and its ability to renew individual certificates."
412,"update CODEOWNERS, set default reviewer for ASF Update PRs
This PR updates the `CODEOWNERS` file:
- Orders the service owners alphabetically.
- Adds missing service sections which have an owner (ACM / @alexrashed, Api Gateway / @calvernaz, Logs / @steffyP, Route53 / @giograno).
- Changed codeowner of IAM from @dominikschubert to @dfangl.
- Changed codeowner of CloudWatch from @silv-io to @steffyP.
- Adds the cloudformation models to some of the sections.
- Adds the generated API code (`localstack.aws.api.`) to each section (i.e. codeowners will be added to ASF Update PRs automatically if anything changes for their service).

I changed this proactively once I realized that some parts are outdated. Please let me know if any of the sections should be set differently (as a comment or in a DM).

In addition, I added myself as a default reviewer for PRs created by the ASF Update Action.",Added reviewers to the ASF updates workflow and updated CODEOWNERS file to include new reviewers for various AWS services.
413,"üåê Add translation for `docs/zh/docs/advanced/response-cookies.md`
Help developer in China","Add new documentation for ""Response Cookies"" in the advanced section of the FastAPI documentation."
414,"Add Pyschonauts API to Games & Comics


- [x] My submission is formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update README.md: added new API endpoint for Pyschonauts World Characters Information and PSI Powers.
415,"[pre-commit.ci] pre-commit autoupdate

updates:
- [github.com/astral-sh/ruff-pre-commit: v0.1.9 ‚Üí v0.1.11](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.9...v0.1.11)
",Update pre-commit configuration to use ruff version v0.1.11. Add Gaussian Error Linear Unit activation function in neural network activation functions.
416,"Change registering unsafely without email logging level to info
Fixes #4408.",Change log level from WARNING to INFO when registering without email.
417,"gh-104799: PEP 695 backward compatibility for ast.unparse



* Issue: gh-104799

",Changes to the Python's ast module to support unparsing of function and class definitions with type parameters.
418,"Ignore empty auth header in api / Add live token count in gui
","**Changes to the Chat UI**The pull request introduces several changes to the chat UI, including:* Updates to the CSS styles for the conversation and toolbar sections* Addition of a new `input-count` element to display the number of words and tokens in the input field* Updates to the JavaScript code to calculate the count of words and tokens in the input field and display it in the `input-count` element* Addition of a new `count_input` function to update the `input-count` element whenever the input field changes* Updates to the `handle_ask` function to include the count of words and tokens in the conversation history* Updates to the `load_conversation` function to include the count of words and tokens in the conversation history* Addition of a new `count_words_and_tokens` function to calculate the count of words and tokens in a given text* Updates to the `GPTTokenizer_cl100k_base` function to include the count of tokens used in the conversation history**Other changes*** The pull request also includes several other changes, including updates to the chat UI, improvements to the conversation history, and updates to the backend API."
419,"Added Deep Learning with Python, Second Edition
Hi,
Branko from Manning here, I thought this book might be a good resource here.
Thanks for your consideration!","Added new course ""Deep Learning with Python, Second Edition"" to the list of courses."
420,"Bilibili: fix ""ÁúüÂΩ© HDR"" video_resolution
Video resolution of Bilibili ""ÁúüÂΩ© HDR"" is 2160p (3840x2160), not 3840p.",Update video resolution from 3840p to 2160p in Bilibili media encoding options.
421,"Update Detections() self.n comment
```python
        self.n = len(self.pred)  # number of images (batch size)
```

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Enhanced code comments for clarity in `models/common.py`.

### üìä Key Changes
- Improved comments for the `self.n` variable in the detection results class.

### üéØ Purpose & Impact
- **Purpose:** Clarify the purpose of the `self.n` variable within the code.
- **Impact:** Provides better understanding of the code for developers and maintainers, potentially reducing confusion and aiding in future debugging or development. This change does not alter functionality but improves code readability. üìñ‚ú®",Update the number of images (batch size) in the `__init__` method.
422,"Add Aqueduct to the list
Aqueduct is an open-source MLOps framework that allows you to write code in vanilla Python, run that code on any cloud infrastructure you'd like to use, and gain visibility into the execution and performance of your models and predictions. @josephmisiti ",Update README.md: added Aqueduct to the list of ML tools.
423,"Add get_coco128.sh for downloading the coco128 dataset
Add a script get_coco128.sh for downloading the coco128 dataset, which accelerates training validation and testing.

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Introducing a simple script for easy downloading of the COCO128 dataset.

### üìä Key Changes
- New `get_coco128.sh` bash script added to automate dataset download and setup.
- Allows quick downloading and setup of the COCO128 dataset for training with just one command.

### üéØ Purpose & Impact
- üéØ Simplifies the process of obtaining the COCO128 dataset, making it more user-friendly for both beginners and advanced users.
- üöÄ Can accelerate the setup time for training YOLOv5 models, as it provides a quick way to access a small, yet useful dataset.
- üë©‚Äçüíª Benefits users looking to experiment with or prototype object detection models without requiring the full COCO dataset's size and complexity.",Add a new script `get_coco128.sh` in `data/scripts` directory to download and unzip the COCO128 dataset.
424,"fix(discover): Various bugs with top events and Other
- This corrects how conditions were being constructed for other in top
  events by using demorgans, and OR-ing the top level conditions instead
  of AND-ing them together
- This also flips tests over to use the new feature flag
- Also fixes a bug with the index of Other being off by one
- Also fixes a bug where Other wasn't compatible with equations",Change value of `top_events_with_multiple_yaxis` from 5 to 6.
425,"New rule: sl -> ls
When typing quickly, I make this mistake all the time. This addition allows you to unfuck sl, replacing it with ls.
",Add a new rule to thefuck to replace'sl' with 'ls' in commands.
426,"update dask compatibility for 1.5.0
Update verified dask version and test script instructions

## Why are these changes needed?



## Related issue number



## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Update documentation for Dask-on-Ray to include new Ray and Dask versions.
427,"Update sort.py to make the list ""alphabetically sorted""
Hi there,
This PR is related to PR https://github.com/vinta/awesome-python/pull/768.
This is a simple script to do the sorting stuff.
Thanks.
",Add functionality to sort libraries in README.md file.
428,"ElectraForQuestionAnswering
This PR adds `ElectraForQuestionAnswering`. One of the missing models in this [project](https://github.com/huggingface/transformers/projects/17)

@LysandreJik , @sgugger ","### DIFF:New functionality added to the Electra model: question answering. The model now supports extractive question-answering tasks like SQuAD, with a span classification head on top."
429,"Reapply ""Hyundai Azera: allow fingerprinting without comma power (#31717)""
I just realized why we might only see EPS on some cars, and it's probably from https://github.com/commaai/openpilot/pull/31334",Add comma power to whitelist ECUs and introduce non-essential ECUs.
430,"New option: --no-caption
Fix #861.

The `--no-caption` option suppresses the download of captions (danmaku, lyrics, subtitles) from
- Bilibili
- music.163.com
- YouTube (see also https://github.com/soimort/you-get/pull/705#issuecomment-159485291)

Example:

``` console
$ you-get http://www.bilibili.com/video/av3535049/
Site:       bilibili.com
Title:      ‰πâËñÑ‰∫ëÂ§©ÔºÅÂ∞èÂ•≥Â≠©‰∏∫‰∫ÜÊïëËá™Â∑±ÁöÑÊúãÂèãÔºåÊçêËÇùËÑèÊ≠ª‰∫éÊâãÊúØÂè∞
Type:       Flash video (video/x-flv)
Size:       113.04 MiB (118531404 Bytes)

Skipping ./‰πâËñÑ‰∫ëÂ§©ÔºÅÂ∞èÂ•≥Â≠©‰∏∫‰∫ÜÊïëËá™Â∑±ÁöÑÊúãÂèãÔºåÊçêËÇùËÑèÊ≠ª‰∫éÊâãÊúØÂè∞.mp4: file already exists

Downloading ‰πâËñÑ‰∫ëÂ§©ÔºÅÂ∞èÂ•≥Â≠©‰∏∫‰∫ÜÊïëËá™Â∑±ÁöÑÊúãÂèãÔºåÊçêËÇùËÑèÊ≠ª‰∫éÊâãÊúØÂè∞.cmt.xml ...
```

``` console
$ you-get --no-caption http://www.bilibili.com/video/av3535049/
Site:       bilibili.com
Title:      ‰πâËñÑ‰∫ëÂ§©ÔºÅÂ∞èÂ•≥Â≠©‰∏∫‰∫ÜÊïëËá™Â∑±ÁöÑÊúãÂèãÔºåÊçêËÇùËÑèÊ≠ª‰∫éÊâãÊúØÂè∞
Type:       Flash video (video/x-flv)
Size:       113.04 MiB (118531404 Bytes)

Skipping ./‰πâËñÑ‰∫ëÂ§©ÔºÅÂ∞èÂ•≥Â≠©‰∏∫‰∫ÜÊïëËá™Â∑±ÁöÑÊúãÂèãÔºåÊçêËÇùËÑèÊ≠ª‰∫éÊâãÊúØÂè∞.mp4: file already exists

Skipping danmaku.
```



[](https://reviewable.io/reviews/soimort/you-get/864)


",Add option to download captions.
431,"Add support for Pinterest, as a showcase of using VideoExtractor
As requested in #744.

At this point, the usage of `VideoExtractor` was limited on just a few sites: **YouTube**, **Youku**, and **IQiyi**. All of them are non-trivial to implement; and relevant code is hard to read. As I understood, this has been a major obstacle for starters who wish to implement new extractors using `VideoExtractor`.

I implemented the code for extracting images from **Pinterest**, as a showcase of using `VideoExtractor` - it's all in some 50 lines of code, well commented and easy to make sense for anyone (hopefully).

It seems I'm unlikely to refactor this part for at least a while. So, you may use [`pinterest.py`](https://github.com/soimort/you-get/blob/f44a99536680c3d6529a28cf0f10ce09f2f86279/src/you_get/extractors/pinterest.py) here in this PR as a minimal boilerplate of implementing your own `VideoExtractor` instance. It can't be perfect, but should just work.

```
Œª  you-get-develop [pinterest] ./you-get -i https://www.pinterest.com/pin/243827767295808495/
site:                Pinterest
title:               Nyhavn is a 17th century waterfront, canal, and entertainment district in Copenhagen, Denmark.
streams:             # Available quality and codecs
    [ DEFAULT ] _________________________________
    - format:        original
      container:     jpg
      size:          0.8 MiB (881940 bytes)
    # download-with: you-get --format=original [URL]

    - format:        small
      container:     jpg
      size:          0.1 MiB (70598 bytes)
    # download-with: you-get --format=small [URL]
```

Example links to test with:
- https://www.pinterest.com/pin/243827767295808495/
- https://www.pinterest.com/pin/135600638756980545/
- https://www.pinterest.com/pin/328410997798990469/
- https://www.pinterest.com/pin/44965696253076739/
- https://www.pinterest.com/pin/471681760952953557/
- https://www.pinterest.com/pin/471681760952953562/



[](https://reviewable.io/reviews/soimort/you-get/756)


",Add Pinterest extractor to You Get.
432,"fix format
",Remove unnecessary comments and add docstrings.
433,"Use built-in 8-bit compression method instead of huggingface/accelerate
",Improved memory usage and compatibility with different devices and platforms.
434,"Dynamically choose python binary
Different approach to solving the problem discussed in #681.

Thoughts @jmhodges, @kuba? 
",Update Python version in manual.py to use Python 2 or Python 2.6/2.7 if available.
435,"ACME: pylint to 80 chars
",### Change configuration for Pylint to allow longer lines. Change maximum line length from 100 to 80. Update regular expression for lines that are allowed to be longer than the limit.
436,"docs: fix PromptGuard docs
<!-- Thank you for contributing to LangChain!

Replace this entire comment with:
  - Description: a description of the change, 
  - Issue: the issue # it fixes (if applicable),
  - Dependencies: any dependencies required for this change,
  - Tag maintainer: for a quicker response, tag the relevant maintainer (see below),
  - Twitter handle: we announce bigger features on Twitter. If your PR gets announced and you'd like a mention, we'll gladly shout you out!

Please make sure your PR is passing linting and testing before submitting. Run `make format`, `make lint` and `make test` to check this locally.

See contribution guidelines for more information on how to write/run tests, lint, etc: 
https://github.com/hwchase17/langchain/blob/master/.github/CONTRIBUTING.md

If you're adding a new integration, please include:
  1. a test for the integration, preferably unit tests that do not rely on network access,
  2. an example notebook showing its use. These live is docs/extras directory.

If no one reviews your PR within a few days, please @-mention one of @baskaryan, @eyurtsev, @hwchase17, @rlancemartin.
 -->

Fix PromptGuard docs. Noticed several trivial issues on the docs when integrating the new class.
cc @baskaryan 
",Change the class name `PromptGuardLLM` to `PromptGuard` in the `PromptGuard` class.
437,"fix pf.conf on macOS
As mentioned in https://github.com/mitmproxy/mitmproxy/issues/3255

Didn't find out how to deal with the ```redir_users = ""{ !="" $tproxy_user ""}""``` syntax, and I removed it üòÖ , feel free to enlight me",Update documentation for transparent proxy configuration.
438,"Fix documentation typo: accesible ‚Üí accessible
Fixes #3597

While this change will not be necessary after #3975 is merged, that could take a while, or not happen at all.","Update in the comment: ""following page is only accesible to authenticated users"" changed to ""following page is only accessible to authenticated users"""
439,"Docker fix
",Update Python version from 3.6 to 3.10.3.
440,"fix: rename the invalid function name of GoogleSerperResults Tool for OpenAIFunctionCall
- Description: rename the invalid function name of GoogleSerperResults Tool for OpenAIFunctionCall
- Tag maintainer: @hinthornw

When I use the GoogleSerperResults in OpenAIFunctionCall agent, the following error occurs:
```shell
openai.error.InvalidRequestError: 'Google Serrper Results JSON' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'functions.0.name'
```

So I rename the GoogleSerperResults's property ""name"" from ""Google Serrper Results JSON"" to ""google_serrper_results_json"" just like GoogleSerperRun's name: ""google_serper"", and it works.
I guess this should be reasonable.","Rename variable ""name"" from ""Google Serrper Results JSON"" to ""google_serrper_results_json""."
441,"Filter Media into separate categories
The Media category was becoming too vague and too cluttered with items that fell into other, already existing, categories. By breaking up this category, we are able to allow viewers to quickly find what they need, instead of making them search the category they want and then have to double-check the ""catch-all"" basin that was Media. 

These changes should cover issue #127.
","### API CollectionThis pull request adds a new section to the API collection, ""Art & Design"", which includes APIs related to art, design, and photography. The section includes APIs such as Dribbble, Noun Project, Rijksmuseum, and Unsplash.The ""Business"" section has also been updated to include new APIs such as LiveCoding, Chuck Norris Database, and File.io.The ""Data Validation"" section has been updated to include new APIs such as Battle.net, Clash of Clans, and Comic Vine.The ""Media"" section has been updated to include new APIs such as 500px, An API of Ice And Fire, and Chuck Norris Database.The ""Music"" section has been updated to include new APIs such as Deezer, Discogs, and EchoNest.The ""Open Source projects"" section has been updated to include new APIs such as Minor Planet Center and Asterank.com.The ""Photography"" section has been updated to include new APIs such as 500px, Flickr, and Unsplash.The ""Security"" section has been updated to include new APIs such as Forismatic and Medium.The ""Science"" section has been updated to include new APIs such as Facebook API, Full Contact, and LinkedIn REST-API.The ""Social Media"" section has been updated to include new APIs such as Instagram API, LinkedIn REST-API, and Telegram MTProto API.The ""Video"" section has been updated to include new APIs such as Vimeo and YouTube."
442,"Fix broken link not compatible with MkDocs
Relative github link `Cloud%20-%20Azure%20Pentest.md/#azure-recon-tools` is converted with MkDocs to:
```
https://swisskyrepo.github.io/PayloadsAllTheThingsWeb/Methodology%20and%20Resources/Active%20Directory%20Attack/Cloud%20-%20Azure%20Pentest.md/#azure-recon-tools
```
Instead of
```
https://swisskyrepo.github.io/PayloadsAllTheThingsWeb/Methodology%20and%20Resources/Cloud%20-%20Azure%20Pentest/#azure-recon-tools
```

A fix on this could be interesting. We could even go further and convert automatically links in .md files such as `https://github.com/swisskyrepo/PayloadsAllTheThings/` to `https://swisskyrepo.github.io/PayloadsAllTheThingsWeb`.",Update link to AzureHound documentation from internal to external source.
443,"fix underscore missing in repl
## Type of changes

- [x] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Checklist

- [ ] I've run the latest [black](https://github.com/ambv/black) with default args on new code.
- [ ] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [ ] I've added tests for new code.
- [ ] I accept that @willmcgugan may be pedantic in the code review.

## Description

Fixes https://github.com/willmcgugan/rich/issues/235",Fixed underscore with display hook and updated version to 5.2.1.
444,"Add basic load tests
* closes #1622

I‚Äôve used locust to write a basic load test which will hit two endpoints sequentially that mimic the text-client in the inference server.
* `/chat` to start a new conversation with chat_id
* `/chat/{chat_id}/message` to send a message to the Assistant

An isolated load test user workflow is summarised by first spawning X users every Y seconds to a maximum amount of N concurrent users

1. A user starts a conversation with the Assistant
2. Then they enter a conversation loop
    a. Send a chat message to the Assistant
    b. Wait until Assistant responds
    c. Wait Z further seconds
Repeat

I've tested this locally on my MacBook and results show the Bot seems to be capable of scaling to 100's of users without any adverse errors (but response times do naturally increase). I'll be able to provide more quantitative results once I run it in a more realistic setup with GPUs.","### Description of Pull Request:This pull request updates the README.md file in the inference directory to include new information about running the inference server, workers, and text client. It also adds a section on distributed testing using the Locust package. Additionally, it updates the locustfile.py file to include a new chat task that simulates a conversation between a user and an assistant. The worker settings.py file is also updated to include a new setting for the maximum total tokens."
445,"BOTMETA: remove hekonsek from team_scaleway
##### SUMMARY

[As requested](https://github.com/ansible/ansible/pull/48671#issuecomment-438755239):
* remove `hekonsek` contributor from `team_scaleway`
* don't notify contributor anymore.

##### ISSUE TYPE
- Bugfix Pull Request

##### COMPONENT NAME
.github/BOTMETA.yml",Update team_scaleway members to remove hekonsek and add sivel.
446,"Fix unstable unarchive test
##### SUMMARY

There is a bug in the time calculation used to determine if a change was made or not. This results in the task reporting changed when it should not, and the test failing. This seems to only happen on Ubuntu1604, but it is documented in the code.

https://github.com/ansible/ansible/blob/58145dff9ca1a713f8ed295a0076779a91c41cba/lib/ansible/modules/unarchive.py#L472-L474

Add `mode` to copy tasks to preserve the existing file mode.

These tests were added in #65666.


##### ISSUE TYPE

- Test Pull Request

##### COMPONENT NAME

`test/integration/targets/unarchive`",Changes to unarchive tasks: * Added mode: preserve to copy tasks in prepare_tests.yml and test_tar_gz_keep_newer.yml.* Added stat and assert tasks to test_unprivileged_user.yml to verify unarchived file changes.* Improved comments in test_unprivileged_user.yml to explain the purpose of the tasks.* Added ignore_errors: yes to the always block in test_unprivileged_user.yml to handle a known bug in the unarchive module.
447,"Chat history download creates more detailed file names
Chat history downloads used to contain the name of the character or mode and a timestamp. That made it easy for me to put all my exports into a folder and have it organized, especially when using Instruct mode. However, when I recently updated to the newest version, I noticed all history downloads were now named ""exported_history.json""
I decided to make a fork using the formatting code from a previous commit in order to restore the more detailed file names. Most of my changes are put into one function that works similarly to `save_persistent_history`. 
I briefly tested the code in Chat and Instruct mode on Windows.","Added functionality to save chat history at user request. The `save_history_at_user_request` function now takes three parameters: `history`, `character`, and `mode`. The `character` and `mode` parameters are used to determine the file path for saving the chat history."
448,"DOC Fix docstring in sklearn.metrics._ranking.label_ranking_loss #21350 #pariswimlds
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Addresses https://github.com/scikit-learn/scikit-learn/issues/21350


#### What does this implement/fix? Explain your changes.

Adding description for the return value
#### Any other comments?
Fix docstring in sklearn.metrics._ranking.label_ranking_loss

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Update documentation for `label_ranking_loss` function in `_ranking.py` and remove `label_ranking_loss` from the list of tested functions in `test_docstrings.py`.
449,"bpo-38291: Fix a deprecation warning in typing tests
The test was accessing typing.{io,re}.__all__, which triggered the
warning. This check isn't necessary anymore, since the objects from
typing.{io,re}.__all__ are in typing.__all__ as well, since Python 3.10.

A NEWS should not be required, since this affects the tests only.

Cc @serhiy-storchaka 


https://bugs.python.org/issue38291

",Remove check for io and re typing submodules.
450,"Jupyter progress
## Type of changes

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [x] Other

## Checklist

- [ ] I've run the latest [black](https://github.com/ambv/black) with default args on new code.
- [ ] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [ ] I've added tests for new code.
- [ ] I accept that @willmcgugan may be pedantic in the code review.

## Description

Enables progress bars with Jupyter
","### DIFF:Changes to the rich library, including updates to the CHANGELOG, documentation, and code.**Added*** Refresh per second option for progress tracking* Styles for pulsing bars* Refresh per second option for progress tracking* Styles for pulsing bars* Refresh per second option for progress tracking* Styles for pulsing bars**Changed*** Progress bars now work in Jupyter notebooks* Added refresh_per_second to progress.track* Added styles to BarColumn and progress.track**Fixed*** None**Updated*** CHANGELOG: updated to reflect changes* Documentation: updated to reflect changes* Code: updated to reflect changes"
451,"Fix for bug causing failing test keras/utils/vis_utils_test.py test_layer_range_value_fail second value (empty list).
According to the tests, an empty list passed for `layer_range` should cause an exception in `model_to_dot`. As implemented an empty list is ignored. This changes to the behaviour to match the test.","Change condition for layer_range from unconditional to conditional, requiring it to be not None."
452,"feat(browser-starfish): move data around in resource summary
1. Add time spent cell
2. Move resource info around to match mocks
![image](https://github.com/getsentry/sentry/assets/44422760/0d4caa46-3c86-4feb-a20b-4931fb38fec3)
",Added new metrics to ResourceSummary and ResourceInfo components: `sum(${SPAN_SELF_TIME})` and `time_spent_percentage()`.
453,"Fix dummy_inputs() to dummy_inputs in symbolic_trace doc string
# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->
",Update the comment in the input_names parameter to remove the unnecessary parentheses.
454,"Fix RobotEnv rgb_array rendering
Issue #1000 also applies to `RobotEnv`: when `env.render` is called with `mode='rgb_array'`, currently only a 500x500 square from the bottom-left corner of the image is drawn. We fix it here using the same patch.

(As Issue #1000 notes, this code is shared between `RobotEnv` and `MujocoEnv`. Ideally we would move this to a more abstract class. I'm not sure what the best way of doing this is; for the time being I propose we just put up with a kludgy fix in two separate places.)","Added rendering functionality to FetchEnv, HandEnv, and RobotEnv."
455,"[3.11] gh-93738: Documentation C syntax (:c:type:`FILE` -> :c:expr:`FILE`) (GH-97769)
:c:type:`FILE` -> :c:expr:`FILE`
(cherry picked from commit 192d401ba53224020f5f9ca6e1ff2c9f89511ac4)


Co-authored-by: Adam Turner 


* Issue: gh-93738

",Update documentation for C API functions to use :c:expr instead of :c:type.
456,"[ie/CloudflareStream] Improve `_VALID_URL`
Closes #9171

Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)


",Update CloudflareStreamIE to support additional URLs.
457,"Add disable_tls_verify arg for use with self-signed certs
**Describe what this pull request is trying to achieve.**

This PR adds support for `disable-tls-verify` CLI arg and updates gradio version in `requirements.txt` to latest which added support for this flag.

**Additional notes and description of your changes**

More technical discussion about your changes go here, plus anything that a maintainer might have to specifically take a look at, or be wary of.

**Environment this was tested in**

List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.
 - OS: Windows, Linux by making changes to gradio/automatic manually
 - Browser: n/a
 - Graphics card: 4090
",Add new option `--disable-tls-verify` to disable TLS verification and update the `webui` function to use this option.
458,"Respect the order of keys in a Dict's observation space when flattening
Prior to this change, during flattening, the order of the key/values in the observation was used instead of the order in the Dict's observation space. unflatten already uses the order specified by the Dict's observation space.","Changes to the flatten observation wrapper in Gym. The wrapper now correctly handles flattening and unflattening of observations in a dictionary space. The test cases cover various scenarios, including ordered and unordered dictionary spaces."
459,"Add signum function
### Describe your change:



* [x] Add an algorithm?
* [ ] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [x] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.

Fixes: #7525","New file added: maths/signum.py. This file contains the implementation of the signum function, which returns the sign of a given number. The function is tested with various inputs and the results are asserted to be correct."
460,"gh-101100: Fix Sphinx nitpicks in `library/collections.abc.rst`
<!--
Thanks for your contribution!
Please read this comment in its entirety. It's quite important.

# Pull Request title

It should be in the following format:

```
gh-NNNNN: Summary of the changes made
```

Where: gh-NNNNN refers to the GitHub issue number.

Most PRs will require an issue number. Trivial changes, like fixing a typo, do not need an issue.

# Backport Pull Request title

If this is a backport PR (PR made against branches other than `main`),
please ensure that the PR title is in the following format:

```
[X.Y]  (GH-NNNN)
```

Where: [X.Y] is the branch name, e.g. [3.6].

GH-NNNN refers to the PR number from `main`.

-->



* Issue: gh-101100




----
üìö Documentation preview üìö: https://cpython-previews--113116.org.readthedocs.build/

",Change documentation of ABCs (Abstract Base Classes) in collections module.
461,"Fix oauth TypeError (int + string)
Ensure that authorization `expires_in` token is an integer before adding it to another int

This is my first PR to sentry in and the contributing guidelines don't mention if I need to provide more information, stack errors etc. Let me know if you want them and I'll endeavor to add to future PRs","Change the calculation of the ""expires"" value in the get_oauth_data method from using a float to using an integer."
462,"Black formatting
## Type of changes

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [x ] Other

## Checklist

- [ x] I've run the latest [black](https://github.com/psf/black) with default args on new code.
- [ x] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [ x] I've added tests for new code.
- [ x] I accept that @willmcgugan may be pedantic in the code review.

## Description

Perform black formatting
",Here is the description of the pull request:**Changes to Rich library**This pull request updates the Rich library to include several bug fixes and improvements. The changes include:* Updates to the `CONTRIBUTORS.md` file to add a new contributor.* Fixes to the `dynamic_progress.py` example to correctly display the progress bar.* Updates to the `top_lite_simulator.py` example to correctly display the process table.* Fixes to the `ratio_distribute` function in `rich/_ratio.py` to correctly distribute the ratio.* Updates to the `WindowsConsoleFeatures` class in `rich/_windows.py` to correctly detect the Windows console features.* Fixes to the `__repr__` method in `rich/bar.py` to correctly display the bar.* Updates to the `get_bottom` method in `rich/box.py` to correctly display the bottom of the box.* Fixes to the `chop_cells` function in `rich/cells.py` to correctly chop the cells.* Updates to the `blend_rgb` function in `rich/color.py` to correctly blend the RGB colors.* Fixes to the `__rich_console__` method in `rich/json.py` to correctly display the JSON data.* Updates to the `tree` method in `rich/layout.py` to correctly display the tree.* Fixes to the `process_renderables` function in `rich/live.py` to correctly process the renderables.* Updates to the `restore_cursor` method in `rich/live_render.py` to correctly restore the cursor.* Fixes to the `__rich_console__` method in `rich/progress_bar.py` to correctly display the progress bar.* Updates to the `get_padding` method in `rich/table.py` to correctly get the padding.* Fixes to the `render_locals` function in `rich/traceback.py` to correctly render the locals.* Updates to the `add` method in `rich/tree.py` to correctly add the nodes.* Fixes to the `get_time` function in `rich/progress.py` to correctly get the time.* Updates to the `test_live_state` function in `tests/test_live.py` to correctly test the live state.* Fixes to the `test_download_progress_uses_decimal_units` function in `tests/test_progress.py` to correctly test the download progress.* Updates to the `test_columns` function in `tests/test_stack.py` to correctly test the columns.* Fixes to the `test_status` function in `tests/test_status.py` to correctly test the status.* Updates to the `test_rich_measure` function in `tests/test_table.py` to correctly test the rich measure.* Fixes to the `level3` function in `tests/test_traceback.py` to correctly test the traceback.These changes should improve the stability and functionality of the Rich library.
463,"feat(node): add AWS app-runner #645
As noticed in the 645 issue App-runner was missing.
https://github.com/mingrammer/diagrams/issues/645","Added AppRunner class to diagrams/aws/compute.py and updated documentation in aws.md and onprem.md files. Also, added a new file app-runner.png to resources/aws/compute directory."
464,"Bump moto-ext to 4.2.2.post2
Ext run passing :heavy_check_mark: ( see Ext Integration Tests # 2367 )

cc: @macnev2013 ",Update moto-ext[all] from 4.2.2.post1 to 4.2.2.post2.
465,"fix arg typo in SimpleDirectoryReader
",Change type of 'errors' parameter from bool to str.
466,"Fix figsize issue when using matplotlib locally
This is a pretty minor issue, but I'm seeing the following when passing `figsize=(20,10)` when plotting a Series in the `ipython --pylab` environment:

![Series_figsize](https://f.cloud.github.com/assets/440095/267751/46f8b184-8ebd-11e2-8631-2131ce0421f6.png)

Everything's there - the window is just initialized to the wrong size. Without the `--pylab` flag and manually calling `plt.show()`, the window shows up as the default size (which is incorrect).

The issue appears to be that `plot_series()` will call `plt.gca()` before `figsize=` has been passed to `plt.figure()`. This initializes the window to the default size and it apparently isn't updated when redrawn.

What this patch does is use `plt.get_fignums()` to see if we have any existing figures. If so, we can call `plt.gca()` without automatically creating a figure and window at the default sizes.

The addition of `new_ax.right_axis = new_ax` is to accommodate Series plotted with `secondary_y=True` and thus getting `new_ax` as their `Axes`. It renders fine without it but a few tests expect `right_axis` to exist. This could be handled differently if this is inappropriate.

Finally, there's a test to check that `figsize=` is at least implemented on the figure. I don't know of a way to test this bug in a backend-independent way, so this only checks that we're not regressing to the pre-v0.10.0 state where `figsize=` was silently ignored by `plot_series()`.
",Add support for figsize and title in test_plot method.
467,"Variable dropout rate

**Describe what this pull request is trying to achieve.**

Implements variable dropout rate from #4549

Fixes hypernetwork multiplier being able to modified during training, also fixes user-errors by setting multiplier value to lower values for training.

Changes function name to match torch.nn.module standard.

Fixes RNG reset issue when generating previews by restoring RNG state.

**Additional notes and description of your changes**

These changes were tested in [extension](https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension/blob/main/patches/hypernetwork.py), but only variable dropout related changes are imported.

Also, now hypernetwork has `.optional_info` attribute that may store additional informations, maybe training information, dataset, etc. 
![image](https://user-images.githubusercontent.com/35677394/211474164-757e4bb1-f2ac-4da0-a8b6-81aabbb83d69.png)
Currently its read-only.

**Environment this was tested in**

List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.
 - OS: Windows
 - Browser: Chrome and Edge
 - Graphics card: NVDIA RTX 3070

**Screenshots or videos of your changes**

Hypernetwork loading change, to check any compatibility issues:
![image](https://user-images.githubusercontent.com/35677394/211473318-9e1874cb-2ce8-423c-b824-fc2d9626fef3.png)

Testing variable dropout hypernetwork, and image test:
![image](https://user-images.githubusercontent.com/35677394/211474573-2f4fb2e5-7a47-4814-9bc2-d0965a268ac1.png)


Training newly created hypernetwork works correctly:
![image](https://user-images.githubusercontent.com/35677394/211473610-693e3ac3-2a7a-40aa-a709-c9bd2b141e14.png)

UI change, for variable dropout rate sequence:
![image](https://user-images.githubusercontent.com/35677394/211473707-97fb6f55-0e2d-46d9-94f8-de8554ad3a7e.png)

For safety, generated images are not attached regardless of its subject.","### DIFF:Changes to the Hypernetwork module:* Added a new parameter `dropout_structure` to the `__init__` method, which allows for custom dropout structures.* Modified the `parse_dropout_structure` function to generate a default dropout structure based on the layer structure.* Updated the `Hypernetwork` class to use the `parse_dropout_structure` function to generate the dropout structure.* Added a new parameter `dropout_structure` to the `create_hypernetwork` function, which allows for custom dropout structures.* Updated the `train_hypernetwork` function to use the `dropout_structure` parameter when creating the hypernetwork.* Added a new parameter `dropout_structure` to the `ui.py` file, which allows for custom dropout structures.* Updated the `update_orig` function to include the `dropout_structure` parameter in the hypernetwork creation process.Changes to the UI:* Added a new field for the `dropout_structure` parameter in the hypernetwork creation process.* Updated the `update_orig` function to include the `dropout_structure` parameter in the hypernetwork creation process.Changes to the documentation:* Added a new section to the documentation explaining the `dropout_structure` parameter and how it can be used to customize the dropout structure of the hypernetwork.Changes to the code:* Updated the code to use the `parse_dropout_structure` function to generate the dropout structure.* Updated the code to use the `dropout_structure` parameter when creating the hypernetwork.* Updated the code to include the `dropout_structure` parameter in the hypernetwork creation process.### Description:Changes to the Hypernetwork module to allow for custom dropout structures. The `dropout_structure` parameter can be used to specify a custom dropout structure, which can be used to customize the dropout behavior of the hypernetwork."
468,"fixbug: #1095
**Features**
fixbug: #1095","### DIFF:Changes were made to the code to add issue and requirement information to the WriteCodePlanAndChange action. The issue and requirement are now displayed as text in the action's instruction. Additionally, the code was modified to handle the case where the requirement analysis is refined into a string list."
469,"Added type hints to dependency injection pattern
Added type hints to _dependency injection_ pattern and also checked with mypy.",Added type hints for method parameters and return types.
470,"fix typo: ""Python'd"" -> ""Python's""
",Update documentation: changed debugging tool recommendation from `pdb` to `ipdb`.
471,"Documented additional parameters
These are included in a recent PR (https://github.com/tiangolo/fastapi/pull/1166) but not in the docs yet.
* response_model_exclude_none
* response_model_exclude_defaults",Update documentation for response model with additional information on excluding unset and none values.
472,"feat(suggested-solution): Add suggested solution to sentry options conf
Related to https://github.com/getsentry/sentry/pull/47771",Add support for AI suggested solution in organization details.
473,"Fixed #23861 -- Added an API to deprecate model fields.
https://code.djangoproject.com/ticket/23861
",Deprecation of IPAddressField: Support for IPAddressField has been deprecated. Use GenericIPAddressField instead.
474,"È¢ÑÊµãÁªìÊûú‰øùÂ≠òÂ¢ûÂä†Êç¢Ë°å
Á¨¨140Ë°å ` fout.write(file + ""\t"" + info) ` Êîπ‰∏∫  `fout.write(file + ""\t"" + info + ""\n"")`
Âéü‰ª£Á†Å‰øùÂ≠òÁªìÊûúÊ≤°ÊúâÊç¢Ë°åÔºåÊ†ºÂºèÂêÑËÆ∞ÂΩïÁ≤òÂú®‰∏ÄËµ∑‰∫Ü„ÄÇ","### DIFF:Document Visual Q&A (DOC-VQA) is a type of VQA task, mainly asking questions about the textual content of text images. The DOC-VQA algorithm in PP-Structure is developed based on PaddleNLP natural language processing algorithm library.The main features are:* Integrated LayoutXLM model and PP-OCR prediction engine.* Support Semantic Entity Recognition (SER) and Relation Extraction (RE) tasks based on multi-modal methods. Based on SER task, text recognition and classification in images can be completed. Based on RE task, we can extract the relation of the text content in the image, such as judging the problem pair.* Support custom training for SER and RE tasks.* Support OCR+SER end-to-end system prediction and evaluation.* Support OCR+SER+RE end-to-end system prediction.### Note: This project is based on the open source implementation of [LayoutXLM](https://arxiv.org/pdf/2104.08836.pdf) on Paddle 2.2, and at the same time, after in-depth polishing by the flying Paddle team and the Industrial and Commercial Bank of China in the scene of real estate certificate, jointly open source."
475,"Update Nginx ciphersuites to use Mozilla Intermediate
Related to #4284, but only partially solves the problem.",Update ssl_ciphers configuration to use more secure ciphers.
476,"Fix netatmo weatherstation setup error 
## Description:
Check if station data is received before adding the sensor.
Reduce method calls.

**Related issue (if applicable):** fixes #24727 and possibly #22066

**Pull request with documentation for [home-assistant.io](https://github.com/home-assistant/home-assistant.io) (if applicable):** home-assistant/home-assistant.io#

## Example entry for `configuration.yaml` (if applicable):
```yaml

```

## Checklist:
  - [x] The code change is tested and works locally.
  - [ ] Local tests pass with `tox`. **Your PR cannot be merged unless tests pass**
  - [x] There is no commented out code in this PR.
  - [x] I have followed the [development checklist][dev-checklist]

If user exposed functionality or configuration variables are added/changed:
  - [ ] Documentation added/updated in [home-assistant.io](https://github.com/home-assistant/home-assistant.io)

If the code communicates with devices, web services, or third-party tools:
  - [ ] [_The manifest file_][manifest-docs] has all fields filled out correctly. Update and include derived files by running `python3 -m script.hassfest`.
  - [ ] New or updated dependencies have been added to `requirements_all.txt` by running `python3 -m script.gen_requirements_all`.
  - [ ] Untested files have been added to `.coveragerc`.

If the code does not interact with devices:
  - [ ] Tests have been added to verify that the new code works.

[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html",Update the Netatmo sensor component to support multiple device types.
477,"fix(performance): Remove max from performance histogram
This was copied from the web vitals histogram where the max was needed to render
the baseline marker. This isn't necessary for the landing page, and removing it
gives us a better looking y axis.

# Screenshots

## Before

![image](https://user-images.githubusercontent.com/10239353/115927225-16b27600-a452-11eb-9aa7-c375db929891.png)

## AFter

![image](https://user-images.githubusercontent.com/10239353/115927232-1a45fd00-a452-11eb-8a71-61b19dbfb621.png)
",Remove unused variable `max` and its calculation.
478,"Add `RecordVideo` docs
",Update the `RecordVideo` wrapper to record videos in Gym environments.
479,"BugFix: A single term is larger than the allowed chunk size
Related issue:
https://github.com/jerryjliu/llama_index/issues/1031",Improved chunking logic for token splitting.
480,"Fixed typo
","Change ""thow"" to ""throw"" in the C++ Core Guidelines."
481,"update readme with data interpreter usage
1. Provide Data Interpreter example in README
2. Streamline README, replace Docker instruction with reference to the doc site","Update documentation for MetaGPT, including changes to README.md, installation, and usage."
482,"Toyota: Add Lexus GS F 2016
**Checklist**
- [ ] added entry to CarInfo in selfdrive/car/*/values.py and ran `selfdrive/car/docs.py` to generate new docs
- [x] test route added to [routes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/car/tests/routes.py)
- [x] route with openpilot: ab9b64a5e5960cba|2023-10-17--18-59-12
- [x] route with stock system: ab9b64a5e5960cba|2023-10-14--00-38-19",Added support for Lexus GS F 2016.
483,"‰øÆÂ§çnp.int
",Update documentation for Windows VS2019 build and README for quantization.
484,"Bump docker/setup-buildx-action from 1 to 2
Bumps [docker/setup-buildx-action](https://github.com/docker/setup-buildx-action) from 1 to 2.

Release notes
Sourced from docker/setup-buildx-action's releases.

v2.0.0

Node 16 as default runtime by @‚Äãcrazy-max (#131)

This requires a minimum Actions Runner version of v2.285.0, which is by default available in GHES 3.4 or later.



Full Changelog: https://github.com/docker/setup-buildx-action/compare/v1.7.0...v2.0.0
v1.7.0

Standalone mode by @‚Äãcrazy-max in (#119)
Update dev dependencies and workflow by @‚Äãcrazy-max (#114 #130)
Bump tmpl from 1.0.4 to 1.0.5 (#108)
Bump ansi-regex from 5.0.0 to 5.0.1 (#109)
Bump @‚Äãactions/core from 1.5.0 to 1.6.0 (#110)
Bump actions/checkout from 2 to 3 (#126)
Bump @‚Äãactions/tool-cache from 1.7.1 to 1.7.2 (#128)
Bump @‚Äãactions/exec from 1.1.0 to 1.1.1 (#129)
Bump minimist from 1.2.5 to 1.2.6 (#132)
Bump codecov/codecov-action from 2 to 3 (#133)
Bump semver from 7.3.5 to 7.3.7 (#136)

v1.6.0

Add config-inline input (#106)
Bump @‚Äãactions/core from 1.4.0 to 1.5.0 (#104)
Bump codecov/codecov-action from 1 to 2 (#101)

v1.5.1

Explicit version spec for caching (#100)

v1.5.0

Allow building buildx from source (#99)

v1.4.1

Fix docker: invalid reference format (#97)

v1.4.0

Update dev deps (#95)
Use built-in getExecOutput (#94)
Use core.getBooleanInput (#93)
Bump @‚Äãactions/exec from 1.0.4 to 1.1.0 (#85)
Bump y18n from 4.0.0 to 4.0.3 (#91)
Bump hosted-git-info from 2.8.8 to 2.8.9 (#89)
Bump ws from 7.3.1 to 7.5.0 (#90)
Bump @‚Äãactions/tool-cache from 1.6.1 to 1.7.1 (#82 #86)
Bump @‚Äãactions/core from 1.2.7 to 1.4.0 (#80 #87)

v1.3.0

Display BuildKit version (#72)

v1.2.0

Remove os limitation (#71)
Add test job for config input (#68)



... (truncated)


Commits

dc7b971 Merge pull request #131 from crazy-max/node16
f55bc08 Merge pull request #141 from crazy-max/fix-test
aa877a9 ci: fix standalone test
130c56f Node 16 as default runtime
See full diff in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=docker/setup-buildx-action&package-manager=github_actions&previous-version=1&new-version=2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


",Update Docker setup action from v1 to v2.
485,"DOC: Corrected code_checks.sh
- [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).

Towards https://github.com/pandas-dev/pandas/issues/37875

",Improved documentation for `pd.Period.end_time` and `pd.Series.dt.end_time` methods.
486,"[NFC] polish colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_k‚Ä¶
‚Ä¶ernel.cu code style",Changes to the L2NormFunctor and MaxNormFunctor structs to support different norms and addition of a cleanup function to handle the final reduction of the norms.
487,"fix iqiyi tiltes when accessed with curid=
#1894

test cases:
```
./you-get -i 'http://www.iqiyi.com/v_19rr9y7d7s.html?vfm=m_103_txsp#curid=625521800_eb5dcb7fbd6399a6ef85a45386f51e3b'
site:                Áà±Â•áËâ∫ (Iqiyi)
title:               È≠îÂäõÁæéÈ£ü_20170228Êúü
streams:             # Available quality and codecs
    [ DEFAULT ] _________________________________
    - format:        TD
      container:     m3u8
      video-profile: 720p
    # download-with: you-get --format=TD [URL]

    - format:        HD
      container:     m3u8
      video-profile: 540p
    # download-with: you-get --format=HD [URL]

    - format:        SD
      container:     m3u8
      video-profile: 360p
    # download-with: you-get --format=SD [URL]

    - format:        LD
      container:     m3u8
      video-profile: 210p
    # download-with: you-get --format=LD [URL]
```

```
./you-get -id 'http://www.iqiyi.com/v_19rr9y7d7s.html?vfm=m_103_txsp#curid=598026100_0bf7ea3bed1c77c42590927fba73d3d8'
[DEBUG] get_response: http://www.iqiyi.com/v_19rr9y7d7s.html?vfm=m_103_txsp#curid=598026100_0bf7ea3bed1c77c42590927fba73d3d8
[DEBUG] get_content: http://mixer.video.iqiyi.com/jp/mixin/videos/598026100
[DEBUG] get_content: http://www.iqiyi.com/v_19rr9xpp20.html
[DEBUG] get_content: http://cache.m.iqiyi.com/tmts/598026100/0bf7ea3bed1c77c42590927fba73d3d8/?t=1493013134556&sc=71c5088ffe7f22b3bc0cd452c36709b6&src=76f90cbd92f94a2e925d83e8ccd22cb7
site:                Áà±Â•áËâ∫ (Iqiyi)
title:               È≠îÂäõÁæéÈ£ü_20170106Êúü
streams:             # Available quality and codecs
    [ DEFAULT ] _________________________________
    - format:        TD
      container:     m3u8
      video-profile: 720p
    # download-with: you-get --format=TD [URL]

    - format:        HD
      container:     m3u8
      video-profile: 540p
    # download-with: you-get --format=HD [URL]

    - format:        SD
      container:     m3u8
      video-profile: 360p
    # download-with: you-get --format=SD [URL]

    - format:        LD
      container:     m3u8
      video-profile: 210p
    # download-with: you-get --format=LD [URL]
```

current behavior
```
you-get -i 'http://www.iqiyi.com/v_19rr9y7d7s.html?vfm=m_103_txsp#curid=625521800_eb5dcb7fbd6399a6ef85a45386f51e3b'
site:                Áà±Â•áËâ∫ (Iqiyi)
title:               È≠îÂäõÁæéÈ£ü_20170104Êúü
streams:             # Available quality and codecs
    [ DEFAULT ] _________________________________
    - format:        TD
      container:     m3u8
      video-profile: 720p
    # download-with: you-get --format=TD [URL]

    - format:        HD
      container:     m3u8
      video-profile: 540p
    # download-with: you-get --format=HD [URL]

    - format:        SD
      container:     m3u8
      video-profile: 360p
    # download-with: you-get --format=SD [URL]

    - format:        LD
      container:     m3u8
      video-profile: 210p
    # download-with: you-get --format=LD [URL]
```
title for 19rr9y7d7s got, which is incorrect.

This should also fix #1740",Update the title extraction logic to use the real video URL.
488,"[Windows] Create an installer for Certbot
This PR is the first step to create an official distribution channel of Certbot for Windows. It consists essentially in creating a proper Certbot Windows installer.

Usually distributing an application requires, in a way or another, to stabilize the application logic and its dependencies around a given version. On Windows, this usually takes the form of a freezed application, that vendors its dependencies into a single executable.

There are two well-known solutions to create an executable shipping a Python application on Windows: [py2exe](http://www.py2exe.org/) and [pyinstaller](https://www.pyinstaller.org/). However these solutions create self-executable `.EXE` files: you run the `.EXE` file that launches immediately the software.

This is not a end-user solution. Indeed when a Windows user wants to install a piece of software, he expects to find and download an installer. When run the installer would interface with Windows to setup configuration entries in the Registry, update the environment variable, add shortcuts in the Start Menu, and declare a uninstaller entry into the Uninstaller Manager. Quite similarly, this is what you would get from a `.deb` or `.rpm` package.

A solution that builds proper installers is [pynsis](https://pynsist.readthedocs.io/en/latest/). It is a Python project that constructs installers for Python software using [NSIS](https://sourceforge.net/projects/nsis/), the most known free Windows installer builder solution.

This PR uses pynsist to build a Windows installer. The Powershell script to launch the installer build is `.\windows-installer\construct.ps1`. Once finished, the installer is located at `.\windows-installer\build\nsis\certbot-[CERTBOT_VERSION]-win32_install.exe`.

This installer will do the following operations during the installation:
* copy in the install path a full python distribution used exclusively for Certbot
* copy all Python requirements gathered from the `setup.py` of relevant certbot projects
* copy `certbot`, `acme` and all dns plugins packages
* pre-build python binary assets
* create a shortcut on the start menu to open the online Certbot documentation
* register the existence of the application correctly in Windows Registry
* prepare a procedure to uninstall Certbot
* and of course, expose `certbot` executable to the Windows command line, like on Linux, to be able to launch it as any CLI application from Batch or Powershell

This installer support updates: downloading a new version of it and running it on a Windows with existing installation of Certbot will replace it with the new version.

Future capabilities not included in this PR:
* auto-update of Certbot when a new release is available
* online documentation for Windows (that should be the target 
for the created shortcut in Start Menu)
* register a scheduled task for certificate renewal
* installer distribution (continuous deployment + distribution channels)
* method to check the downloaded installer is untampered",Add Windows Installer for Certbot.
489,"Cache linting venv on CI.
Ensure that we cache the linting virtualenv as well as the pip cache for the `pip install -e langchain` step.

This is a win of about 60-90s overall.

Before:
![image](https://github.com/langchain-ai/langchain/assets/2348618/f55f8398-2c3a-4112-bad3-2c646d186183)

After:
![image](https://github.com/langchain-ai/langchain/assets/2348618/984a9529-2431-41b4-97e5-7f5dd7742651)
",Update GitHub Actions workflow to include caching of pip install and poetry dependencies.
490,"Update flatpages.txt
Fixed typo
",Update regular expression pattern in URLconf from 'pages/' to 'r'^pages/'.
491,"[youtube] fix `KeyError: 'url'` on some videos
Hopefully this fixes https://github.com/soimort/you-get/pull/2811.",Update the YouTube extractor to handle signatureCipher instead of cipher.
492,"Remove stale action workflow file
",Remove stale issue and pull request workflow.
493,"Slight correction to <str>.strip descriptions
Removes \ (or whitespace) from start of \ up to the first
non-\ and from the end of \ up to the last non-\.

[Python 3 Docs](https://docs.python.org/3/library/stdtypes.html?highlight=strip#str.strip)",Update documentation: Added 'from start and end' to strip() method descriptions.
494,"fix download issue
thx to @GianlucaFicarelli

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/rg3/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [ ] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)
- [x] this is a fix motivated by @GianlucaFicarelli

### What is the purpose of your *pull request*?
- [x] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---

closes #18741 

### Description of your *pull request* and other information

Explanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.",Update the canonical URL from 'https://ipfs.io/ipfs/' to 'https://video.dtube.top/ipfs/'.
495,"ÈíàÂØπxx_net.shÂÅöÁöÑ‰øÆÊîπ
1,mv xx_net.sh to 'root' dir,for no need to change the service link when do update.
2,according code/version.txt to start the specific version xx_net
3,add 'status' command for service",Update the xx_net.sh script to include a status command and improve the package path calculation.
496,"[Doctest] Add `configuration_decision_transformer.py`
Add `configuration_decision_transformer.py` to `utils/documentation_tests.txt` for doctest.

Based on issue https://github.com/huggingface/transformers/issues/19487

@ydshieh could you please check it?
Thank you :)",Update documentation tests to include new DecisionTransformer model configuration file.
497,"Added a warning that the signin email link might be going to spam
","Update text in h1 tag to include ""(likely going to spam)"" in the sign-in link message."
498,"Refresh Multi Modal Doc to include more docs
# Description

Adding more docs into MM doc for use cases part

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [x] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
","Added new sections to the multimodal use cases documentation, including comparisons of LLaVa-13, Fuyu-8B, and MiniGPT-4 models, a Pydantic program for generating structured output, chain of thought prompting for GPT4-V, and a simple evaluation of multi-modal RAG."
499,"na_ontap_dns: Add new parameter skip_validation
##### SUMMARY

Add skip_validation parameter to bypass the DNS configuration check on the Netapp Ontap. Refers to the skip-config-validation parameter on the SDK.



##### ISSUE TYPE

- Feature Pull Request

##### COMPONENT NAME

na_ontap_dns

##### ADDITIONAL INFORMATION


Tested on FAS8200 with DOT 9.4 with DNS and domain not reachabled (Create and Modify): OK


```paste below

```
",Add a new boolean parameter `skip_validation` to the `na_ontap_dns` module. This parameter allows users to bypass the check for all name servers specified in the `nameservers` field.
500,"[ie/hotstar] Support `/clips/` URLs
Closes #7699


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at 462b368

### Summary
üéûÔ∏èüîß‚úÖ

<!--
1.  üéûÔ∏è - This emoji represents the `clips` category, which is a type of short video content that HotStar offers. It also suggests the idea of extracting or downloading clips from the website.
2.  üîß - This emoji represents the update to the URL regex, which is a technical task that involves modifying the pattern that matches HotStar URLs. It also suggests the idea of fixing or improving something.
3.  ‚úÖ - This emoji represents the addition of a test case, which is a way of verifying that the extractor works as expected and handles different scenarios. It also suggests the idea of checking or confirming something.
-->
Support `clips` category in HotStar extractor. Update `yt_dlp/extractor/hotstar.py` to handle `clips` URLs and extract metadata from `content` API endpoint.

> _We're the extractors of the night_
> _We rip the clips from HotStar's site_
> _We update our regex to catch them all_
> _We map them to the content and we make them fall_

### Walkthrough
*  Update regex for matching HotStar video URLs to include `clips` category ([link](https://github.com/yt-dlp/yt-dlp/pull/7710/files?diff=unified&w=0#diff-0a6703759f63eb9f8d2a82b989320bcfddf8e3cd634bbc3b4e4c10e643f6ed9eL87-R87))
*  Add new key-value pair to `_FEED_TYPES` dictionary to map `clips` type to `content` endpoint ([link](https://github.com/yt-dlp/yt-dlp/pull/7710/files?diff=unified&w=0#diff-0a6703759f63eb9f8d2a82b989320bcfddf8e3cd634bbc3b4e4c10e643f6ed9eR175))
*  Add test case for a video from the `clips` category with expected metadata and format information ([link](https://github.com/yt-dlp/yt-dlp/pull/7710/files?diff=unified&w=0#diff-0a6703759f63eb9f8d2a82b989320bcfddf8e3cd634bbc3b4e4c10e643f6ed9eR146-R157))




",Added support for clips in HotStar extractor.
501,"Add NAVER API


- [x] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update to the README.md file: added a new API endpoint for NAVER.
502,"Do not hardcode the home directory
Fixes #369

Use [`os.path.expanduser()`](https://docs.python.org/3/library/os.path.html#os.path.expanduser) to avoid hardcoding the home directory.","Change the hardcoded path ""/home/hackingtoolpath.txt"" to a user-expandable path using os.path.expanduser() in Linux."
503,"üîß Add new Gold Sponsor vim.so üéâ
üîß Add new Gold Sponsor vim.so üéâ","Add a new sponsor ""Vim So"" to the README and sponsors.yml files, and upload the corresponding image."
504,"Fix an exception when auto-casting dicts to fp16
Deepspeed raises an exception when using fp16 auto_cast mode with a model that takes a dictionary as input. Quick test on `0.7.3` to display what I'm talking about:

```
$ cat test.py
import torch
import deepspeed
import argparse

parser = argparse.ArgumentParser(description='Test model')
parser.add_argument('--local_rank', type=int, default=-1,
                    help='local rank passed from distributed launcher')
parser = deepspeed.add_config_arguments(parser)
cmd_args = parser.parse_args()

class ValueDoubler(torch.nn.Module):
    def forward(self, d):
        return d[""value""] * 2

m = ValueDoubler()
m, _, _, _ = deepspeed.initialize(args=cmd_args, model=m)

print(""Torch: "", torch.__version__)
print(""DeepSpeed: "", deepspeed.__version__)
print(""New value: "", m({""value"": 1.0}))
$ cat ds.json
{""fp16"": {""auto_cast"": true, ""enabled"": true}, ""train_batch_size"": 1}
$ deepspeed test.py --deepspeed --deepspeed_config ds.json
...
Torch:  1.12.1+cu102
DeepSpeed:  0.7.3
Traceback (most recent call last):
  File ""test.py"", line 20, in 
    print(m({""value"": 1.0}))
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/utils/nvtx.py"", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1664, in forward
    inputs = self._cast_inputs_half(inputs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1693, in _cast_inputs_half
    new_inputs.append(self._cast_inputs_half(v))
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1697, in _cast_inputs_half
    for k, v in inputs:
ValueError: too many values to unpack (expected 2)
```

And testing on master (I had to fuss it a little to get master to work):
```
$ deepspeed test.py --deepspeed --deepspeed_config ds.json
...
Torch:  1.12.1+cu102
DeepSpeed:  0.7.4+eed4032
Traceback (most recent call last):
  File ""test.py"", line 20, in 
    print(m({""value"": 1.0}))
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/utils/nvtx.py"", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1664, in forward
    inputs = self._cast_inputs_half(inputs)
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1693, in _cast_inputs_half
    new_inputs.append(self._cast_inputs_half(v))
  File ""/data/home/mattks/.local/share/virtualenvs/test-t-h-loqS/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1697, in _cast_inputs_half
    for k, v in inputs:
ValueError: too many values to unpack (expected 2)
```

This PR fixes this:
```
$ deepspeed test.py --deepspeed --deepspeed_config ds.json
Torch:  1.12.1+cu102
DeepSpeed:  0.7.4+45d39d7
New value:  2.0
```",Update the code to correctly handle casting of inputs in the _cast_inputs_half method.
505,"[requires.io] dependency update on main branch
",Changes to the OpenSSL library and related functions in the mitmproxy project.
506,"removed Resource > Website > Python Hackers due to dead link (status ‚Ä¶
I'm suggesting that that you remove a dead resource.  The URL no longer exists.  ",Update to README.md: removed link to [Python Hackers](http://www.oss.io/open-source/).
507,"[3.9] bpo-39883: Use BSD0 license for code in docs (GH-17635)
The PSF board approved this use.
(cherry picked from commit e223d06a8b2067c68e9f97c4628c2b0f056dcae6)


Co-authored-by: Todd 


https://bugs.python.org/issue39883

","Update the license information in the Python documentation to include the Zero-Clause BSD license for code, examples, and recipes, in addition to the existing Python 2.0 license."
508,"#1096: Rule: Bypass failed git hook

#1096 ","Added support for `git_hook_bypass` rule in `thefuck`. This rule adds the `--no-verify` flag to `git am`, `git commit`, and `git push` commands."
509,"Created folder for losses in Machine_Learning
### Describe your change:
Added a directory named `losses` in `machine_learning` that contains two loss functions namely `binary_cross_entropy.py` and `mean_squared_error.py`


* [x] Add an algorithm?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.
* [x] If this pull request resolves one or more open issues then the description above includes the issue number(s) with a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue): ""Fixes #ISSUE-NUMBER"".
","### New loss functions for binary classification and regression tasksThis pull request introduces two new loss functions: Binary Cross-Entropy (BCE) and Mean Squared Error (MSE). The BCE loss function is widely used in binary classification tasks, while the MSE loss function is commonly used in regression tasks. Both loss functions are implemented in Python and include documentation and example usage."
510,"Add Python 3.8 official support
Fixes #4085",Update HTTP cache storage backend to use LevelDB storage backend.
511,"Remove show footer embed option
## Describe your changes

This PR fully removes the useless `showFooter` embed option. There isn't any need for this option since we recently removed the footer.

## GitHub Issue Link (if applicable)

- Closes https://github.com/streamlit/streamlit/issues/7785

## Testing Plan

- Updated tests

---

**Contribution License Agreement**

By submitting this pull request you agree that all contributions to this project are made under the Apache 2.0 license.
",Remove the display of footer.
512,"Primer: Virtualenv is now formatted with newest Black
Re: https://github.com/pypa/virtualenv/pull/1939","Change the ""expect_formatting_changes"" value from true to false in the primer.json file."
513,"Errbot chatbot (http://errbot.io)
## Why this framework/library/software/resource is awesome?

The easiest and most popular chatbot to implement ChatOps in Python.
Very gentle learning curve but very powerful once you start to master
the tool (conversation flows, security, markdown formatting and
template, ...). It comes with developer friendly text and graphical
consoles to debug your plugins easily.

It is compatible out of the box with Slack, Hipchat, XMPP, IRC and lot
more. But you can also extend it easily to support a custom chat system.

Already 300+ public plugins are available !
Check out the list here: https://github.com/errbotio/errbot/wiki
## Vote for this pull request

Who agrees that this change should be merged could add your reactions (e.g. :+1:) to this pull request.
",Update README.md: added Errbot to the list of inspirations.
514,"Allow to open images in new browser tab by MMB.
## Description

Basically, as a shortcut for context-menu -> Open Image in New Tab.

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
","Add a new condition to check if the left mouse button is clicked, and if so, open the image in a lightbox."
515,"Add to authors and fix case
Fix sentence case requests by sigmavirus24 here: https://github.com/kennethreitz/requests/pull/2168
Added myself to authors.
","Update AUTHORS.rst to include a new contributor, Jonathan Wong, and modify the resolve_redirects function in requests/sessions.py to keep track of redirect history."
516,"bug fix - run_internal_graph()
See #9565",Fixed bug in layer's compute_mask method when layer has multiple outputs.
517,"infra: rm api build CI
",Removed API docs build workflow.
518,"feat/startup-script: Feature to avoid package installation errors when installing custom nodes.
Currently, in ComfyUI-Manager, automatic installation of dependencies is performed when installing custom nodes. However, this can lead to module conflicts or failure to install due to file locking issues, especially in Windows environments.

To address this problem, a solution has been implemented where the imports are minimized at the top of main.py, allowing the install script to run in an environment with minimal imports.",Add startup script execution functionality and ignore startup-scripts folder in.gitignore.
519,"UI: lazy onboarding widgets init
`OnboardingWindow::showEvent` is always called on startup  which will cause the TermsPage to be initialized every time( in `TermsPage::showEvent`).~and the screen may flicker.~",Changes to OnboardingWindow: - Added a check for accepted terms and training completion in the `updateActiveScreen` method. - Added a check for accepted terms and training completion in the constructor. - Added a check for accepted terms and training completion in the `showEvent` method. - Added a check for accepted terms and training completion in the `showTrainingGuide` method. - Added a check for accepted terms and training completion in the `completed` method.
520,"Vevo sig changed again, please update for us! Thanks very much! (fixes #...
...1375)
",Change the logic for decrypting signature in YouTube extractor.
521,"Enum.2: typo/formatting
","Update documentation: Replace ""Such off-by-one switch"" with ""Such off-by-one `switch`""."
522,"Pin to PyTest 8.0.0
Fix failures in nv-accelerate-v100 unit tests.

Fix running on this PR: https://github.com/microsoft/DeepSpeed/actions/runs/7923894998/job/21634533848
Original failure [here](https://github.com/microsoft/DeepSpeed/actions/runs/7961285826/job/21748664124?pr=5129#step:7:415):
```
    from _pytest.doctest import (
E   ImportError: cannot import name 'import_path' from '_pytest.doctest' (/tmp/actions-runner/_work/DeepSpeed/DeepSpeed/unit-test-venv/lib/python3.8/site-packages/_pytest/doctest.py)
```",Update version of pytest from any version to 8.0.0 or less.
523,"Fixed docstrings in keras/optimizer_v2/learning_rate_schedule.py
Minor doc correction:
```
The schedule a ... -> The schedule is a ... 
```",Improved documentation for learning rate schedules.
524,"Fix code snippet in config.rst for env vars Fixes #3117
Changes the snippet in the config documentation to no longer assign a bool as the default value from os.environ.get()",Update documentation to include new configuration options.
525,"Added rasa_nlu(Natural language understanding) to the list of python-nlp
Natural Language Understanding(NLU) is the next step of NLP. There are very few libraries implementing NLU. As of now, we can include it to the list of python-nlp. Later, when we have sufficient NLU libraries, we can create a separate section for the same.

Please add this to the list,

Thanks,
Hitesh",Update README.md: added rasa_nlu to the list of NLP tools.
526,"Fixed #34513 -- Added system check for relational fields in ModelAdmin.list_display.
Fixed [ticket #34513](https://code.djangoproject.com/ticket/34513)
Add system check for wrong related fields in `ModelAdmin.list_display`.",Improved check for many-to-many and reverse foreign key fields in list_display.
527,"Use xfail marker for a test expected to fail without Internet connection
This is only a minor improvement on the great work of https://github.com/kennethreitz/requests/pull/2859 that permits to run tests on hosts without Internet connection without failures.
",Add a new test case to test_auth_is_stripped_on_redirect_off_host with a pytest mark to expect a ConnectionError.
528,"Fix device problem
Before, the one_hot could only run in device='cuda'. Now it will run on input device automatically.",Change the device parameter in the `one_hot` and `mixup_target` functions from 'cuda' to 'x.device' to match the device of the input tensor.
529,"Import urllib3's Retry location from the right place
Importing from urllib3's top-level location causes the namespace to be
urllib3.util.retry.Retry instead of
requests.packages.urllib3.util.retry.Retry. Without this fix, an using
requests with an un-vendored version of urllib3 will break when urllib3's
retry handling kicks in.

Closes shazow/urllib3#567
",Update import statement for Retry from urllib3.util.retry to urllib3.packages.urllib3.util.retry.
530,"minor cleanup and dead code removal in bootstrap.py
Just some minor cleanup I did while looking through bootstrap.py to better understand what it does",Fixed permissions on /var/run/docker.sock and added a check for running in a CI environment.
531,"Falcon support
- Include 'einops' in requirements for Falcon support
- Append Falcon details to README regarding 'trust-remote-code'
- Implement 'trust-remote-code' option in UI
- Integrate Falcon configuration",Update the description of the ChatGLM and Falcon models to include the trust_remote_code option.
532,"Fix assignment of template field in `__init__` in `analyticdb_spark.py`
related: https://github.com/apache/airflow/issues/36484

removed the templated field because it's been run over in the constructor and has no meaning. (this can be cause an issue if someone will inherited from those classes).

fix:
```
AnalyticDBSparkSQLOperator's constructor lacks direct assignments for instance members corresponding to the following template fields (i.e., self.field_name = field_name or 
super.__init__(field_name=field_name, ...) ):
['spark_params']
AnalyticDBSparkSQLOperator's constructor contains invalid assignments to the following instance members that should be corresponding to template fields (i.e., self.field_name = field_name):
['self.spark_params']
AnalyticDBSparkBatchOperator's constructor lacks direct assignments for instance members corresponding to the following template fields (i.e., self.field_name = field_name or 
super.__init__(field_name=field_name, ...) ):
['spark_params']
AnalyticDBSparkBatchOperator's constructor contains invalid assignments to the following instance members that should be corresponding to template fields (i.e., self.field_name = field_name):
['self.spark_params']
```

cc: @shahar1


<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 ""License""); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
 -->

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of an existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->




---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).
",Remove unnecessary variable and assign it directly.
533,"Add fingerprint for 2019 Honda Civic Hatchback
Add fingerprint for Honda Civic Hatchback 1.0T Elegance (Europe - Poland).

Route: 742cb48652d68d6d|2021-10-01--17-07-24
Discord user: miglanz",Add new value '37805-5AK-T530\x00\x00' to the list of values for CAR class.
534,"[docs] streaming contents: use app.response_class
Examples should use app.response_class to encourage code that doesn't need to be changed should the response class be customized later on.",Improved streaming response functionality in Flask.
535,"Create Web_Scraper.py
A python program to scrape job openings from naukri.com","Added a new file `Web_Scraper.py` with a web scraping script. The script uses Selenium and BeautifulSoup to scrape job profiles from a specific URL. It initiates a Chrome driver, loads the URL, waits for the page to load, and then extracts the job profiles from the page. The script prints the top ten job profiles and then closes the driver."
536,"ja: Fix translation in horizontal scaling section
- The Japanese translation is ambiguous about ‚Äúvertical scaling‚Äù means scaling out or scaling up.
- The word ‚Äúexpensive‚Äù is missing in the Japanese translation.",Update README-ja.md: Clarify horizontal scaling benefits and drawbacks.
537,"Fix training for newer PyTorch builds
When training, the in-place operations in `process_tokens()` can cause an error during the backward pass. This PR fixes that.",Change the calculation of `z` in the `process_tokens` function to use element-wise multiplication instead of broadcasting.
538,"[infoq] Add audio only format if available
Refactor cookie code into a function.
Renamed formats to http_video, http_audio, rtmp_video
Renamed extract functions to video instead of videos as they return
one or no video.

## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/rg3/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Add audio only format for InfoQ extractor if available",Added headers parameter to _is_valid_url method. Added cookies extraction and usage in HTTP video and audio extraction.
539,"rawgpsd tests
closes #28382",Add test for rawgpsd assistance loading.
540,"Adds support for using message metadata.
# Description
Modifies the `load_data` method of the Discord Reader class with an additional optional parameter to return complete message metadata, namely by way of a boolean value `complete_metadata` (defaulted to false). 

This change is needed for users who wish to query information about the users and allows for further advanced filtering (such as NSFW content, among others). This change requires no dependency changes.


Fixes # (issue)

## Type of Change

Please delete options that are not relevant.


- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Ran a modified version of the [Discord Reader Demo](https://gpt-index.readthedocs.io/en/stable/examples/data_connectors/DiscordDemo.html) locally both with and without the flag and queried the index/engine for a list of users who sent messages in the specified channel (query: `list all users who posted to the channel` produced an output similar to `There is no information provided about the users who posted ideas to the channel.` with the flag set to `False`, and produced an accurate list when the metadata was provided by way of the override flag.

- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",Change the return type of `_read_channel` method from `str` to `List[Document]` and wrap each message in a `Document` containing the text as well as some useful metadata properties.
541,"introduce six lib, used python-modernize + manual fix
","### DIFF:This pull request adds compatibility support for Python 2 and 3. It includes changes to the `flask/_compat.py` file, which provides some compatibility support that is not yet available in ""six"" 1.3.0. The changes also update the `flask/helpers.py` file to use the `six.advance_iterator` function. Additionally, the `flask/json.py` and `flask/sessions.py` files have been updated to use the `six.text_type` function. The `flask/templating.py` file has been updated to use the `six.itervalues` function. The `flask/testsuite` directory has been updated to use the `six.text_type` and `six.advance_iterator` functions."
542,"Combine cookies from original request and session file
This pull request should address #824. Cookies that are set through a request in the CLI are added to the session cookies. @gmelodie","### DIFF:Changes to the HTTPie documentation and tests:* Added a new section to the README.rst file explaining cookie storage behavior.* Updated the `httpie/sessions.py` file to handle cookie storage and priority.* Added new tests to the `tests/test_sessions.py` file to cover cookie storage and priority.* Updated the `tests/test_sessions.py` file to include tests for expired cookies and cookie storage.* Added new tests to the `tests/test_sessions.py` file to cover cookie storage and priority.* Updated the `tests/test_sessions.py` file to include tests for cookie storage and priority.### Description:Changes to the HTTPie documentation and tests to improve cookie storage behavior and priority. The changes include adding a new section to the README.rst file, updating the `httpie/sessions.py` file, and adding new tests to the `tests/test_sessions.py` file. The changes aim to improve the handling of cookies in HTTPie sessions and ensure that the correct priority is followed when storing cookies."
543,"West Australia open data


- [X] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [X] My addition is ordered alphabetically
- [X] My submission has a useful description
- [X] The description does not have more than 100 characters
- [X] The description does not end with punctuation
- [X] Each table column is padded with one space on either side
- [X] I have searched the repository for any relevant issues or pull requests
- [X] Any category I am creating has the minimum requirement of 3 items
- [X] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Update to the README.md file: Added a new government open data source, West Australia, to the list of government open data sources."
544,"Fix Rag example docstring
# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



Fixes #7829


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dimiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests), 
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to the it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests? 


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors which may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

 albert, bert, XLM: @LysandreJik 
 GPT2: @LysandreJik, @patrickvonplaten
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @patrickvonplaten, @TevenLeScao
 Blenderbot, Bart, Marian, Pegasus: @sshleifer
 T5: @patrickvonplaten
 Rag: @patrickvonplaten, @lhoestq
 EncoderDecoder: @patrickvonplaten
 Longformer, Reformer: @patrickvonplaten
 TransfoXL, XLNet: @TevenLeScao, @patrickvonplaten
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->
",Change the `generate` method to generate output with `context_input_ids` and `context_attention_mask` instead of `input_ids`.
545,"Remove already migrated cache e2e test
## Describe your changes

This test already got migrated to https://github.com/streamlit/streamlit/blob/develop/e2e_playwright/st_cache_data.py using `cache_data`. The spec also already got deleted, so the test script is not even been run anymore.

---

**Contribution License Agreement**

By submitting this pull request you agree that all contributions to this project are made under the Apache 2.0 license.
","Delete file `e2e/scripts/cached_widget_replay_widget_key.py` and remove its content, including copyright notice, import statement, and functions."
546,"IP address regex updated to not provide a false positive
Updated the IP address regex from `\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b` to `^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$` the latter will not provide a false positive. The old style regex would match anything as long as it's in the format `0.0.0.0` including `999.999.999.999` see [here](https://regex101.com/r/hcQKT9/5) for examples. The newer style regex will not match anything over `255.255.255.255` see [here](https://regex101.com/r/hcQKT9/3) for examples of the new regex.

Basic examples:

```
>>> import re
>>> IP_REGEXS = [r""\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b"", r""^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$""]
>>> IP_ADDRS = [""127.0.0.1"", ""255.255.255.255"", ""999.999.999.999"", ""257.12.34.543"", ""12.09.87.24""]  # Only the first two should match (127.0.0.1, 255.255.255.255)
>>> for item in IP_REGEXS:
	for ip in IP_ADDRS:
		if re.match(item, ip):
			print(""{} matched {}"".format(item, ip))
		else:
			print(""{} did not match {}"".format(item, ip))

			
\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b matched 127.0.0.1
\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b matched 255.255.255.255
\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b matched 999.999.999.999
\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b matched 257.12.34.543
\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b matched 12.09.87.24
^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ matched 127.0.0.1
^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ matched 255.255.255.255
^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ did not match 999.999.999.999
^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ did not match 257.12.34.543
^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ did not match 12.09.87.24  # Did not match because ..09.. is not a valid IP address, valid would just be 9
>>> 
```",Update IP_ADDRESS_REGEX regular expression to match IP addresses in a more strict and accurate way.
547,"bump 290
",Bump version of langchain from 0.0.289 to 0.0.290.
548,"Clear sampler and garbage collect before decoding images to reduce VRAM
## Description

Follow-up to https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12514, more significant VRAM reduction. For a batch size of 4 with hires fix (512x512 -> 1024x1024) this cut off around 2GB of VRAM for me. For a **single** 512x512 @ hr = 2 image (not part of a batch), it cut off around 750MB, and 512x768 @ hr = 2 a bit over 1GB.
This was being done [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/da80d649fd6a6083be02aca5695367bd25abf0d5/modules/processing.py#L1105-L1108) but it can be performed even earlier and should be since decoding the latents is pretty heavy.

~~I hadn't tested this with SDXL yet, not sure if this causes any issue for the refiner. I'm assuming it doesn't but I haven't taken the time to review how that's implemented yet. Probably will reduce VRAM usage for SDXL in general though too if this works as I expect.~~ Occurred to me this is not related to SDXL code at all, so this doesn't affect it.

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Update the `save_intermediate` function to set `self.sampler` to `None` and call `devices.torch_gc()` after applying token merging.
549,"Fixed links to Sphinx docs.
dependent on https://github.com/sphinx-doc/sphinx/pull/4801","Update documentation: removed references to Sphinx-specific markup and reStructuredText Primer, and updated links to Sphinx's built-in markup and Django's extra description units."
550,"hwp
Added hwp reader module. ",Added support for HWP file reader in llama_index.
551,"Fix gan128
This is a fix to GAN128 to patch out the multi gpu until Keras updates their upstream issue (as mentioned in #287 )

In addition, while I was debugging, I fixed the exception handling in train.py so that it properly passes errors that it doesn't catch..",Add check for multiple GPUs and adjust model saving accordingly.
552,"Tests on traceback to 100% coverage
## Type of changes

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [x] Tests
- [ ] Other

## Checklist

- [x] I've run the latest [black](https://github.com/ambv/black) with default args on new code. **But** there was a file (`tests/_card_render.py`) about which black complained, but this one isn't included in the changes.
- [x] I've updated CHANGELOG.md and CONTRIBUTORS.md where appropriate.
- [x] I've added tests for new code.
- [x] I accept that @willmcgugan may be pedantic in the code review.

## Description

Another small contribution to #37 . I am not so sure about the last two tests, because I couldn't exactly pinpoint what the traceback is supposed to do in those cases. 

Also: I added a `no cover` statement. I noticed that when I add a print statement before the `break`, both the `break` and the print are reached according to coverage (which makes sense). When I remove the print, the `break` isn't reached anymore. Not sure what happens there, might be a bug in ~~pytest~~ _coveragepy_?

Happy to make some changes / additions to this PR. Let me know if you need or want it. Thanks!


**Edit:** formatting and small change in wording",Added a comment to the code to indicate that the coverage tool doesn't recognize the code.
553,"Add support for specifying (persistent) id for SVG exports
As part of Airflow's CI we save SVG images of our command's help output and commit them to git.

Without this change a single character change in the help text results in every single class and id in the SVG changing, which makes the diffs unreadable.

This adds an option for us to be able to specify a fixed prefix so that our git diffs are minimized.

(I also tided up the help docstring for export_svg and save_svg which didn't match signature any more)
","Added a new optional parameter `unique_id` to the `export_svg` and `save_svg` methods, which allows the caller to specify a unique ID that is used as the prefix for various elements (CSS styles, node ids) in the generated SVG."
554,"Prevent first part of KoboldAI API responses from getting cut off with --chat enabled
This is related to issue [1053](https://github.com/oobabooga/text-generation-webui/issues/1053)

I'm not sure what the intent of the [len(prompt):] part was, but it causes the first part of API responses to be cut off.",Add conditional logic to handle chat mode in API response.
555,"Not sorting OrderedDict
If the input is already an OrderedDict, its keys were getting sorted (as `isinstance(spaces, dict)` evaluates to `True`), which is usually not desired.",Change the condition in the `__init__` method of `Dict` class to only allow `OrderedDict` when the input is a dictionary.
556,"Add ability to pass `response` to `HTTPError()`
Just a little refactoring, but it seems nicer to me to be able to pass
the response when constructing the `HTTPError` instance instead of
constructing it and then changing the member variable.
",Update HTTPError class to include response object and add test case for HTTPError.
557,"Fixed #33515 -- Prevented recreation of migration for ManyToManyField to lowercased swappable setting.
ticket-33515

Thanks Chris Lee for the report.

Regression in 43289707809c814a70f0db38ca4f82f35f43dbfd.

Refs ticket-23916.","Fixed a regression in Django 4.0.1, where `makemigrations` would generate infinite migrations for a model with a `ManyToManyField` to a lowercased swappable model such as `'auth.user'`."
558,"Fix import ordering s.t. future versions of pylint won't warn on it.
pylint gets more strict about import order in the future, so adjust it now to make upgrading smoother. pylint is following the order from PEP-8:

> Imports should be grouped in the following order:
> 1. standard library imports
> 2. related third party imports
> 3. local application/library specific imports
> 
> You should put a blank line between each group of imports.
",Update imports in multiple files to use consistent import style.
559,"Singly linked list with all operations
","Implementation of singly linked list with methods to add, insert, delete and display nodes."
560,"add vyos action plugin for handling local operations
This plugin extends the functions of the net_config plugin to vyos
","New file: lib/ansible/plugins/action/vyos_config.py, adding Vyos configuration plugin for Ansible."
561,"fsync_dir: remove goto
",Update fsync_dir function to return the result of close call.
562,"[XMinus] Added new extractor
New extractor for x-minus.org as requested in #4300 
",Add XMinusIE extractor to youtube-dl.
563,"Rename GCD File
GCD was wrongly written as ""Greater Common Divisor"", instead of ""Greatest Common Divisor"".","Rename function and variable name from ""Greater Common Divisor"" to ""Greatest Common Divisor"" in maths/greatest_common_divisor.py."
564,"E.30: Remove suggestion to use `throw()`
Removed the suggestion to use `throw()` from E.30 (""Don't use exception specifications""), as it was deprecated by C++11, and is rejected by C++20.",Remove the note about using `throw()` or its equivalent `noexcept`.
565,"[version] bump version to 0.8.9
",Update changelog to reflect new version 0.8.9 and added support for FalkorDB/RedisGraph graph store.
566,"üìù Update `Annotated` notes in `docs/en/docs/tutorial/schema-extra-example.md`
When working on the translation, I noticed that this page is missing the annotated tips that can be found in the rest of the documentation (I checked, and it's the only page where they're missing).",Update documentation to include tips and improved code highlighting.
567,"#N/A: Improve how version is fetched for all shells
üêö‚Ñπ",
568,"Fix docs URL in submit button error message
The warning displayed when a form is missing a submit button contains an incorrect link to the relevant section of our API reference. 

**What it says:**
https://docs.streamlit.io/api.html#form

**What it should say:**
https://docs.streamlit.io/api.html#streamlit.form",Update documentation links for forms from `form` to `streamlit.form`.
569,"Optimized BaseDatabaseSchemaEditor._effective_default().
This eliminates an unneeded call to `datetime.now()` when `field.get_internal_type()` equals `""DateTimeField""`.",Update the handling of auto_now and auto_now_add fields in the _effective_default method to correctly return the current datetime when the field type is DateTimeField.
570,"Added celery worker container to github actions 
Added worker and worker beat to ansible playbook as requested in the discord chat 

@olliestanley Let me know if anything major is missing ",Added support for building and deploying the backend worker and Celery worker.
571,"üíö Disable Gitter notification as it's currently broken
üíö Disable Gitter notification as it's currently broken

...no idea why yet. üòî",Remove the Notify job.
572,"Use name in xyz_grid

**Describe what this pull request is trying to achieve.**
X/Y/Z grid was still using the old hash, prone to collisions. This changes it to use the new `shorthash`, and calculates it if not yet known.

Should fix #10521.

**Additional notes and description of your changes**


Had the same issue as poster of issue #10521, where X/Y/Z grid would sometimes load the wrong model. Found out that in my case it was due to the hashes colliding, as X/Y/Z was using the old hash. This changes to use the new shorthash. Also, the shorthash is calculated if it wasn't known before. No extra work is done since it would otherwise be calculated while loading anyway.

**Environment this was tested in**

Should work on all platforms where loading models by shorthash works.

Tested on:
 - OS: Windows, Linux
 - Browser: Firefox
 - Graphics card: N/A (Nvidia RTX3070)",Change the'sd_model_checkpoint' hash to'sd_model_checkpoint' name in the 'apply_checkpoint' function.
573,"press enter to submit chat prompt
close #2518 ",Update the `ChatForm` component to include a new `handleKeydown` function that triggers the `onSubmit` function when the Enter key is pressed.
574,"Basic documentation for custom_code.py template
Title says it all. Not sure I'm being totally accurate or missing anything else useful, I'm a bit of a novice.",Add new methods to the Script class for custom code execution and display.
575,"Update FAQ to indicate components can go in the sidebar
",Update FAQ documentation to include new information on adding components to the sidebar and layout containers.
576,"Add Pusher's Beam Api
Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [x] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [x] Your additions are ordered alphabetically
- [x] Your submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column should be padded with one space on either side
- [x] You have searched the repository for any relevant issues or pull requests
- [x] Any category you are creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Added new API documentation for Pusher Beams.
577,"fix partition activations issue when mp=2 and pp=2
The issue is caused by the original input tensor not passed from forward to backward, when using ctx.save_for_backward(), it is fixed by saving as a context attribute.
Also adding a new utility function for printing backward tensors, it is useful in debugging this type issues.

Fix #1538",Change the `ctx.saved_tensors` to `ctx.deepspeed_saved_tensors` in the `backward` function and add a new function `print_backward_tensors` to debug the backward tensors.
578,"Test BipedalWalker hardcore kwargs. Refer to #2767
As a continuation of  #2767 I'm adding a test for the argument `hardcore` of BipedalWalker. The `hardcore=False` version create only grass terrain while the `hardcore=True` version create also pit, stumps and ladders.","Add a new test file `test_bipedal_walker.py` in the `tests/envs` directory, which contains a test for the BipedalWalker environment with hardcore creation. The test checks if the terrain has ladders, stumps, and pitfalls when `hardcore=True`."
579,"CLN: Deprecation of assert_
#7131: Run s/self.assert_(/self.assertTrue( on remaining instances I can find. These were the multi-line expressions, and I tried to retain the formatting of the expressions
","### DIFF:Changes to pandas tests for data type conversions, frequency inference, and timezone handling.*   In `test_dict_float`, `test_dict_complex`, and `test_dict_numpy_float`, the assertions were updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_dict_numpy_complex`, the assertion was updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_numpy_array_float` and `test_numpy_array_complex`, the assertions were updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_list_mixed`, the assertion was updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_keys`, `test_repr`, `test_ndim_indexables`, and `test_to_string`, the assertions were updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_apply`, the assertion was updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_infer_freq`, the assertions were updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_append_aware_naive`, the assertions were updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_equal_join_ensure_utc`, the assertion was updated to use `self.assertTrue` instead of `self.assert_`.*   In `test_coercing_dates_outside_of_datetime64_ns_bounds`, the assertion was updated to use `self.assertTrue` instead of `self.assert_`."
580,"Fix for ""Scheduler selection"" #15333
## Description

- Fix ""X/Y/Z plot"" [not working](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15333#issuecomment-2014964014) with ""Schedule type""
- Fix ""Schedule type"" not being saved to ""params.txt""

## Screenshots/videos:
![Screenshot 2024-03-23 004916](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/44464226/840dc222-23a0-438f-85e9-8859c4cbbbf8)


## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
","Update the ""Schedule type"" option to use the list of available schedulers from `sd_schedulers.schedulers`."
581,"fix #4280
This PR should fix #4280 on master, I'll follow up with a test on the sans-io branch.","Update host header in reverse proxy mode to use the upstream server's scheme, host, and port."
582,"[requires.io] dependency update on main branch
",Update types-requests from 2.25.1 to 2.25.2.
583,"[3.1.x] Added admin translatable string.
",Update POT-Creation-Date from 2020-05-11 to 2020-07-14.
584,"Add OpenVINO metadata to export
Write .yaml file automatically when exporting model to openvino to be used during inference

<!--
Thank you for submitting a YOLOv5 üöÄ Pull Request! We want to make contributing to YOLOv5 as easy and transparent as possible. A few tips to get you started:

- Search existing YOLOv5 [PRs](https://github.com/ultralytics/yolov5/pull) to see if a similar PR already exists.
- Link this PR to a YOLOv5 [issue](https://github.com/ultralytics/yolov5/issues) to help us understand what bug fix or feature is being implemented.
- Provide before and after profiling/inference/training results to help us quantify the improvement your PR provides (if applicable).

Please see our ‚úÖ [Contributing Guide](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md) for more details.
-->


## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Improved metadata handling for OpenVINO exports and model class name loading in YOLOv5.

### üìä Key Changes
- üîß Added `yaml` import for metadata handling.
- ‚öôÔ∏è Modified `export_openvino` function to include the model parameter.
- ‚ú® OpenVINO export now saves a `meta.yaml` file with model stride and class names.
- üîç The `__init__` method in `models/common.py` no longer loads class names from `data.yaml` directly.
- üì¶ New `_load_metadata` method in `MultiBackend` class to load metadata from `meta.yaml`.
- üõ†Ô∏è Refactored code related to warmup to ensure GPU model warmup.

### üéØ Purpose & Impact
- üîÑ The changes allow for better integration and metadata management when exporting models to OpenVINO format, enabling users to retain important information about the model such as stride and class names.
- üß† This improves the ease of use and compatibility of YOLOv5 models with the OpenVINO toolkit, potentially broadening the user base involved in deploying YOLOv5 on different platforms.
- üèÉ‚Äç‚ôÇÔ∏è Warmup refactoring ensures that the models are correctly primed for inference, particularly on GPU devices, leading to more efficient operations post-deployment.",### Change metadata.yaml export in OpenVINO export function.
585,"Refactor detect.py arguments
@SkalskiP @KalenMike

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Refactored `detect.py` for readability and default argument setup.

### üìä Key Changes
- üîß Changed `detect` function parameters to have default values, streamlining command-line argument parsing.
- üßπ Cleaned up the argument processing to use function arguments directly, instead of an `opt` object.
- ‚úÇÔ∏è Removed conditional update logic from main execution block to a more appropriate location within the `detect` function.

### üéØ Purpose & Impact
- üìà **Enhanced Readability**: The refactor makes the code easier to read and understand, improving maintainability.
- üõ† **Default Function Parameters**: Users can now call `detect` with or without specific arguments, facilitating easier integration as a module.
- üß© **Modularity**: Direct use of arguments in the `detect` function makes it more self-contained and modular, benefiting users who import it as a library.
- üîÑ **Smoother Updates**: Integrating the update check into the `detect` function simplifies the main code structure and helps in keeping models up-to-date seamlessly.","### DIFF:Update function signature and variable names to match the new arguments.### Description:Update the `detect` function to accept new arguments and update the variable names to match the new arguments. The function now accepts `weights`, `source`, `imgsz`, `conf_thres`, `iou_thres`, `max_det`, `device`, `view_img`, `save_txt`, `save_conf`, `save_crop`, `nosave`, `classes`, `agnostic_nms`, `augment`, `update`, `project`, `name`, `exist_ok`, and `line_thickness`. The function now uses these new arguments to update the model, source, and other variables."
586,":globe_with_meridians: Add Portuguese translation for `docs/pt/docs/advanced/templates.md`
Added Portuguese translation to `templates.md`",Create a new template engine for FastAPI using Jinja2.
587,"Update README.md
new img payload","Update to the README.md file in the XSS Injection project, adding a new example of a blind XSS attack payload."
588,"DOC: Fix typo in docstring of DataFrame.memory_usage 
Cherry picked commit from #25761 and removed trailing whitespace.

@jorisvandenbossche if you don't mind taking a look",Update documentation for `memory_usage` method to improve clarity and consistency.
589,"Created test automation software for the XORCipher created by Christi‚Ä¶
‚Ä¶an Bender.",Test automation software for XORCipher created by Christian Bender.
590,"Changed 'robust' to 'scalable' because...
The sentence implied that SQLite isn't robust which is misleading since it is extremely robust. I think the right term is scalable.
",Update tutorial text to recommend a more scalable database like PostgreSQL instead of a more robust one.
591,"Change to shorter form code example for runserver
Change to shorter form code example for runserver, and specify that this is necessary for Vagrant users.",Improved documentation for the development server.
592,"Correct parameters of Conv2D layer
Changed border_mode='same' to padding='same' in the example to match it with the parameters of Conv2D.",Update the `border_mode` parameter to `padding` in the `Conv2D` layer.
593,"Fix typo in comments
Looks like it was added in (#18533)

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).
","Change the comment in two scripts to fix a typo in ""verriding"" to ""overriding""."
594,"Correct ""bug,"" typo to ""bug"", in README.md
","Update to reporting issues with the model, adding a comma after ""bug""."
595,"Fix some typos
`reponse` -> `response`","Update documentation: Fix typos in ""Reponse"" and ""Response"" in developer-tools.rst and downloader-middleware.rst."
596,"Code cleanup scrapy.utils.python.WeakKeyCache #4684
Resolves #4684",Deprecation warning added for the WeakKeyCache class.
597,"Update link to the Server forum category
Let's Encrypt [closed it](https://community.letsencrypt.org/t/closing-the-server-category/93016) in favor of the Help category.

## Pull Request Checklist

- [x] Edit the `master` section of `CHANGELOG.md` to include a description of
  the change being made.
- [x] Add [mypy type
  annotations](https://certbot.eff.org/docs/contributing.html#mypy-type-annotations)
  for any functions that were added or modified.
- [ ] Include your name in `AUTHORS.md` if you like.","Update changelog to link to Help category in Let's Encrypt forum instead of Server category. Also, update error message in client.py to link to Help category."
598,"Verify both symlink and target
fixes #2054 and supersedes #2088
","Add a new method to get the target of a symbolic link. This method checks if the target is absolute, and if not, joins it with the directory of the link."
599,"Add sfc-gh-mnowotka as a protobuf CODEOWNER
Protobuf changes are common, and we'd like the Warsaw team to be able to approve them",Update CODEOWNERS file to add sfc-gh-mnowotka to the list of owners for the proto directory.
600,"Add DevContainer Support
### Describe the changes you have made:
For users who have difficulties with local installation, they can skip the local installation step, only need to visit our repository's GitHub page and press the `,` key, and they can directly create a Codespace virtual container. The container comes pre-installed with open-interpreter and includes the vscode interface. Users can seamlessly start communicating with open-interpreter.

You can now give it a try:
Visit https://github.com/weihongliang233/open-interpreter-devcontainer, press `,` on the webpage to create a codespace, and then you can start communicating with open-interpreter!

More details:
[DevContainer and Codespaces](https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/adding-a-dev-container-configuration/introduction-to-dev-containers)

### Reference any relevant issues (e.g. ""Fixes #000""):

### Pre-Submission Checklist (optional but appreciated):

- [x] I have included relevant documentation updates (stored in /docs)
- [x] I have read `docs/CONTRIBUTING.md`
- [x] I have read `docs/ROADMAP.md`

### OS Tests (optional but appreciated):

- [ ] Tested on Windows
- [ ] Tested on MacOS
- [x] Tested on Linux
",Changes to the repository:* Added a `.devcontainer/DockerFile` and a `.devcontainer/devcontainer.json` file to set up a development container for the project.* Updated the `README.md` file to include information about GitHub Codespaces and how to use them.* Added a `docs/getting-started/setup.mdx` file to provide instructions on how to set up the project.* Added a section to the `README.md` file comparing the project to ChatGPT's Code Interpreter.
601,"ref(custom-scm): Add to external actor provider choices
Lots of context in https://github.com/getsentry/sentry/pull/25789 but we are basically adding another integration that will be used for codeowners. 

In order for someone to be able to use this integration with codeowners they need to set up their external users and teams which is why I'm adding this as another provider for the `ExternalActor` table. ",Add custom SCM provider to ExternalActor model.
602,"Fix minor formatting issues
","Update documentation: Changed definition of ""concept"" from ""(1) a notion, and idea; (2) a set of requirements, usually for a template argument"" to ""(1) a notion, and idea; (2) a set of requirements, usually for a template argument."""
603,"small fix for docker cmd. tools.py->faceswap.py
","Update command in docker_cuda method from ""docker exec deepfakes-gpu python /srv/tools.py gui"" to ""docker exec deepfakes-gpu python /srv/faceswap.py gui""."
604,"Replace single element slice by next() in hive provider
<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 ""License""); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
 -->

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of an existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->




---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).
",Change the list comprehension for extracting partition values from `list(p.values())[0]` to `next(iter(p.values()))`.
605,"the cancelOrder call doesn't have the required parameter fund_id
The cancelOrder call to privateDeleteFundsFundIdOrdersId needs to pass the required fund_id parameter, which is missing...",Add 'fund_id' parameter to privateDeleteFundsFundIdOrdersId method.
606,"fix NL.9 ALL_CAPS does not apply to non-macro symbolic constants
",Update note about non-macro symbolic constants to include a reference to the rule about using ALL_CAPS for enumerators.
607,"üåê Add Chinese translation for `docs/zh/docs/deployment/deta.md`
as title","### Create a free Deta account and deploy a FastAPI applicationThis pull request adds a new documentation page for deploying a FastAPI application on Deta. The page provides a step-by-step guide on how to create a free Deta account, install the Deta CLI, and deploy a FastAPI application. The documentation includes code examples, screenshots, and tips for troubleshooting common issues."
608,"Implement general forward method for all method in built-in lora ext
## Description
For efficiency in lora-block-weight extension. We want to use old lora implementation (the bypass-forward one). But LyCORIS method doesn't support it at all.

Since not all the lyco method can implement efficient forward method. (And some of it is super tricky)
I decide to add general forward at first so at least it will not cause any error and still give out reasonable result.

Method specific implementation (for OFT and LoKr and GLoRA) will in the future PR.

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Add support for PyTorch's nn.Module and its subclasses.
609,"Fix: Adding an LLM param to fix generator producing only ""#####"" characters
The newer versions of `llama-cpp-python` do not have the parameter `offload_kqv` set to True. Fixing this in privateGPT is pretty straightforward. Instead of installing a lower version of `llama-cpp-python`, adding `""offload_kqv"": True` to `model_kwargs` enables KGV offloading, and also significantly boosts GPU performance. ",Update model_kwargs to include offload_kqv=True.
610,"add query bundle to node postprocessor call
# Description

Some node-postprocessors require the query bundle, we should be passing that in for users.

## Type of Change

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

- [x] I stared at the code and made sure it makes sense
",Update CHANGELOG.md to include new unreleased changes and bug fix for passing query bundle to node postprocessors.
611,"fix for invalid avatarUrl error
fix for https://github.com/LAION-AI/Open-Assistant/issues/1985

If the avatarUrl ends up being invalid, it will use the same default value as if the avatarUrl is missing. 

I tried to fix it without useState but it caused flickering as Next.js kept trying to fetch the invalid url 


",Add support for dynamic avatar URLs using React's useState hook and update the avatar URL fetching logic.
612,"fix link
## Type of changes

- [X] Bug fix
- [ ] New feature
- [ ] Documentation / docstrings
- [ ] Tests
- [ ] Other

## Description

I just noticed that somehow a merge in my PR #1920 broke the link of a contributor. Not sure how it happened, but here's the fix.",Update contributor list: Patrick Arminio's GitHub handle changed from `patrick9` to `patrick91`.
613,"Bug fixes for GCP (as of 2019-08-19T06:07:37+00:00)
##### SUMMARY
Bug fixes for GCP (as of 2019-08-19T06:07:37+00:00)
This PR is the result of the following Magic Modules PRs:
  - [Fix for SQL database import formats](https://github.com/GoogleCloudPlatform/magic-modules/pull/2199)
  - [ML Engine Models in Terraform](https://github.com/GoogleCloudPlatform/magic-modules/pull/2071)

##### ISSUE TYPE
- Bugfix Pull Request

##### ADDITIONAL INFORMATION
```
This was generated by [Magic Modules](https://github.com/googlecloudplatform/magic-modules)
```",Added descriptions for `isDefault` and updated descriptions for `charset` and `collation` to include information about supported values and links to relevant documentation.
614,"fix bug
fix load_dygraph_params",Update the check for the existence of the pretrained model's parameters file to also check for the existence of the file itself.
615,"Add support for QLoRA
## Why are these changes needed?
This PR supports [QLoRA ](https://github.com/artidoro/qlora)which is a new parameter-efficient fine-tuning method that loads the model backbone method in 4bit. This will lead to a 13B model taking about 10GB and a 7B model taking about 5GB of memory. One can train in the same way as before except add an argument `--q_lora True` to your CLI command.

Notes about implementation:
- Had to comment out `torch_dtype` in `apply_lora` to make merging the base model with Lora weights work. [Reference to Solution](https://github.com/tloen/alpaca-lora/issues/497)
- **Deepspeed3 Incompatibility:** I believe that DeepSpeed3 does not work with QLoRA, since it isn't possible to load the model in a mixed-int format while using stage 3, but I could be wrong. I added a warning to the code for that. [Reference](https://github.com/huggingface/transformers/blob/8f093fb799246f7dd9104ff44728da0c53a9f67a/src/transformers/modeling_utils.py#L2226-L2229). I also tried to train without explicitly naming a device map such as `device_map = None` but that led to tensors being on different devices error.
- **FSDP Incompatability**: Running the finetuning using FSDP will lead to an error: ""ValueError: `FlatParameter` requires uniform `requires_grad`"", which is due to how FSDP is constructed in the current pytorch-stable build. However, the nightly version does support it if we set `use_orig_params=True` as reported [here](https://github.com/pytorch/pytorch/issues/100945), but the memory savings may not be high.


Reference Deepspeed Stage 2 `ds_config.json`:
```
{
    ""zero_optimization"": {
      ""stage"": 2,
      ""offload_optimizer"": {
              ""device"": ""cpu""
      },
      ""contiguous_gradients"": true,
      ""overlap_comm"": true
    },
    ""optimizer"": {
      ""type"": ""AdamW"",
      ""params"": {
          ""lr"": ""auto"",
          ""betas"": ""auto"",
          ""eps"": ""auto"",
          ""weight_decay"": ""auto""
      }
  },
    ""train_micro_batch_size_per_gpu"": ""auto"",
    ""gradient_accumulation_steps"": ""auto""
  }
```

## Related issue number (if applicable)
#1657 

## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed.
- [ ] I've made sure the relevant tests are passing (if applicable).

## Who Could Review This? 
@DachengLi1 
",Change the quantization configuration for BitsAndBytesConfig.
616,"Fix INTERNAL ERROR caused by removing parens from pointless string
Fixes #1846.","Improved handling of ""pointless"" strings in StringParenStripper."
617,"Add CI for Intel XPU/Max1100
Add basic workflow for tests on intel xpu. Currently we have part of tests enabled. We will add more tests in later PRs.",Update the workflow to support XPU-based testing.
618,"Add 'one' initialization to docs
Initialization with one was missing from the docs.",Add new initialization option: one.
619,"Update Twitter in Community Section
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create links to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Towards #21236 
 
#### What does this implement/fix? Explain your changes.
Adding a Twitter link to the Community section on the homepage of the Scikit-learn website (https://scikit-learn.org/stable/) as mentioned in the issue.



#### Any other comments?
I have not added the LinkedIn link, since it was mentioned by @adrinjalali that LinkedIn is not being maintained at the moment.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Add Twitter link to community section.
620,"[nightly] Add staging nightly test for gcs ha




## Why are these changes needed?
This PR adds four staging nightly tests for gcs :
- many_actors
- many_tasks
- many_pgs
- many_nodes

These are benchmark tests that are highly related to gcs ha. 

To make it easier to add tests, this PR also change e2e.py a little bit to include testing flags to app config.




## Related issue number



## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Add new tests for Redis HA scenarios.
621,"[MRG+1] Adding objective function in Ridge regression docstring
#### What does this implement/fix? Explain your changes.
This adds the optimization criterion of Ridge regression inside the dosctring itself (as in Lasso and other cases).
This clarifies that the normalization is 1 / 2  for Ridge, while it is 1 / (2 * n_samples) for Lasso.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Update documentation for Ridge regression model to include the objective function.
622,"Add file_size() function


## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Enhanced export functionality with file size feedback for model exports.

### üìä Key Changes
- Added `file_size` function to `utils/general.py` to calculate file sizes.
- Modified `models/export.py` to print out file size information when saving models.

### üéØ Purpose & Impact
- The addition of file size output helps users to be more informed about the storage requirements of their exported models.
- Users can now see at a glance how much disk space each exported model file consumes, useful for deploying models to environments with limited storage. üìâüõ†Ô∏è",Update file size calculation in export.py and general.py.
623,"Commented doctests that were causing slowness at Travis. #2
",Commented out code for faster execution on Travis.
624,"Cleanup leftover boto2 code in S3DownloaderHandler
Remove leftover attribute `S3DownloaderHandler.conn`. Seems we forgot to remove it in 5e99758.",Remove unnecessary import and comment.
625,"DEPR: treating object-dtype blocks as bool_only
- [ ] closes #xxxx (Replace xxxx with the Github issue number)
- [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature
- [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).
- [x] Added an entry in the latest `doc/source/whatsnew/vX.X.X.rst` file if fixing a bug or adding a new feature.
","Deprecation of treating all-bool object-dtype columns as bool-like in `DataFrame.any` and `DataFrame.all` with `bool_only=True`, explicitly casting to bool instead."
626,"have letsencrypt.VERSION, show it in letsencrypt --help, use it in setup.py
note: we had some discussion about potential problems importing VERSION from main package.

SO link: http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package

See also my comment in **init**.py - maybe we can add that ""version detection from git tags"" magic later.
",Update version number in setup.py and README.rst/CHANGES.rst files.
627,"Add download ca.cert link in status page
1. Add download ‚Äòca.cert' link
2. Translate it by google
","Update translations for fa_IR and zh_CN languages, adding ""Download"" option."
628,"Fix: There is no current event loop in thread.
The error you are encountering is related to attempting to use the default asyncio event loop within a non-main thread. asyncio expects you to use the default event loop only in the main thread.

To resolve this issue, asyncio.new_event_loop() was used instead of asyncio.get_event_loop() to create a new event loop for the current thread.",Update the `run` function to use a new event loop and set it as the current event loop.
629,"MediaPipeline: log media_to_download errors before stripping them
Fixes #5067

To do:

- [x] Make sure that existing tests pass
- [x] Add a test for the issue being fixed here",Changes to Scrapy pipelines and tests:* Added error handling in `_process_request` method of `media.py` pipeline to log exceptions.* Added `_log_exception` method to log exceptions in `_process_request` method.* Updated `_modify_media_request` method to set `handle_httpstatus_list` meta value.* Added test cases for media download pipeline in `test_pipeline_crawl.py`:	+ Test for downloading media files with redirected URLs.	+ Test for error handling in `file_path` method of media pipeline.
630,"[WIP] Fix broken unit tests.
### Describe the changes you have made:
Some of the unit tests are broken, which makes the CI pipeline fails every time. Broken tests are meaningless because they can't locate the real problems.

### Reference any relevant issues (e.g. ""Fixes #000""):
Fixes #980

### Pre-Submission Checklist (optional but appreciated):

- [x] I have included relevant documentation updates (stored in /docs)
- [x] I have read `docs/CONTRIBUTING.md`
- [x] I have read `docs/ROADMAP.md`

### OS Tests (optional but appreciated):

- [ ] Tested on Windows
- [x] Tested on MacOS
- [ ] Tested on Linux
",Remove assertion for user message in test_hello_world.
631,"fix: docs/intro/tutorial.rst checks
Fix docs/intro/tutorial.rst checks:

command:
```
pytest docs
```

output:
```
FAILED docs/intro/tutorial.rst::line:254,column:1
FAILED docs/intro/tutorial.rst::line:350,column:1
FAILED docs/intro/tutorial.rst::line:412,column:1
```","Update documentation and configuration files to use ""query"" instead of ""xpath"" in Scrapy selectors."
632,"settings: button press states
",Change the styles of buttons in the settings window and button control.
633,"docs: update ""pad_sequences()"" parameters in the docs
Updated the incomplete documentation of `pad-sequences()` function. It lacked the description of a few important parameters
","Add new parameters `__padding__`, `__truncating__`, and `__value__` to the `sequence` transformation."
634,"removed extra 'all' from comments
<!-- Thanks for contributing to manim!
    Please ensure that your pull request works with the latest version of manim.
-->

## Motivation

fixed typo in comments
## Proposed changes

- remove an extra 'all'
- 
- 

## Test

**Code**:

**Result**:","Update comment in `construct` method: ""On all frames"" instead of ""On all all frames""."
635,"Added type hints
related to #373

- Added type hint.
- Change string literal to class name according to [PEP-0563#forward-references](https://www.python.org/dev/peps/pep-0563/#forward-references)

- [x] doctest Passed
- [x] mypy Passed",### Changes in Behavioral Patterns* Improved type hints and annotations in `chaining_method.py` and `iterator_alt.py` for better code readability and maintainability.* Added a `from __future__ import annotations` statement in `chaining_method.py` and `iterator_alt.py` to enable the use of type hints and annotations.* Improved the `update` method in `observer.py` to correctly update the observers when the subject changes.* Added a `modifier` parameter to the `notify` method in `observer.py` to allow for more flexibility in the notification process.* Improved the `scan` method in `state.py` to correctly display the current station being scanned.* Added a `private_name` attribute to the `DiscountStrategyValidator` class in `strategy.py` to store the private name of the attribute.* Improved the `clone` method in `prototype.py` to correctly clone the prototype and update the inner attributes dictionary.* Added a `get_objects` method to the `PrototypeDispatcher` class in `prototype.py` to retrieve all objects.* Improved the `__getattr__` method in `delegation_pattern.py` to correctly delegate the attribute to the delegate object.
636,"[pre-commit.ci] pre-commit autoupdate

updates:
- [github.com/pre-commit/mirrors-prettier: v3.0.0 ‚Üí v3.0.1](https://github.com/pre-commit/mirrors-prettier/compare/v3.0.0...v3.0.1)
",Update prettier version from v3.0.0 to v3.0.1.
637,"[Bot] Synchronize Submodule References
Automated PR to update submodule commits",Update subproject commit to b7be3b9076b820d0aed64c9b5e640f6105e07809
638,"[3.6] bpo-9842: Add references for using ""..."" as a placeholder to the index. (GH-10330)
(cherry picked from commit 6c48bf2d9e1e18dfbfa35f7582ddd32f11f75129)


Co-authored-by: Serhiy Storchaka 


https://bugs.python.org/issue9842

","Update documentation for pprint, reprlib, and textwrap modules."
639,"Add Ambrosia
Add Ambrosia, a tool for cleaning text datasets.",Update README.md: added Ambrosia to the list of recommended tools.
640,"re-add deprecated `Config.from_json` method
This was removed accidentally, it's deprecated until Flask 2.1.

- fixes #4078 

<!--
Ensure each step in CONTRIBUTING.rst is complete by adding an ""x"" to
each box below.

If only docs were changed, these aren't relevant and can be removed.
-->

Checklist:

- [x] Add tests that demonstrate the correct behavior of the change. Tests should fail without the change.
- [x] Add or update relevant docs, in the docs folder and in code.
- [x] Add an entry in `CHANGES.rst` summarizing the change and linking to the issue.
- [x] Add `.. versionchanged::` entries in any relevant code docs.
- [x] Run `pre-commit` hooks and fix any issues.
- [x] Run `pytest` and `tox`, no tests failed.
",Deprecation of `Config.from_json` and addition of a warning to encourage the use of `Config.from_file` instead.
641,"Fixed #27195 -- Stopped DROPing DEFAULT on NULLable column addition.
Thanks Rob Golding from Zapier for the report.
",Update the `skip_default` method to use `effective_default` when dropping the default value for a field.
642,"Fix clip skip of 1 not being restored from prompts
Closes #4713, #4359",Add a default value for 'CLIP stop at last layers' and update the description of 'CLIP skip' in generation parameters.
643,"[CozyTV] Add extractor
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

CozyTV (https://cozy.tv/) is a live streaming site for american neo-nazis, criminals, ""terrorists"" and ""degenerates"".

",Added new extractor for CozyTV and updated the list of available extractors.
644,"update the Go section
- A lot of Go packages are missing from the list; I updated it based on the references found on gopherdata. 
- Gorgonia was mistakenly added in the python section; I moved it back to the Go section. 
- I also reorganized the list to put deprecated packages at the bottom.
- Some links were not working; I corrected this and checked that they all work now.",
645,"Bugfix: spaces.MultiDiscrete.contains() can handle different shapes
See also spaces.Box.contains(), it already has this change

Reproduce Error with following lines:

from gym import spaces
import numpy as np
space_Multidiscrete = spaces.MultiDiscrete(np.ones((10,3)))
sample_test = np.ones((9,3))
space_Multidiscrete.contains(sample_test) #errors: ValueError: operands could not be broadcast together with shapes (9,3) (10,3)",Change condition in `contains` method from `x.shape == self.shape and (0 <= x).all() and (x < self.nvec).all()` to `return (0 <= x).all() and (x < self.nvec).all()`
646,"Simplify testing of RHEL 8
Fixes https://github.com/certbot/certbot/issues/7301.",Update the version check for RHEL 8 in test_leauto_upgrades.sh.
647,"[ie/floatplane] Add extractor
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

Alternative implementation to #5912, supporting audio entries and multiple videos in a single post.
Most posts on the site only contain a single video; thus I decided to only return a single video instead of a playlist in those cases and use the post metadata for that video. This is also how the site visually represents it.
Posts with multiple videos/audio files get returned as a playlist.

Fixes #5877, Closes #5912


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [x] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))


","### Description of Pull Request:This pull request adds support for extracting video metadata and streams from Floatplane, a video sharing platform. The changes include:* Adding a new extractor, `FloatplaneIE`, to extract video metadata and streams from Floatplane.* Implementing the `_real_extract` method to download video metadata and streams.* Adding support for extracting video formats, thumbnails, and descriptions.* Implementing the `_fetch_page` method to fetch multiple pages of video data.* Adding support for extracting video playlists and channels.* Updating the `FloatplaneChannelIE` extractor to extract channel metadata and playlists.* Adding tests for the new extractors."
648,"image_test cleanup
Minor refactor: move various `st.image` tests out of `streamlit_test.py` and into `image_test.py`. No behavior changes.",Changes to image tests in Streamlit.
649,"install.sh: sudo apt-get install python3-pip
Replaces #219",Update the installation script to install python3-pip instead of python-pip.
650,"Fix #2448: Improve verbosity options
- [x] Improve verbosity options, change it to `str` type.",### Change log verbosity options to be more explicit and user-friendly.The changes made to the log verbosity options in the `mitmproxy` codebase aim to make them more explicit and user-friendly. The options are now more descriptive and provide a clearer indication of what each level of verbosity entails. This change should make it easier for users to understand and configure the log verbosity settings in their `mitmproxy` instances.
651,"Adding bqplot to the data visualization section
## What is this Python project?

bqplot is one of the leading libraries in using the Jupyter widgets infrastructure to enable seamless interactive plotting!

## What's the difference between this Python project and similar ones?

bqplot is the first library to provide seamless bi-directional communication between the JavaScript and Python. Any plot that is generated is a full web application, which can be controlled by Python interactively.
--

Anyone who agrees with this pull request could vote for it by adding a :+1: to it, and usually, the maintainer will merge it when votes reach **20**.
","Added new link to README.md: bqplot, an interactive plotting library for the Jupyter Notebook."
652,"Add support for different TFDS `BuilderConfig`s
As of now a user can't use a TFDS `BuilderConfig` different than the default one: the current parser assumes that the dataset is in the form `{torch,tfds,...}/dataset`, while in the case of TFDS datasets it can be `tfds/dataset/builder-type`, e.g. `tfds/diabetic_retinopathy_detection/btgraham-300` (as in [here](https://www.tensorflow.org/datasets/catalog/diabetic_retinopathy_detection#diabetic_retinopathy_detectionbtgraham-300)).

This PR fixes this by taking whatever is split at line 10 after the first element (which is the source of the dataset) and joining it again with `/`.

In the case there is just one `/`, then `name[1:]` will contain just one element, e.g., if we pass `tfds/imagenet2012`, then `name[1:] == [""imagenet2012""]` and `""/"".join(name[1:]) == ""imagenet2012""`, leaving the current behavior unchanged. If instead there are multiple `/` (e.g. `""tfds/diabetic_retinopathy_detection/btgraham-300""`), then `name[1:] == [""diabetic_retinopathy_detection"", ""btgraham-300""]`, and `""/"".join(name[1:]) == ""diabetic_retinopathy_detection/btgraham-300""`, which is compatible with the `tfds` format.",Update the name variable to join the list of names with '/' instead of taking the last element.
653,"Docs: clarify fmt:on/off requirements
<!-- Hello! Thanks for submitting a PR. To help make things go a bit more
     smoothly we would appreciate that you go through this template. -->

### Description
A small clarification about our requirements for `# fmt: on/off`, prompted by #2985 (does not fix).

### Checklist - did you ...

- [x] N/A Add a CHANGELOG entry if necessary?
- [x] N/A Add / update tests if necessary?
- [x] Add new / update outdated documentation?
",Update documentation for _Black_ code style to reflect changes in formatting rules.
654,"Fix Debian dependencies, and add the required openssl header file
symlink fix. Restructure _deb_common.sh to optionally install
virtualenv.
",Update Vagrantfile to install git-core and add a comment explaining the requirement.
655,"Adding type hints to RedBlackTree
### **Describe your change:**



* [ ] Add an algorithm?
* [x] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### **Checklist:**
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [ ] All new Python files are placed inside an existing directory.
* [ ] All filenames are in all lowercase characters with no spaces or dashes.
* [ ] All functions and variable names follow Python naming conventions.
* [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [ ] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [ ] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
","### DIFF:Changes to the RedBlackTree class in the data_structures/binary_tree/red_black_tree.py file.The changes include:* Added type hints for function parameters and return types.* Added a `__repr__` method to provide a string representation of the tree.* Added a `__eq__` method to compare two trees for equality.* Added a `__bool__` method to return a boolean value indicating whether the tree is empty.* Added a `__len__` method to return the number of nodes in the tree.* Added a `preorder_traverse`, `inorder_traverse`, and `postorder_traverse` method to traverse the tree in different orders.* Added a `color` function to return the color of a node.* Added a `test_rotations`, `test_insert`, `test_insert_and_search`, `test_insert_delete`, `test_floor_ceil`, `test_min_max`, and `test_tree_traversal` function to test the tree's functionality.* Added a `test_tree_chaining` function to test the tree's chaining functionality.* Added a `pytests` function to run all the tests.* Added a `main` function to run the tests.These changes improve the functionality and usability of the RedBlackTree class, and provide a more comprehensive testing framework."
656,"Make DefaultDataCollator importable from root
This is a small PR to fix an oversight - the DefaultDataCollator class was not importable from root (this is separate from the `default_data_collator` function). It also adds some missing docstring arguments, and the missing docstring for DefaultDataCollator.","Added documentation for DefaultDataCollator, DataCollatorWithPadding, DataCollatorForTokenClassification, DataCollatorForSeq2Seq, and DataCollatorForLanguageModeling."
657,"pass sys.argv to flask cli
closes #3297 

Can be removed when pallets/click#536 is fixed.",Update version number from 1.1.1 to 1.1.2.dev in `__init__.py` and add a new version 1.1.2 in `CHANGES.rst`.
658,"[3.12] gh-107226: PyModule_AddObjectRef() should only be in the limited API 3.10 (GH-107227)
(cherry picked from commit 698b01513550798886add5e06a1c3f9a89d7dfc6)

Co-authored-by: Serhiy Storchaka 


* Issue: gh-107226

",Addition of `PyModule_AddObjectRef` function with limited API availability and documentation update for `c:func:`PyModule_AddObjectRef` in NEWS file.
659,"Bitstamp: add new exception for ""Invalid offset.""
This happens when you send:

```
fetch:
 bitstamp POST https://www.bitstamp.net/api/v2/user_transactions/
body: ""limit=1000&offset="" 
```

Maybe we also need to add a case in the code, when offset is given in the params, then replace it with ""0"" when its empty.

This PR gives at least a proper Error (BadRequest) instead of ExchangeError.","Update error handling in bitstamp.js to include a new ""BadRequest"" error type."
660,"Add SnakeViz to the list
Hi,

Thank you for this awesome and useful list!
","Added a new link to the ""A curated list of awesome Python frameworks, libraries and software"" in README.md. The new link is to SnakeViz, a browser based graphical viewer for the output of Python's cProfile module."
661,"BUG: AttributeError when read_sas used with GCS
With GCSFS (and when a binary file buffer is passed in), the output from
the file read is `bytes`, not a Unicode `str`. The `encode` call is unnecessary
in this case.

- [x] closes #33069 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
","Fixed bugs in `read_csv` and `read_sas` methods, including issues with blank lines, permissions, and attribute errors when reading files from Google Cloud Storage."
662,"Avoid deadlock for unsynchronized non-zero checkpointing
For non-ZeRO checkpointing, we should not synchronize all ranks at checkpoint. This way we avoid deadlocks.",Change the logic for creating checkpoint files to ensure they are created without collision in parallel.
663,"Ford: add Escape test route and remove dashcam only
","### DIFF:The following changes were made to the CARS.md file:* Four new supported cars were added: Ford Escape Hybrid 2020, Ford Escape Plug-in Hybrid 2021-22, Ford Kuga 2020-22, and Ford Kuga Plug-in Hybrid 2020-22.* The list of supported cars was updated to include these new additions.* The interface.py file was updated to include the new cars.* The values.py file was updated to include the new cars.* The routes.py file was updated to include a new test route for the Ford Escape MK4.### Description:This pull request adds support for four new cars: Ford Escape Hybrid 2020, Ford Escape Plug-in Hybrid 2021-22, Ford Kuga 2020-22, and Ford Kuga Plug-in Hybrid 2020-22. The changes include updates to the CARS.md file, the interface.py file, the values.py file, and the routes.py file."
664,"Fix a failing test on macOS Mojave
Apparently macOS Mojave allows non-root users to bind ports < 1024. Similar reports:

* https://github.com/nodejs/node/pull/23550/files
* https://news.ycombinator.com/item?id=18302380",Add a skip condition for macOS Mojave in the test_err method.
665,"Ford: handle metric cruise speed (v2)
**Description**

I found a signal which appears to match the IPC ""Show km/h"" setting.
Requires commaai/opendbc#1010. Previous attempt: https://github.com/commaai/openpilot/pull/29526.

Closes #29494.

**Verification**

- [x] Test in car and confirm that toggling the ""Show km/h"" setting does not result in the cruise speed shown in openpilot being incorrect.  Started in kph: `e886087f430e7fe7/2024-02-14--19-23-28`, in mph: `e886087f430e7fe7/2024-02-14--19-27-31`
- [x] Test in a non-English (metric) car: `32a35015aa4d5598/2024-03-03--14-58-48`





<!--- ***** Template: Fingerprint *****

**Car**
Which car (make, model, year) this fingerprint is for

**Route**
A route with the fingerprint

-->

<!--- ***** Template: Car Bugfix *****

**Description**

A description of the bug and the fix. Also link the issue if it exists. 

**Verification**

Explain how you tested this bug fix. 

**Route**

Route: [a route with the bug fix]


-->

<!--- ***** Template: Bugfix *****

**Description**

A description of the bug and the fix. Also link the issue if it exists. 

**Verification**

Explain how you tested this bug fix. 


-->

<!--- ***** Template: Car Port *****

**Checklist**

- [ ] added entry to CarInfo in selfdrive/car/*/values.py and ran `selfdrive/car/docs.py` to generate new docs
- [ ] test route added to [routes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/car/tests/routes.py)
- [ ] route with openpilot:
- [ ] route with stock system:
- [ ] car harness used (if comma doesn't sell it, put N/A):


-->

<!--- ***** Template: Refactor *****

**Description**

A description of the refactor, including the goals it accomplishes. 

**Verification**

Explain how you tested the refactor for regressions. 


-->

",Update calculation of speed in `carstate.py` to use metric units based on vehicle settings.
666,"Numeric Password Cracker
Description:
**The Numeric Password Cracker is a memory-efficient Python script designed to find numeric passwords using permutation generation. The program utilizes generators from the itertools library, which allows it to generate password permutations on-the-fly without storing them in memory. This approach significantly reduces memory consumption, making the script efficient even for large password lengths.**

**_Advantages and Positive Points:_**
1. Memory-Efficient: By using generators, the program generates password permutations on-the-fly, minimizing memory usage. This enables it to handle long password lengths without excessive memory consumption.
2. Fast Execution: The memory-efficient design also contributes to faster execution times, as the program does not need to store and manage large datasets in memory.
3. Scalability: Due to its memory efficiency, the Numeric Password Cracker can efficiently process a wide range of password lengths, accommodating various use cases.
4. Versatility: Users can easily customize the maximum password length to be checked, allowing the program to adapt to different target passwords.
5. Non-Destructive: The script operates in a read-only manner, making it non-destructive and safe to use in testing and educational scenarios.
6. Open-Source: As an open-source tool, the Numeric Password Cracker encourages collaboration, code review, and contributions from the developer community.
7. Educational Value: The program serves as an educational resource for password security, algorithms, and Python's generator concept.
8. Ethical Approach: The script emphasizes responsible and ethical use, discouraging any illegal or unauthorized activities.

Note: While the Numeric Password Cracker is efficient and useful for legitimate purposes such as educational learning and security assessments, it must never be used for illegal or unauthorized activities. Always ensure proper authorization before attempting to crack passwords or access systems.","Update README.md to improve documentation and add new file numeric_password_cracker.py, a Python script to crack numeric passwords."
667,"[shardformer] Add layernorm
## üìå Checklist before creating the PR

- [x] I have created an issue for this PR for traceability
- [x] The title follows the standard format: `[doc/gemini/tensor/...]: A concise description`
- [x] I have added relevant tags if possible for us to better distinguish different PRs


## üö® Issue number

> Link this PR to your issue with words like fixed to automatically close the linked issue upon merge
>
> e.g. `fixed #1234`, `closed #1234`, `resolved #1234`
fixed #4071 


## üìù What does this PR do?

> Summarize your work here.
> if you have any plots/diagrams/screenshots/tables, please attach them here.
add layernorm and fit it to all supported models


## üí• Checklist before requesting a review

- [x] I have linked my PR to an issue ([instruction](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue))
- [x] My issue clearly describes the problem/feature/proposal, with diagrams/charts/table/code if possible
- [x] I have performed a self-review of my code
- [x] I have added thorough tests.
- [x] I have added docstrings for all the functions/methods I implemented

## ‚≠êÔ∏è Do you enjoy contributing to Colossal-AI?

- [x] üåù Yes, I do.
- [ ] üåö No, I don't.

Tell us more if you don't enjoy contributing to Colossal-AI.
","### DIFF:Changes to colossalai/shardformer/layer/layernorm.py:* Added support for `LayerNorm1D` in `ShardFormer`.* Implemented `LayerNorm1D` as a parallel module.* Added support for `FusedLayerNorm` in `ShardFormer`.* Implemented `FusedLayerNorm` as a parallel module.Changes to colossalai/shardformer/shard/shard_config.py:* Added `use_mixedfusedLN` parameter to `ShardConfig`.* Set `fused_layernorm` to `True` by default.Changes to colossalai/shardformer/test_layer/test_layernorm.py:* Added test cases for `LayerNorm1D` and `FusedLayerNorm`.Changes to colossalai/shardformer/test_layer/test_linearconv_1d.py:* Added test cases for `LinearConv1D_Col` and `LinearConv1D_Row`.Changes to colossalai/shardformer/test_model/_utils.py:* Updated `build_model` function to use `ShardConfig` with `fused_layernorm` set to `True`.### Description:This pull request adds support for `LayerNorm1D` and `FusedLayerNorm` in `ShardFormer`. It also updates the test cases for `LayerNorm1D` and `FusedLayerNorm` in `test_layernorm.py`. Additionally, it updates the test cases for `LinearConv1D_Col` and `LinearConv1D_Row` in `test_linearconv_1d.py`. Finally, it updates the `build_model` function in `_utils.py` to use `ShardConfig` with `fused_layernorm` set to `True`."
668,"add prefix sum
### Describe your change:



* [x] Add an algorithm?
* [ ] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [ ] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.
* [ ] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
",Added new file `data_structures/arrays/prefix_sum.py` with a class `PrefixSum` that implements prefix sum calculation and provides methods to get sum of array from start to end indexes and to check if array contains a target sum.
669,"Adding Manning Publication's books and one course to respective lists
Hello,

I was wondering if you would consent to adding the books and course that I have added to the respective resource lists on your page.

Thanks for your consideration.","Update to the list of free, open source books and online courses on machine learning, statistics, and data science."
670,"Issue 9401 - SequentialChain runs the same callbacks over and over in async mode
Issue: https://github.com/langchain-ai/langchain/issues/9401

In the Async mode, SequentialChain implementation seems to run the same callbacks over and over since it is re-using the same callbacks object.

Langchain version: 0.0.264, master

The implementation of this aysnc route differs from the sync route and sync approach follows the right pattern of generating a new callbacks object instead of re-using the old one and thus avoiding the cascading run of callbacks at each step.

Async mode:
```
        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()
        callbacks = _run_manager.get_child()
        ...
        for i, chain in enumerate(self.chains):
            _input = await chain.arun(_input, callbacks=callbacks)
            ...
```

Regular mode:
```
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        for i, chain in enumerate(self.chains):
            _input = chain.run(_input, callbacks=_run_manager.get_child(f""step_{i+1}""))
            ...
```

Notice how we are reusing the callbacks object in the Async code which will have a cascading effect as we run through the chain. It runs the same callbacks over and over resulting in issues.

Solution:
Define the async function in the same pattern as the regular one and added tests.

<!-- Thank you for contributing to LangChain!

Replace this entire comment with:
  - Description: a description of the change, 
  - Issue: the issue # it fixes (if applicable),
  - Dependencies: any dependencies required for this change,
  - Tag maintainer: for a quicker response, tag the relevant maintainer (see below),
  - Twitter handle: we announce bigger features on Twitter. If your PR gets announced and you'd like a mention, we'll gladly shout you out!

Please make sure your PR is passing linting and testing before submitting. Run `make format`, `make lint` and `make test` to check this locally.

See contribution guidelines for more information on how to write/run tests, lint, etc: 
https://github.com/hwchase17/langchain/blob/master/.github/CONTRIBUTING.md

If you're adding a new integration, please include:
  1. a test for the integration, preferably unit tests that do not rely on network access,
  2. an example notebook showing its use. These live is docs/extras directory.

If no one reviews your PR within a few days, please @-mention one of @baskaryan, @eyurtsev, @hwchase17, @rlancemartin.
 -->
",Update callback manager to use `AsyncCallbackManagerForChainRun` in sequential chain.
671,"Add more tests for the kubernetes executor
This change adds tests to improve coverage for the Kubernetes executor
Part of https://github.com/apache/airflow/issues/15523

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
",Add new imports and methods to test_process_error_event_for_410 and test_process_error_event_for_raise_if_not_410.
672,"convert fp16 params to fp32 when params is fp16 format
",### Change the value of `is_float16` from `False` to `True` in the `load_model` and `load_pretrained_params` functions.
673,"remove --py36
Fixes #703.",### Change: Removed `--py36` option and replaced it with `--target-version` option.
674,"[AIRFLOW-XXXX] Fix typo in ci_prepare_backport_packages.sh
---
Issue link: `Document only change, no JIRA issue`

Make sure to mark the boxes below before creating PR: [x]

- [X] Description above provides context of the change
- [X] Unit tests coverage for changes (not needed for documentation changes)
- [X] Commits follow ""[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)""
- [X] Relevant documentation is updated including usage instructions.
- [X] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).

---
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.
","Typo fix: ""bacport"" to ""backport"" in several places."
675,"fix(apm): Improve acceptance test for transaction span view
Fixes:

- missing instrumentation span shouldn't have descendants
- missing instrumentation span can never be the last span


","Update the `isLast` property in `SpanTree` component to `false` and adjust the `numOfSpanChildren` and `renderedSpanChildren` properties accordingly. Also, update the `span_tree_blueprint` in `generate_timestamp` function to include new child nodes."
676,"Add failed and success count stats to feedstorage backends
Resolves https://github.com/scrapy/scrapy/issues/3947

Example:
```
import scrapy
from scrapy.crawler import CrawlerProcess


class QuotesToScrapeSpider(scrapy.Spider):
    name = ""quotes""

    custom_settings = {
        ""DOWNLOAD_DELAY"": 1,
        ""COOKIES_DISABLED"": True,  # mistyped, should be enabled
        ""CONCURRENCY"": 5,
        ""FEEDS"": {
            ""file:///tmp/tmp-%(batch_time)s.json"": {
                ""format"": ""json"",
            },
            ""s3://mybucket/path/to/export-%(batch_time)s.csv"": {
                ""format"": ""csv"",
            },
        },
        ""FEED_EXPORT_BATCH_ITEM_COUNT"": 5,
    }

    def start_requests(self):
        yield scrapy.Request(url='http://quotes.toscrape.com/', callback=self.parse)

    def parse(self, response):
        for quote in response.css(""div.quote""):
            yield {
                ""quote"": quote.css(""span.text::text"").extract(),
                ""author"": quote.css(""small.author::text"").extract(),
                ""tags"": quote.css(""a.tag::text"").extract()
            }
            break
        next = response.css(""li.next a::attr(href)"").extract_first()
        if next:
            yield scrapy.Request(url=response.urljoin(next), callback=self.parse)


process = CrawlerProcess()
process.crawl(QuotesToScrapeSpider)
process.start()
```
if S3 fails to store, stats will be:
```
{'downloader/request_bytes': 2692,
 'downloader/request_count': 10,
 'downloader/request_method_count/GET': 10,
 'downloader/response_bytes': 23026,
 'downloader/response_count': 10,
 'downloader/response_status_count/200': 10,
 'elapsed_time_seconds': 11.61577,
 'feedexport/failed_count/S3FeedStorage': 2,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 10, 17, 20, 41, 6, 16674),
 'item_scraped_count': 10,
 'log_count/DEBUG': 218,
 'log_count/ERROR': 2,
 'log_count/INFO': 16,
 'memusage/max': 70389760,
 'memusage/startup': 70389760,
 'request_depth_max': 9,
 'response_received_count': 10,
 'scheduler/dequeued': 10,
 'scheduler/dequeued/memory': 10,
 'scheduler/enqueued': 10,
 'scheduler/enqueued/memory': 10,
 'start_time': datetime.datetime(2020, 10, 17, 20, 40, 54, 400904)}
```
Ready to review :smile: ",Improved logging and statistics for feed export.
677,"fix 11291#issuecomment-1646547908
## Description

* Fix https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/11921#issuecomment-1646547908

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
","Change the line `env['PYTHONPATH'] = f""{os.path.abspath('.')}{os.pathsep}{env['PYTHONPATH']}""` to `env['PYTHONPATH'] = f""{os.path.abspath('.')}{os.pathsep}{env.get('PYTHONPATH', '')}` to handle the case where `env['PYTHONPATH']` is empty."
678,"Add exception handling to API image decode
Malformed API request can crash WebUI server since there are no error checks performed while trying to decode image.

Example crash log:

```text
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File ""/home/vlado/.local/lib/python3.10/site-packages/anyio/streams/memory.py"", line 94, in receive
    return self.receive_nowait()
  File ""/home/vlado/.local/lib/python3.10/site-packages/anyio/streams/memory.py"", line 89, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/vlado/.local/lib/python3.10/site-packages/starlette/middleware/base.py"", line 77, in call_next
    message = await recv_stream.receive()
  File ""/home/vlado/.local/lib/python3.10/site-packages/anyio/streams/memory.py"", line 114, in receive
    raise EndOfStream
anyio.EndOfStream

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
...
  File ""/home/vlado/dev/automatic/modules/api/api.py"", line 255, in extras_single_image_api
    reqDict['image'] = decode_base64_to_image(reqDict['image'])
  File ""/home/vlado/dev/automatic/modules/api/api.py"", line 56, in decode_base64_to_image
    return Image.open(BytesIO(base64.b64decode(encoding)))
  File ""/home/vlado/.local/lib/python3.10/site-packages/PIL/Image.py"", line 3283, in open
    raise UnidentifiedImageError(msg)
PIL.UnidentifiedImageError: cannot identify image file 
```
",Add error handling to decode_base64_to_image function to handle invalid encoded images.
679,"Readme and Windows Usage update
Created a new usage instructions file for Windows users per https://github.com/AntonOsika/gpt-engineer/pull/336. Added GUI steps for completeness but I'm indifferent about their inclusion (more user accessibility but likely non-coders that may raise support load).

Linked from README and cleaned some formatting along the way, mostly headers, links, and spacing. Feel free to unwind any for preferences.","Add Windows-specific instructions to the README.md file, including information on installing and setting up the gpt-engineer package, as well as usage instructions."
680,"[3.9] Update FAQ release schedule and estimated users (GH-21180)
Update FAQ to include:
* The new yearly release schedule from PEP 602
* Estimated users from ""tens of thousands"" to ""millions""
(cherry picked from commit 3fa4799c3f9d9de7cac30e5db3627e9e125b9ce5)


Co-authored-by: E-Paine ",Update the stability information in the FAQ to reflect the new release schedule and the growing user base.
681,"Replace closeConnection with shutdownRuntime and disconnectWebsocket
## üìö Context

This is another change that's being made because we need it for the `feature/session-manager` branch
but is being made straight to `develop` to keep the final diff size down.

The special test-only `closeConnection` proto message type has a pretty misleading name because what
it really does is shut down the runtime entirely. Because we need a way to test reconnect behavior (and thus
a way to close *only* the browser websocket connection without nuking the entire Streamlit server), we replace
this proto type with `debugShutdownRuntime` and `debugDisconnectWebsocket`, which do what their names
suggest.

## üß™ Testing Done

- [x] Added/Updated e2e tests","### DIFF:Changes were made to the e2e tests for Streamlit's WebSocket connection handling. The tests now cover the following scenarios:* Disabling widgets* Disconnecting the client* Shutting down the runtimeThe tests use special development mode-only messages (`debug_disconnect_websocket` and `debug_shutdown_runtime`) to test the reconnect behavior and disabling widgets. The tests also cover the case where the client tries to disconnect or shut down the runtime when not in development mode.The changes include:* Adding new tests for the `debug_disconnect_websocket` and `debug_shutdown_runtime` messages* Updating the `BrowserWebSocketHandler` to handle these new messages* Updating the `BackMsg` proto to include the new fields* Updating the `Streamlit` code to handle the new messages and fieldsThe changes are intended to improve the reliability and robustness of Streamlit's WebSocket connection handling, particularly in development mode."
682,"[zero] use double buffers to handle grad
In order to support partial offload, we use double buffers to handle grad (`fp16_grad` and `fp32_grad`). `fp16_grad` is always on GPU, and `fp32_grad` may be partially on GPU and CPU.",### DIFF:Changes were made to the Colossal-AI zero hook and sharded model to improve memory tracing and debugging. The following changes were made:1. Added `MemStatsCollector` and `GLOBAL_MODEL_DATA_TRACER` to track memory usage and debug memory issues.2. Modified the `pre_bwd_exec` method to store the `fp32_grad` attribute of the parameter.3. Added a check to ensure that the `fp32_grad` attribute is not None before storing it.4. Modified the `_post_backward_operations` method to store the `fp32_grad` attribute of the parameter.5. Added a check to ensure that the `fp32_grad` attribute is not None before storing it.6. Modified the `_reduce_scatter_callback` method to store the `fp32_grad` attribute of the parameter.7. Added a check to ensure that the `fp32_grad` attribute is not None before storing it.8. Modified the `state_dict` method to store the `fp32_grad` attribute of the parameter.9. Added a check to ensure that the `fp32_grad` attribute is not None before storing it.These changes aim to improve the memory tracing and debugging capabilities of the Colossal-AI zero hook and sharded model.
683,"win_user_right - fix non json output issue (#54495) - 2.6
(cherry picked from commit e2d2874d8174f3a814de4b85b8e635b55868d62a)

##### SUMMARY
Backport of https://github.com/ansible/ansible/pull/54495

##### ISSUE TYPE
- Bugfix Pull Request

##### COMPONENT NAME
win_user_right",Fixes for win_user_right module: - Added a new changelog entry for a bugfix related to win_user_right output containing non-json data.- Improved the Compare-UserList function to correctly handle added and removed users.
684,"[Blerp] Add new extractor
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Adds a new extractor for Blerp. Uses Blerp's GraphQL API to query the file in the CDN required for download.

Resolves #31340.",New extractor added for Blerp.com.
685,"Fix infinite loop on wrong Digest Authentication (Issue #541 and #547)
Modified code to use the current fix versus the old fix, which was broken.

Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for issue #541 and #547.
",Update to handle 401 response with digest authentication.
686,"Fix typo
","Fix typo in sentence: ""accpts"" to ""accepts""."
687,"Added the ML Coach library to the readme.md
Found another cool project that should be in the python list",Update README.md: added Coach by Intel AI Lab to the list of projects.
688,"‚úè Fix Typo Errors in README.tr.md As A Native Turkish Speaker ‚úè
I corrected the incorrect usages in the README.tr.md file as a native Turkish speaker.","### Description of Pull Request:Rich documentation has been updated to include more information about the features and usage of the library. The documentation now includes examples of how to use Rich to create stylish console outputs, including the use of syntax highlighting, progress bars, and more. Additionally, the documentation has been updated to include information about the Rich CLI, a command-line interface for Rich that allows users to create and customize their own console outputs."
689,"Update test farm version of boulder to current master
Recent changes are no longer compatible with the old version of boulder used in the test farm tests. This PR updates the version of boulder used, and runs it with the new way of running boulder.

A new ami was created and is used here that uses Ubuntu 18.04, so that docker-compose can be installed more properly.

Removed commented-out section about rabbitmq that was already deprecated.

Switched to using the public DNS resolver 8.8.8.8 for the tests because the way to find the correct local resolver changed.",Change the AMI used by Boulder from 'ami-5f490b35' to 'ami-072a9534772bec854'.
690,"Add roles delete command to cli
closes: #15318

Screenshoot:
---
![image](https://user-images.githubusercontent.com/2135242/185799545-c3c9b034-e589-4913-8568-6739d29b48f7.png)

![image](https://user-images.githubusercontent.com/2135242/186071991-da3938f6-5241-4c44-b5e2-90b97b3c5851.png)

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).
",Add support for role creation and deletion in Airflow CLI.
691,"[pipelines] Change summarization default to distilbart-cnn-12-6
- Also adds an integration test that runs on GPU if available.
- Other pipelines could do the same if that would be helpful.",Update pipeline tests to use AutoModelForSeq2SeqLM and update default model for seq2seq tasks.
692,"Docker Questions
Advance Questions of Docker Architecture components, Cgroups, and namespaces. 

These are the questions today a company asked in DevOps Hiring. ","Changes to README.md file, adding new sections and details about Docker, containerd, and Kubernetes."
693,"fix(readme) - minor !Q
","Update the README.md file to remove the ""Markets"" word and replace it with ""Exchanges""."
694,"[extractor/biliIntl] Add comment extraction 
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

This PR add comment extraction support for `BiliIntlIE`.

Fixes #


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))


",Added support for Bililive comments extraction.
695,"add icdar2015 download
",Update image width from 600 to 700 in detection.md files.
696,"add zone type to az response



## Motivation
issue #9089 


## Changes
adds zone type to zones response

<!-- The following sections are optional, but can be useful! 

## Testing

Description of how to test the changes

## TODO

What's left to do:

- [ ] ...
- [ ] ...

-->

",Add new parameter `ZoneType` to the `describe_availability_zones` function.
697,"[core] Fix support for upcoming Python 3.12
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

This PR adds Python 3.12 as one of the test runners


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [x] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at 899b16b

### Summary
üêçüÜï‚úÖ

<!--
1.  üêç This emoji represents Python, the programming language that yt-dlp is written in and that the workflow tests against. It also suggests that the change is related to adding a new Python version.
2.  üÜï This emoji represents something new or updated, which is what CPython 3.12 is. It also suggests that the change is related to adding a new feature or option to the workflow.
3.  ‚úÖ This emoji represents a check mark or a success, which is what the workflow aims to achieve by running tests on different Python versions. It also suggests that the change is related to improving the quality or reliability of yt-dlp.
-->
Add CPython 3.12 to the testing workflow. This ensures yt-dlp can run on the future Python version.

> _`yt-dlp` tests with_
> _`CPython 3.12` - new branch_
> _autumn of Python_

### Walkthrough
* Add CPython 3.12 to the python-version list for testing compatibility ([link](https://github.com/yt-dlp/yt-dlp/pull/8130/files?diff=unified&w=0#diff-a86bb2175b62f05d86a7f7fe0f3fd6d6c44e2cdfd0f4e4a92759c817d7959d96L16-R16))




",### Description of Pull Request:This pull request updates the yt-dlp project to use the latest Python versions and fixes several issues related to date and time handling. The changes include:* Updating the Python versions used in the project to include 3.12-dev.* Fixing the handling of out-of-range timestamp values in the `YoutubeDL.py` file.* Updating the `aws.py` file to use the correct date format for AWS API requests.* Fixing the handling of dates in the `motherless.py` file to use the correct timezone.* Updating the `panopto.py` file to use the correct date format for Panopto API requests.* Fixing the handling of dates in the `_urllib.py` file to use the correct timezone.* Updating the `exceptions.py` file to disable auto-close of file objects in HTTPError exceptions.* Fixing the handling of dates in the `_utils.py` file to use the correct timezone and precision.These changes should improve the stability and accuracy of the yt-dlp project.
698,"document blueprint static folder without prefix
closes #348 

Documents that blueprint static URL is only accessible if the blueprint has a prefix, otherwise the application static URL takes precedence.",Update documentation for blueprint static files.
699,"[extractors/odnoklassniki] Add support for mobile URLs (closes #16081)
### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/rg3/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [x] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Support for another format of m.ok.ru links as described in https://github.com/rg3/youtube-dl/issues/16081",Update regular expression pattern for Odnoklassniki video URLs to match new formats.
700,"CI Fix lock-file update workflow
Follow-up of https://github.com/scikit-learn/scikit-learn/pull/28068

The update-lock-file failed, see [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/7524548489/job/20479533590).

I used `--select-build-tag` instead of `--select-tag` (flip-flopped between the two namings) the right option in the workflow `.yml` ...
",Update `update_script_args` in workflow file to use `--select-tag` instead of `--select-build-tag`.
701,"bpo-31968: Documentation -- add clarification on the globals dict for exec()
<!--
Thanks for your contribution!
Please read this comment in its entirety. It's quite important.

# Pull Request title

It should be in the following format:

```
bpo-NNNN: Summary of the changes made
```

Where: bpo-NNNN refers to the issue number in the https://bugs.python.org.

Most PRs will require an issue number. Trivial changes, like fixing a typo, do not need an issue.

# Backport Pull Request title

If this is a backport PR (PR made against branches other than `master`),
please ensure that the PR title is in the following format:

```
[X.Y]  (GH-NNNN)
```

Where: [X.Y] is the branch name, e.g. [3.6].

GH-NNNN refers to the PR number from `master`.

-->



https://bugs.python.org/issue31968

",Update documentation for exec function: added note about dictionary requirement for *globals*.
702,"fix env version attribute
if multiple versions of `httpie` are installed on a machine, the `env.config.__meta__.httpie` attribute doesn't have the current version from `__init__.__version__`
this can cause problems if the `env` object is used to make decisions to enforce compatibility based on the httpie version.","Update the `save` method in `httpie/config.py` to be called when `implicit_content_type` is not present in the config. Also, add a test case in `tests/test_config.py` to verify the `save` method is called."
703,"website: Add missing translation keys for stats page
This PR is self explanatory, there was some missing translations based on the keys returned from the server.","Add two new keys to the stats.json file: ""aborted_low_grade"", ""initial_prompt_review"", ""ranking"", and update the existing keys ""backlog_ranking"" and ""ranking""."
704,"Alpha website candidate
I've uploaded my alpha site candidate to an S3 bucket, viewable [here](https://s3.amazonaws.com/public-apis-alpha/index.html). It utilizes Vue over JQuery and Bootstrap, minimizing libraries and crazy amounts of minified file fun. I'm more than happy to listen to suggestions on this plan, but I am going to keep pressing forward in this direction. There is still plenty of work to be done here, such as loading the JSON file locally to speed up the table rendering.","### DIFF:This pull request adds a new public API list website. The website includes a search box, a table to display the API list, and a manifest file for caching. The website uses Vue.js for the frontend and fetches the API list from a JSON file hosted on GitHub. The website also includes a CSS file for styling and a JavaScript file for handling the API list filtering."
705,"Release v0.2.29
",Update version from 0.2.28 to 0.2.29 in __init__.py and pyproject.toml.
706,"Add documentation for PDF JS PoC
","Create a new README.md file for the ""Upload Insecure Files/Extension PDF JS"" project, which allows generating a PDF file containing JavaScript code. The PDF file can be used to check if the code is executed when the file is opened, making it a potential security risk. The project includes examples of JavaScript code that can be used to exploit vulnerabilities, such as XSS, open URLs, and timeout attacks."
707,"Create RayleighQuotient.py
https://en.wikipedia.org/wiki/Rayleigh_quotient

### Computes Rayleigh quotient. It is used in numerical solution of eigen value problems.

* [x] Add an algorithm?
* [ ] Documentation change?

### **Checklist:**
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [x] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
",Implement the Rayleigh quotient for Hermitian matrices.
708,"Add usage for MultiBinary space
All spaces come with a definition and an example usage except for Multibinary(). Adding usage provides uniformity in documentation and ease of understanding.",Improved documentation for the MultiBinary class in gym/spaces/multi_binary.py.
709,"Include plugin selection at the info logging level
Quick fix for #4988

@bmw I included the classnames of the used plugins (`.__class__`) for all the major subcommands  certonly, run, renew and install
",Add logging statements to provide more information about the plugin selection process.
710,"[npu] support triangle attention for llama
## üìå Checklist before creating the PR

- [ ] I have created an issue for this PR for traceability
- [ ] The title follows the standard format: `[doc/gemini/tensor/...]: A concise description`
- [ ] I have added relevant tags if possible for us to better distinguish different PRs


## üö® Issue number

> Link this PR to your issue with words like fixed to automatically close the linked issue upon merge
>
> e.g. `fixed #1234`, `closed #1234`, `resolved #1234`



## üìù What does this PR do?

> Summarize your work here.
> if you have any plots/diagrams/screenshots/tables, please attach them here.



## üí• Checklist before requesting a review

- [ ] I have linked my PR to an issue ([instruction](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue))
- [ ] My issue clearly describes the problem/feature/proposal, with diagrams/charts/table/code if possible
- [ ] I have performed a self-review of my code
- [ ] I have added thorough tests.
- [ ] I have added docstrings for all the functions/methods I implemented

## ‚≠êÔ∏è Do you enjoy contributing to Colossal-AI?

- [ ] üåù Yes, I do.
- [ ] üåö No, I don't.

Tell us more if you don't enjoy contributing to Colossal-AI.
","### DIFF:Changes were made to the colossalai kernel to support NPUs. The `NPUColoAttention` class was added, which implements the scaled dot product attention with softmax. The `npu_sdpa_attention` function was also added, which calculates the attention weights using the NPUs. Additionally, the `triangle_attn.py` file was modified to support NPUs. The `npu_triangle_attention` function was added, which reduces the attention calculation of the mask part by dividing the q, k, and v matrices into blocks."
711,"Fix README Syntax (#1651)
",Update README.rst to fix broken links and add rstcheck to the lint command.
712,"2019 Rav4 Limited AWD
The XLE and Limited fingerprints are able to be combined because they are essentially the same.","Change the comment in the values.py file from ""XLE and AWD"" to ""XLE, Limited, and AWD""."
713,"Remove explicit unicode literal in README.md and doc
As the end of python 2 has come, I open this PR in order to get rid of the explicit unicode literal in the README file and the documentation.
I know that this could be against the PEP 414 https://www.python.org/dev/peps/pep-0414/ but it should be harmless for the project.",Update documentation for GitHub API requests.
714,"Fast CI
Average CI time should now be less than 12 minutes. Before we were lucky if our docker build job was 10-15 minutes, which every other job waited on. Compressed docker image is about 70MB smaller, but image size shouldn't affect CI runtime too much anymore. Also resolves #1422 by completely avoiding the buggy upload/download artifact for passing the docker image around. 

change in commit-to-green-check-mark times for individual tests:
* build release: 27 -> 4 minutes
* linter: 20 -> 2.5 minutes
* unit tests: 20 -> 6 minutes
* process replay: 23 -> 10 minutes
* longitudinal: 22 -> 7.5 minutes
* test car models: 26 -> 11 minutes

Overhead from docker/github actions is pretty minimal now, so any improvements in commit-to-green-check-mark time will come from improving our slower tests.",**Changes in Openpilot CI/CD workflow**The changes in this pull request are focused on updating the Openpilot CI/CD workflow. The main changes include:* Renaming the workflow to use lowercase letters and underscores instead of camelCase.* Updating the Dockerfile to install Python dependencies and set the `PYTHONPATH` environment variable.* Adding a `BUILD` step to build the Docker image.* Updating the `UNIT_TEST` step to use the `coverage run` command.* Adding a `BUILD_RELEASE` step to build the release.* Updating the `push` step to use the `docker push` command.* Adding a `linter` step to run flake8 and pylint.* Updating the `process_replay` step to use the `scons` command.* Adding a `test_car_models` step to test car models.* Updating the `test_longitudinal` step to use the `scons` command.* Adding a `docker_push` step to push the Docker image to Docker Hub.* Updating the `process_replay` step to use the `scons` command.* Adding a `test_car_models` step to test car models.* Updating the `test_longitudinal` step to use the `scons` command.* Adding a `docker_push` step to push the Docker image to Docker Hub.
715,"Allow custom samplers to request discard penultimate sigma
currently custom samplers can add themselves to the list of built-in samplers but there's no facility to request discarding the penultimate sigma. this minor change moves the list of samplers subject to discard penultimate sigma into a `set` that exists as a class property.

this allows a custom sampler that wants to skip the penultimate sigma to do something like:

```python
from comfy.samplers import KSampler
KSampler.DISCARD_PENULTIMATE_SIGMA_SAMPLERS.add('sonar_dpmpp_sde')
```",Add a new attribute `DISCARD_PENULTIMATE_SIGMA_SAMPLERS` to the `KSampler` class and modify the `calculate_sigmas` method to handle the new attribute.
716,"Add missing period
","Update documentation: Changed text in CppCoreGuidelines.md, line 18957, from ""Exception performance is not predictable"" to ""Exception performance is not predictable.""."
717,"Add bond options field.
##### SUMMARY
Don't restrict users to specific bond options based on bond mode; allow users to specify both the desired mode and options as free text.

##### ISSUE TYPE
 - Feature Pull Request",Update Copyright year range to include 2018. Add new option 'options' to ovirt_host_networks module.
718,"Allow copying of PreparedRequests without headers/cookies
Fixes #2077.
",Added checks for copying PreparedRequest objects in tests.
719,"Cloudformation getatt for Firehose and ES Domain
Added missed cloudformation getatt for KinesisFirehose::DeliveryStream and Elasticsearch::Domain
",### Change: Added support for Elasticsearch and Firehose in CloudFormation templates.
720,"Update interpreter for azure inventory script
##### ISSUE TYPE
- Feature Pull Request
##### ANSIBLE VERSION

```
ansible 2.0.2.0
  config file =
  configured module search path = Default w/o overrides
```
##### SUMMARY

Removed the hardcoded path to the Python interpreter, this make it easier for virtual environments. Fixed whitespace.
",Update shebang line in azure_rm.py from #!/usr/bin/python to #!/usr/bin/env python.
721,"Test on Ubuntu 19.10
Fixes https://github.com/certbot/certbot/issues/7851.

You can see tests successfully running with this change at https://travis-ci.com/github/certbot/certbot/builds/160641256.

The AMI-ID was taken from https://cloud-images.ubuntu.com/locator/ec2/. As of writing this, it is the only Ubuntu 19.10 AMI on that page in `us-east-1` using EBS for storage that isn't running on ARM. The instance store AMI cannot be used because that storage type is not available for the instance size we use. See https://aws.amazon.com/ec2/pricing/on-demand/ and https://travis-ci.com/certbot/certbot/builds/113021661 for proof of that.",Add two new Ubuntu targets (ubuntu19.10 and ubuntu18.04LTS) to the targets.yaml and apache2_targets.yaml files.
722,"Add PR template
This PR adds a PR template to streamline the reviewing process.",Create a new PULL_REQUEST_TEMPLATE.md file in the.github directory with a template for pull requests.
723,"Update README.md
",Update release notes for DeepSpeed-Chat to reflect efficiency boost and training stability improvements.
724,"Adjusting version number
","Update version string from ""1.0-dev"" to ""1.0-stable"" and remove revision number from the version string."
725,"[3.6] bpo-34602: Avoid failures setting macOS stack resource limit (GH-14546)
Under some conditions the earlier fix for [bpo-18075](https://bugs.python.org/issue18075), ""Infinite recursion
tests triggering a segfault on Mac OS X"", now causes failures on macOS
when attempting to change stack limit with resource.setrlimit
resource.RLIMIT_STACK, like regrtest does when running the test suite.
The reverted change had specified a non-default stack size when linking
the python executable on macOS.  As of macOS 10.14.4, the previous
code causes a hard failure when running tests, although similar
failures had been seen under some conditions under some earlier
systems.  Reverting the change to the interpreter stack size at link
time helped for release builds but caused some tests to fail when
built --with-pydebug.  Try the opposite approach: continue to build
the interpreter with an increased stack size on macOS and remove
the failing setrlimit call in regrtest initialization.  This will
definitely avoid the resource.RLIMIT_STACK error and should have
no, or fewer, side effects.
(cherry picked from commit 5bbbc733e6cc0804f19b071944af8d4719e26ae6)


Co-authored-by: Ned Deily 


https://bugs.python.org/issue34602

",Avoid test suite failures on macOS by no longer calling resource.setrlimit to increase the process stack size limit at runtime. The runtime change is no longer needed since the interpreter is being built with a larger default stack size.
726,"Added type hints to dependency injection pattern
Added type hints to _dependency injection_ pattern and also checked with mypy.",Added type hints for method parameters and return types.
727,"Switch to using docker compose v2
Azure recently dropped the `docker-compose` standalone executable (aka docker-compose v1), and since it's not receiving updates anymore, let's get with the times and update to v2 as well.",Update docker-compose to docker command in integration tests.
728,"Issue 741: Fix background
This solves issue #741. You many need to perform a hard refresh to clear your browser cache for changes to take effect.",### DIFF:Changes to the color palette in the LiteGraph extension. The `litegraph_base` color palette has been updated to include a new background image and clear background color. The `comfy_base` color palette has been removed. The `node_slot` color palette has been updated to include a new background image. The `app.registerExtension` function has been updated to include a new color palette. The `app.ui.dialog.show` function has been updated to include a new color palette. The `app.canvas.draw` function has been updated to include a new color palette. The `app.canvas.updateBackground` function has been added to update the background image and clear background color of the canvas.
729,"feat(ui): Add power-features icon
I wonder if this belongs more in getsentry?",New file: icon-power-features.svg added to src/sentry/static/sentry/app/icons directory.
730,"Backport PR #32746: DOC: start 1.0.3
https://github.com/pandas-dev/pandas/pull/32746","Update documentation for new release: added v1.0.3 release notes, updated contributors list, and created new file for v1.0.3 release."
731,"Fixed ""CodeFormer visibility:"" rounding to 2 decimals.
",Change the precision of codeformer_visibility from 2 to 2 decimal places.
732,"Fixing a Link in the Docs
There was a trailing `>` in this link that borked it.

Dope project, btw.",Update documentation: removed link to specific commit in README.md.
733,"Fixes typo
","Update integrated support for unit testing to simply ""unit testing support"" in the ""What's in the Box?"" section."
734,"Feature mute stream log for info level
**Features**
see issue : #1109 
llm stream_response alway been print regardless of debug flag
I modified `logs.py` rather than `config2.py`.
This is because log is been set via `define_log_level` rather than `config.yaml`
To keep things simple, a global variable `_print_level` is introduced in `logs.py`
    
**Influence**
`define_log_level(print_level=""INFO"")` will mute `log_llm_stream`
",Update log level print function to use global variable.
735,"bipedal_walker: trivial fixes
I'll have some time this weekend to fix more bugs
","Change the assignment of `self.joints[2].motorSpeed` from `SPEED_KNEE` to `SPEED_HIP` and adjust the calculation of `self.joints[0].maxMotorTorque` and `self.joints[1].maxMotorTorque` to use `np.clip(np.abs(action), 0, 1)` instead of `np.clip(0, 1, np.abs(action))`."
736,"Support other features only modes for EfficientNet
This allows using other feature extraction helpers for effnet (besides the original custom class).

```
mm = timm.create_model('tf_efficientnet_b2', pretrained=True, features_only=True, feature_cfg=dict(flatten_sequential=True), out_indices=[-2, -1])
```","### DIFF:Changes were made to the EfficientNet model's `_efficientnet_blocks.py` and `_efficientnet_builder.py` files. The changes include:* In the `feature_info` method, the `hook_type` field was removed from the returned dictionary.* In the `forward` method, the `shortcut` variable was added to store the shortcut connection.* In the `__call__` method, the `feature_info` dictionary was updated to include the `stage` and `reduction` fields.* In the `FeatureInfo` class, the `get_dicts` method was added to return a list of dictionaries containing the feature information.These changes likely affect the model's feature extraction and output layers."
737,"Update EdgeNeXt to use ClassifierHead as per ConvNeXt
EdgeNeXt has the same style as ConvNeXt, updating classification head code to match ConvNeXt.",Change the `use_fused_attn` function to `use_fused_attn` and `NormMlpClassifierHead` to `ClassifierHead`.
738,"VW MQB: Add FW for 2023 TROC
route nameÔºö0bbe367c98fa1538|2023-12-22--10-53-18--0
and modify model year to 2023 and remove MQB-A0 to MQB



<!--- ***** Template: Fingerprint *****

**Car**
Which car (make, model, year) this fingerprint is for

**Route**
A route with the fingerprint

-->

<!--- ***** Template: Car Bugfix *****

**Description**

A description of the bug and the fix. Also link the issue if it exists. 

**Verification**

Explain how you tested this bug fix. 

**Route**

Route: [a route with the bug fix]


-->

<!--- ***** Template: Bugfix *****

**Description**

A description of the bug and the fix. Also link the issue if it exists. 

**Verification**

Explain how you tested this bug fix. 


-->

<!--- ***** Template: Car Port *****

**Checklist**

- [ ] added entry to CarInfo in selfdrive/car/*/values.py and ran `selfdrive/car/docs.py` to generate new docs
- [ ] test route added to [routes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/car/tests/routes.py)
- [ ] route with openpilot:
- [ ] route with stock system:
- [ ] car harness used (if comma doesn't sell it, put N/A):


-->

<!--- ***** Template: Refactor *****

**Description**

A description of the refactor, including the goals it accomplishes. 

**Verification**

Explain how you tested the refactor for regressions. 


-->

",Update the supported vehicle list for the Volkswagen T-Roc to include the 2018-23 model.
739,"Add user friendly wildcard error for ACMEv1
Part of #5367.

Checking for wildcards in ACME doesn't have to worry about bytes, because the relevant documentation says the value must be unicode/str and none of Certbot's code paths violates this. (`acme.crypto_util._pyopenssl_cert_or_req_all_names` which is used in `BackwardsCompatibleClientV2` before calling this function returns unicode/str in Python 2/3 respectively.) ","### Change handling of wildcard domains in ACME protocolThe changes include the addition of a new error type `WildcardUnsupportedError` to handle cases where a wildcard domain is requested but is unsupported by the ACME version. The `request_challenges` and `new_order` methods have been updated to raise this error when a wildcard domain is requested. Additionally, the `obtain_certificate` method has been updated to handle this error and raise an exception if the ACME CA endpoint does not support issuing wildcard certificates."
740,"Fix pytests
2 issues were causing pytests to fail:
- Tensorflow required `scipy==1.4.1`
- cx-Oracle `8.0.0` broke `test_sqlalchemy_engine_2_oracle`",Update test-requirements.txt to remove cx_oracle dependency and add constraints for cx-Oracle and scipy.
741,"HangMan Game
This is a 'HangMan Game' Program which was written in 'Python' Language.",Add a new HangMan Game program.
742,"[3.9] bpo-39096: Improve description of 'e', 'f' and 'g' presentation types (GH-23537)
* Improve description of 'e', 'f' and 'g' presentation types

* Drop the 'E' from Scientific 'E' notation; remove >= 0 qualifications

* Fix false statement that the alternate form is valid for Decimal

* Nitpick: remove the Harvard/Oxford comma

* Add note that the decimal point is also removed if no digits follow it, except in alternate form
(cherry picked from commit c642374b3ef72f6f300616f07aea2a3f9ed83e51)


Co-authored-by: Mark Dickinson 


https://bugs.python.org/issue39096

",Update documentation for string module to reflect changes in presentation types for floating point and decimal values.
743,"BREEZE: add troubleshooting section to cover ETIMEDOUT during start-airflow
closes: #30947
related: #30947

- Adds section under `troubleshooting` to describe a potential ETIMEDOUT condition when running breeze to start airflow, and inform readers of potential symptoms , reasons, and way to avoid it.",Update documentation for BREEZE.rst to include information about ETIMEOUT error and its resolution.
744,"Revert ""Update bubble_sort.py""
def bubbleSort(alist):
    for passnum in range(len(alist)-1,0,-1):
        for i in range(passnum):
            if alist[i]>alist[i+1]:
                temp = alist[i]
                alist[i] = alist[i+1]
                alist[i+1] = temp

alist = [54,26,93,17,77,31,44,55,20]
bubbleSort(alist)
print(alist)
",Improved the bubble sort algorithm by removing unnecessary loop iterations.
745,"Improve main documentation
The resulting docs look like:
![Screen Shot 2019-11-27 at 2 38 27 PM](https://user-images.githubusercontent.com/6504915/69763933-989a5480-1123-11ea-8538-76a3a354b2bd.png)

I deleted the exceptions because I think it's not feasible to document the possible exceptions raised by all of Certbot.",Update docstrings in certbot/certbot/_internal/main.py and certbot/certbot/main.py to improve clarity and consistency.
746,"Improve dropblock impl, add fast variant, better AMP speed, inplace‚Ä¶
","### DIFF:Changes to the Timm library's models:* Added a new `drop_block_fast_2d` function in `drop.py` which simplifies the DropBlock calculation without considering valid block mask at edges.* Modified the `DropBlock2d` class in `drop.py` to use the new `drop_block_fast_2d` function when `fast` is set to `True`.* Added a `batchwise` parameter to the `DropBlock2d` class and modified the `forward` method to use the new `drop_block_fast_2d` function when `batchwise` is set to `True`.* Modified the `SplitAttnConv2d` class in `split_attn.py` to add a `drop_block` parameter and use it in the `forward` method.* Modified the `ResNeSt` class in `resnest.py` to add a `drop_block` parameter and use it in the `forward` method.* Removed the `drop_block1`, `drop_block2`, and `drop_block3` variables from the `ResNeSt` class and replaced them with a single `drop_block` variable.These changes aim to improve the performance and efficiency of the Timm library's models by simplifying the DropBlock calculation and adding a `batchwise` parameter to the `DropBlock2d` class."
747,"‰øÆÂ§çË•øÁìúËßÜÈ¢ë‰∏ãËΩΩÂ§±Ë¥•ÈóÆÈ¢ò;
fix problem:the JSON object must be str, bytes or bytearray, not NoneType","Change the ixigua_download function to accept an additional stream_id parameter, and modify the video_info_url and video_info to use this parameter."
748,"Linkify Github usernames in authors list
- Linkify github usernames
- Remove usernames that do not exist

Change-Id: Ib88b70a3010e915b3570ae5062c8cb416c9a6462
",Update the AUTHORS.rst file to include GitHub usernames and links for contributors.
749,"Fix whisper STT
Users who updated to `gradio==4.26.0` have reported that the `whisper_stt` extension has stopped working. It appears that the error stems from the fact that passing `None` as the new value to a `gradio.Audio` component (to reset the component) now causes an exception to occur when the `.change` listener is used. This PR simply changes the event listener to `stop_recording`, which fixes the issue while keeping the UI behavior the same.

Fixes https://github.com/oobabooga/text-generation-webui/issues/5850 and perhaps https://github.com/oobabooga/text-generation-webui/issues/5805

## Checklist:

- [X] I have read the [Contributing guidelines](https://github.com/oobabooga/text-generation-webui/wiki/Contributing-guidelines).
",Change the `change` method to `stop_recording` in the `audio` object.
750,"kraken - fetchTradingFee
- Remove hardcoded fees
- Add `fetchTradingFee`",Change the fetchTradingFees method to fetchTradingFee and update the method to fetch trading fees for a specific symbol.
751,"Store Message Toxicity in database
Implementing the calculation of the message toxicity in the workflow as well as storing its value in the database.",### DIFF:Changes were made to the backend of the OASST application. The main changes include:* The addition of a new table `message_toxicity` in the database to store the toxicity scores of messages.* The implementation of a new API endpoint to calculate the toxicity score of a message using the Hugging Face model.* The modification of the `PromptRepository` to store and retrieve toxicity scores.* The modification of the `TreeManager` to calculate the toxicity score of a message and store it in the database.* The addition of a new configuration option `DEBUG_SKIP_TOXICITY_CALCULATION` to allow for skipping the toxicity calculation in debug mode.These changes aim to improve the functionality of the OASST application by providing a more accurate and efficient way to calculate the toxicity score of messages.
752,"Added default file_metadata to get basic metadata that many postprocessor use
# Description

- minor import error fixed
- added default file_metadata to get basic metadata that many postprocessor use

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
","Update metadata extractor to include creation, last modified, and last accessed dates."
753,"Socks proxy support for Websockets 
added support for Socks proxies for websockets,.
have been testing for quite some while and they work

[fixes #20315]",Here is the description of the pull request:Change the proxy settings to include wsSocksProxy and ws_proxy.
754,"Add Github Action for black formatting check




## Why are these changes needed?

Easier validation of PRs. `format.sh` fails on pylint, so it never even runs black.

## Related issue number (if applicable)

#270

## Checks

- [X] I've run `format.sh` to lint the changes in this PR.
- [X] I've included any doc changes needed.
- [X] I've made sure the relevant tests are passing (if applicable).
",
755,"Add ParamountPlusSeriesIE.
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Closes https://github.com/yt-dlp/yt-dlp/issues/602
",Add new extractor for Paramount+ series.
756,"DataTalks.Club podcast, newsletter and blog
Also changed the formatting to lists","### Update to blogs.md: Reorganized and expanded list of data science, machine learning, and AI-related blogs and resources."
757,"create a new ConfigResource and deprecate /?_config_
Relates to #7465

We're adding a migration path to `ConfigUpdateProxyListener(ProxyListener)`

It currently allows to update the config with a POST on `/?_config_`, and some services can set up listeners to react to the changes. 

This endpoint will be deprecated.

We can't remove the current ProxyListener and will log a deprecation message.
It currently match on `/?_config_`, a query string, which we cannot use the router to match. The new route is at `/_localstack/config` and make uses of `LocalstackResources`. 

\cc @thrau for the change in `LocalstackResources`","### Change: Add support for updating config variables via HTTP endpointThe pull request adds support for updating config variables via an HTTP endpoint. This allows users to update config variables programmatically, which can be useful in certain scenarios. The endpoint is located at `/config` and accepts POST requests with a JSON body containing the variable name and new value. The update is then triggered by calling the `update_config_variable` function."
758,"Replace the easy v. secure prompt with more clear choices
Fixes #3328, though we should still make the larger change to a more expressive, expansive UI.",Update the menu options for the `redirect_by_default` function to include a new option for no redirect and a more detailed description for the redirect option.
759,"Merge dev branch
","**Update stale issue workflow**The stale issue workflow has been updated to close issues after 2 months of inactivity, instead of 6 weeks. This change aims to reduce the number of stale issues in the repository.**Tokenizer updates**The `llamacpp_HF` tokenizer has been updated to load llama.cpp as a Transformers model. To use it, you need to download a tokenizer in HF format for your GGUF. The updated documentation provides more information on how to use this tokenizer.**Requirements file updates**The `requirements.txt` and `requirements_noavx2.txt` files have been updated to include new versions of the `AutoGPTQ` and `exllamav2` libraries. These updates aim to improve the stability and performance of the models."
760,"Update README.md
-fixed some grammatical errors
-improved word choice",Changes to the README.md file to provide more context and clarify the intentions of the developers.
761,"Fix pydantic and autodoc_pydantic version to <2.0.0 until support is added.
- pydantic is working on updating to a 2.0.0 that is not backwards compatible, we will need to add this, but since this will break our CI when it is released, this is a preventative step.
- autodoc_pydantic hasn't addressed how they will handle this, though it will be tracked in [this issue](https://github.com/mansenfranzen/autodoc_pydantic/issues/146).  I assume they will update to 2.0.0 once they add support as well.
- Once pydantic 2.0.0 is released, fixes will be added in #3289.",Update dependencies in requirements files: autodoc_pydantic and pydantic versions updated to <2.0.0.
762,"Support unknown backends in entrypoint_prod.sh
Close: https://github.com/apache/airflow/issues/22868

@don1uppa Can you check it?
<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).
","Change in `Dockerfile` and `entrypoint_prod.sh` to handle connection details to the broker. The change adds a check to see if the `DB_HOST` and `DB_PORT` variables are set before running the connectivity check. If they are not set, it will print a message indicating that the connection details could not be determined and skip the check."
763,"Add installation documentation for ubuntu
The conf.py is broken when eulertour merged his doc fork to master. To test the doc add ``master_doc = ""index""`` to conf.py  

I have not test this on a fresh system, the installation process is inferred from my bash history.  ",Update installation instructions for Linux.
764,"Fixed #35056 -- Fixed system check crash on reverse m2m relations with related_name in ModelAdmin.filter_horizontal/vertical.
ticket-35056

Thanks Thomas Feldmann for the report.

Regression in 107865780aa44914e21d27fdf4ca269bc61c7f01.",Fixed checks for many-to-many fields in ModelAdmin.
765,"handle printing even if sys.stdout.buffer is not available
Related: #729, #879",Changes to Whisper library: - Removed import of sys module in `transcribe.py` and moved the `make_safe` function to `utils.py`. - Updated the `make_safe` function to handle non-utf-8 system encodings.
766,"[tune] Hotfix Ax breakage when fixing backwards-compat


## Why are these changes needed?

A variable misname for a code-path added recently for avoiding backwards
compat breakage.


## Related issue number



## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)",Update deprecated `max_concurrent` parameter in AxSearch and NevergradSearch classes.
767,"Store the key file in the config dir
**Description:**
This PR changes the default path where the `media_player.webostv` and `notify.webostv` store their config (ie. pairing key file). It used to be `$HOME/.pylgtv` and now defaults to `$HASS_CONFIG_DIR/webostv.conf`.

**Related issue (if applicable):** fixes #5645

**Pull request in [home-assistant.github.io](https://github.com/home-assistant/home-assistant.github.io) with documentation (if applicable):** home-assistant/home-assistant.github.io#1951

**Example entry for `configuration.yaml` (if applicable):**
```yaml
media_player:
  - platform: webostv
    host: mytv.lan
    filename: webostv.conf

notify:
  - platform: webostv
    host: mytv.lan
    filename: webostv.conf
```

**Checklist:**

If user exposed functionality or configuration variables are added/changed:
  - [x] Documentation added/updated in [home-assistant.github.io](https://github.com/home-assistant/home-assistant.github.io)

If the code communicates with devices, web services, or third-party tools:
  - [x] Local tests with `tox` run successfully. **Your PR cannot be merged unless tests pass**
  - [x] New dependencies have been added to the `REQUIREMENTS` variable ([example][ex-requir]).
  - [x] New dependencies are only imported inside functions that use them ([example][ex-import]).
  - [x] New dependencies have been added to `requirements_all.txt` by running `script/gen_requirements_all.py`.
  - [x] New files were added to `.coveragerc`.

If the code does not interact with devices:
  - [ ] Local tests with `tox` run successfully. **Your PR cannot be merged unless tests pass**
  - [ ] Tests have been added to verify that the new code works.

[ex-requir]: https://github.com/home-assistant/home-assistant/blob/dev/homeassistant/components/keyboard.py#L16
[ex-import]: https://github.com/home-assistant/home-assistant/blob/dev/homeassistant/components/keyboard.py#L51
",Update LG WebOS TV component and notify service to use pylgtv 0.1.3.
768,"[extractor/vidio] Add support for /embed link, fix test in VidioIE
### Description of your *pull request* and other information



<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

ADD DESCRIPTION HERE
This PR add support for `/embed` link in vidio.com. Some sites like enamplus.liputan6.com use this embed link. This PR also fix test error based on test return 

Fixes #


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))
",Update Vidio extractor to support embed URLs.
769,"[URPlay] Fix extractor
### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Closes #28073.
",Update the urplayer data extraction to use the new program container component.
770,"Fix model download link in documentation
",Update the download link for the trained model of ViTSTR from `rec_vitstr_none_none_train.tar` to `rec_vitstr_none_ce_train.tar`.
771,"Adding sensible UI logging for typical user
Closes #599 

There are 2 rules I followed when adjusting the logging level of certain messages.

First, I refrained from changing messages from info to warning (although some messages seemed like they belong more to warning, but that could be done in another PR).

And second, I tried to make it as possible to what a normal user would understand, without the messages being pedantic.
",Improved logging and debugging in Certbot.
772,"zb error types
","### Change in error codes and descriptions in zb.jsThe pull request changes the error codes and descriptions in the zb.js file. The changes include:* Adding new error codes and descriptions for various scenarios, such as authentication errors, permission denied, and insufficient funds.* Updating existing error codes and descriptions to provide more detailed information about the errors.* Removing some error codes and descriptions that are no longer used.The changes aim to improve the error handling and provide more accurate and informative error messages to users."
773,"Add IP Location API
",Added IP Location API to the list of available APIs.
774,"Fixing inference api for FP32 and non-masking GPT-based models
This PR solves https://github.com/microsoft/DeepSpeed/issues/1192 and https://github.com/microsoft/DeepSpeed/issues/1203","### Change in transformer_inference.py: - Removed the requirement for `attn_mask` in `ds_softmax` and `attention_unfused` functions. - Added a new parameter `no_masking` to `ds_softmax` and `attention_unfused` functions. - In `ds_softmax` and `attention_unfused` functions, when `no_masking` is `True`, `attn_mask` is not used."
775,"Apache conf library
More conf files.  And a very hackish script to run each of them through the client, so you can see that it does what you expect (though in some cases, what you expect is breakage because of the nature of the test).
","### Description of Pull RequestThis pull request adds a set of Apache configuration files to test the functionality of the Apache server. The files are organized into three directories: `failing`, `hackish-apache-test`, and `passing`. The `failing` directory contains a configuration file that is missing a double quote, which should cause the Apache server to fail. The `hackish-apache-test` directory contains a script that tests the Apache server's functionality by creating a temporary configuration file and then removing it. The `passing` directory contains a set of configuration files that should be successfully parsed by the Apache server.The pull request also includes a `README.modules` file that lists the required Apache modules to parse the configuration files. Additionally, it includes a `README.conf` file that provides information on how to use the configuration files.Overall, this pull request provides a set of test cases for the Apache server's configuration file parsing functionality."
776,"community[patch]: Use uuid4 not uuid1
Using UUID1 is incorrect since it's time dependent, which makes it easy to generate the exact same uuid
",### DIFF:The changes in this pull request are related to the generation of unique IDs for texts and images in various vector stores. The IDs are currently generated using `uuid.uuid1()` but are being changed to use `uuid.uuid4()` instead. This change ensures that the generated IDs are more unique and less likely to collide.
777,"Feature/verify trigger rule
Make the dag classes verify the syntax on trigger_rule.

Because you can enter text in this field and typo can cost a lot on this, since the scheduler does not complain about unknown statuses.
",Add validation for trigger rule in TriggerRule and raise AirflowException if the trigger rule is invalid.
778,"Fix misdetection of project root with `--stdin-filename`
There are a number of places this behaviour could be patched, for
instance, it's quite tempting to patch it in `get_sources`. However
I believe we generally have the invariant that project root contains all
files we want to format, in which case it seems prudent to keep that
invariant.

Fixes #3207",Fix misdetection of project root and verbose logging of sources in cases involving `--stdin-filename` (#3216)
779,"Fixing pool set and pool delete via cli
There was a typo in 1.10.15 which was causing pool set and pool delete to fail when done via the new CLI command.
Fixed the typos.

closes: #14940
related: #14940
",Update the `pool_set` and `pool_delete` functions to use `args.pool` instead of `args.name` and `pool.name` respectively.
780,"TST/REF: collect tests from test_multilevel
",
781,"Update sites.md
",Update to the list of supported sites: increased total count from 304 to 305.
782,"Fix sparse attention for small block-sizes
This fixes https://github.com/microsoft/DeepSpeed/issues/1513",Update calculation of `start_width` to use a conditional expression.
783,"Introducing a custom converter for regex
I've been trying to introduce a new `BaseConverter` to match an arbitrary regex for custom endpoints. Need some direction from @thrau üòá ",Update the Werkzeug routing to use a custom converter for regex-based routing.
784,"CP.44 improve example and add more accurate discussion
",Improved documentation for C++ Core Guidelines: added example to illustrate the issue with unnamed local objects.
785,"python 3.11 need this
",Remove numpy version from requirements list.
786,"Added: Cross-Site WebSocket Hijacking (CSWSH)
Hi

This adds Cross-Site WebSocket Hijacking (CSWSH).

Cool repo btw :)

Best regards,
Emanuel","Addition of a new section ""Cross-Site WebSocket Hijacking (CSWSH)"" to the README.md file, which describes a potential security vulnerability in WebSockets and provides an example exploit."
787,"[TIPC] Fix amp
att","### Change epoch number from 50 to 1 for whole_train_whole_infer in three configurations: ch_PP-OCRv3_rec, ch_PP-OCRv3_rec_PACT, and en_table_structure."
788,"Fix Typo
Fixed typo",Update to README.md: Changed wording in paragraph about AP (Application Partitioning) to allow for eventual consistency.
789,"Atari env fix
remove redundant repeated local variable assigments",Remove import of logger from gym.
790,"Add support for GPT4All-13B-Snoozy
## Why are these changes needed?

Add support for the GPT4All-13B-Snoozy model: A finetuned LLaMa 13B model on assistant style interaction data trained by Nomic AI.
Model Card: https://huggingface.co/nomic-ai/gpt4all-13b-snoozy

Results:
![image](https://github.com/lm-sys/FastChat/assets/49086305/62756f43-b01d-48f6-a6c1-331c0f2b51f6)


## Related issue number (if applicable)

Closes #863

## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [ ] I've made sure the relevant tests are passing (if applicable).
",Added support for snoozy model in FastChat.
791,"feat(extensions): Allow hiding link connectors
Thank you for adding this feature (linksRenderMode) to core. I would like to add the ""Hidden"" option (invalid number 3 will just hide the connector lines), so that I can remove that extension from my extension pack to prevent conflicts

https://github.com/failfa-st/failfast-comfyui-extensions","Update the options in Link Render Mode extension to include ""Hidden"" as an option."
792,"place the logo in the middle in README.md
I placed the logo in the center in README.md.
because I think it's better.

and I placed title in the middle too.",Update README.md: Add a title and center-align the HTTPie logo.
793,"Remove pdb from llama_cpp
# Description

During the fix of https://github.com/jerryjliu/llama_index/issues/7547, a call to `pdb` was introduced, in https://github.com/jerryjliu/llama_index/commit/94912afd2c15fc10b7eddbe7f95cba8555fd1fc6. It should not be in the final code.

Fixes #7653  

## Type of Change

Please delete options that are not relevant.

- [X] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [X] I stared at the code and made sure it makes sense

No need to test. Just a minor clean up.

# Suggested Checklist:

- [X] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [X] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
",Remove pdb.set_trace() statement.
794,"Fix type confusion bug
fixes #4325",Change the type of the `page` string from bytes to str when checking if it starts with the BOM signature.
795,"Update README.md
added new source to Deep Learning on how to create neural network using Keras.",Update to README.md: added Keras Beginner Tutorial link.
796,"configurable masks color and default brush color
Oh my eyes my eyes
I'm blinded by the Light

## Description
https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/682ff8936df018330e0d2a259794a262dc3251b2 I know you're overjoyed
but at least make it consistent and configurable

the default brush color for the color sketch is not important, as we have a `color picker` built into the tool
but since you changed it on `Inpaint sketch` and not for `sketch` I decide to annoy you to put all three options in
> and set the default color black to annoy you, change it to white if you want

unfortunately I don't think there is a `color picker` on the noirmal `sketch` tool (only on the `color-sketch`)
so we cannot change the mask brush color during use
and the mask color is not updated by the settings immediately and requires a Reload
maybe it's achievable if we do some messing around with JavaScript

able to change the transparent mask color is important
as depending on what images you're working on some mask color may not be suited

## Screenshots/videos:
before pr

https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/40751091/87d663b5-ec3a-4490-afa7-255e18cb7027

after PR
![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/40751091/35a9b32b-0a12-441c-93ba-68dc6e65e5ef)


## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Added options for img2img sketch and inpaint mask brush colors.
797,"Reword proxies section and add warning
This adds a warning around the Session proxies issues in Requests 2.x and cleans up a few erroneous sentences in the following section. Notably, rewording the environment intro to match the warning above, and removing `curl_ca_bundle` which was added in #5670. It's not particularly relevant to this section and is covered elsewhere.

## Preview Render

",Update documentation to clarify behavior of `session.proxies` and provide warning about environmental proxies.
798,"Added escape and double escape character(s).
",Add two new values to the list in blns.json: backslash (`\`) and double backslash (`\\`).
799,"Remove the need for TRAVIS to be set.
I initially added this when the script was doing things like migrating all LXD containers to the snap. I think the external side effects are now pretty minimal thought so I think we can remove the need for this environment variable which makes it easier to use outside of CI for manual testing.",Remove Travis CI check and related comments.
800,"skflow, simplified interface to tensorflow by Google, added
","Update README.md: added a new link to skflow, a simplified interface for TensorFlow."
801,"Solve issue add sound in Windows 10
Change way to remove sound_file_path
",Remove unnecessary file deletion using `os.remove` instead of `subprocess.call`.
802,"gh-93351: Ensure the position information in AST nodes created by the parser is always consistent
Closes: #93351
<!--
Thanks for your contribution!
Please read this comment in its entirety. It's quite important.

# Pull Request title

It should be in the following format:

```
gh-NNNNN: Summary of the changes made
```

Where: gh-NNNNN refers to the GitHub issue number.

Most PRs will require an issue number. Trivial changes, like fixing a typo, do not need an issue.

# Backport Pull Request title

If this is a backport PR (PR made against branches other than `main`),
please ensure that the PR title is in the following format:

```
[X.Y]  (GH-NNNN)
```

Where: [X.Y] is the branch name, e.g. [3.6].

GH-NNNN refers to the PR number from `main`.

-->
",Added tests for invalid position information in the AST.
803,"Bump minimum Python version to 3.5.2
See https://github.com/scrapy/scrapy/pull/4243#issuecomment-632899356",Update Python version requirements to 3.5.2.
804,"fix onprem storage incorrect config
fix: https://github.com/mingrammer/diagrams/issues/344","Changes to configuration and documentation files. In `config.py`, the ""storage"" section has been updated to remove the ""ceph"" and ""Ceph-OSD"" keys, and add new keys ""Ceph"" and ""CephOsd"". In `diagrams/onprem/storage.py`, the `CephOsd` and `Ceph` classes have been added, and the `Glusterfs` class has been updated. In `docs/nodes/onprem.md`, the documentation for on-prem storage nodes has been updated to include `CephOsd` and `Ceph` as aliases."
805,"Approve functions used as default arguments
### **Describe your change:**

Use linter directive `# noqa: B0008` for these functions because they set a seed value and `BaseBackend` is reused across all calls and `LinearCongruentialGenerator.__init__()` is only called once for each instance.

[flake8-bugbear](https://pypi.org/project/flake8-bugbear) ___B008 Do not perform function calls in argument defaults.  The call is performed only once at function definition time. All calls to your function will reuse the result of that definition-time function call.  If this is intended, assign the function call to a module-level variable and use that variable as a default value.___

* [ ] Add an algorithm?
* [x] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### **Checklist:**
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [x] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
",Improved documentation and type hints for function parameters.
806,"Fix requirements in gui  api
",### Change in API.py: Added a new function `user_select_image` to handle file selection for images. The function is now wrapped in a try-except block to handle potential errors. The `except` block now catches both `ImportError` and `NameError`. ### Delete main.py: The file `main.py` has been deleted.
807,"Fix various item tags (missing colons)
I.2, C.10, A.1, A.2, A.4, NL.5",Update documentation for C++ Core Guidelines: added new rules and updated existing ones.
808,"Forcing pip to use https on older docker images (Wheezy & Precise)
As was pointed out in PR #5205, certain integration tests on Travis tests are failing because it seems PyPi now requires HTTPS, and the default for pip on Wheezy and Precise is to use HTTP. This PR updates the Dockerfiles to use HTTPS.",Update Dockerfile to include PIP_INDEX_URL environment variable and install nose.
809,"Exception of CP.100 refined to include the ""sequentially consistent memory model"" only
Atomic variables are safe with the sequentially consistent memory model (**memory_order_seq_cst**) only. Weakly ordered atomics cannot be considered safe for mainstream use. The exception clause of CP.100 refined.
","Update documentation for CppCoreGuidelines.md to include information about the sequentially consistent memory model for atomic variables, and add a new entry to isocpp.dic."
810,"Fix crash when email submission endpoint unavailable
Handle KeyError and ValueError so that if the email submission endpoint
goes down, Certbot can still run.

Add tests to eff_test.py:
 - simulate non-JSON response as described in issue #5858
 - simulate JSON response without 'status' element

Non-JSON response throws an uncaught ValueError when attempting to
decode as JSON. A JSON response missing the 'status' element throws an
uncaught KeyError when checking whether status is True or False.

Teach _check_response to handle ValueError and KeyError and report an
issue to the user.

Rewrite if statement as assertion with try-except block to make error
handling consistent within the function. Update test_not_ok to make
mocked raise_for_status function raise a requests.exceptions.HTTPError.

Resolves #5858",Improved error handling in Certbot's EFF module and tests.
811,"[MRG+1] Remove --lsprof command-line option. 
See #1531 for motivation.
",Remove the lsprof profiling stats output and update the Scrapy command-line options to only include the profile option.
812,"Fix tf-idf
Fix #2974
",Update the calculation of tfidf weights to use weighting scheme 2 from Wikipedia.
813,"Requirements version bump
This bumps the versions of:

- scandir
- h5py
- Keras
- opencv-python

to their latest vesions.

Virtual Environment will need to be setup again to make use of these.",
814,"Add rauth.
Added rauth to the list of modules.
",Update documentation: added rauth as an alternative to requests-oauth for OAuth support.
815,"remove `fix_return_consumed_capacity`, fixes #6437
`fix_return_consumed_capacity` was added for issue #2049.

This seems to be the cause of issue #6437 and it doesn't solve the problem for issue #2049 from what I can tell.
If `ReturnValues` are not specified it always forces `ReturnConsumedCapacity` as `TOTAL`.

Steps to try reproduce #2049 without success:
```bash
# Create table to repro the issue
awslocal dynamodb create-table \
  --table-name ""put-item-response"" \
  --billing-mode ""PAY_PER_REQUEST"" \
  --attribute-definitions \
    AttributeName=pkey,AttributeType=S \
  --key-schema \
    AttributeName=pkey,KeyType=HASH \
  --region eu-west-1

# This works as expected, no values are returned as there is no previous item
awslocal dynamodb put-item \
  --table-name ""put-item-response"" \
  --item '{""pkey"":{""S"":""test""}}' \
  --return-values ""ALL_OLD""

# This works as expected, the values for the previous item is returned.
awslocal dynamodb put-item \
  --table-name ""put-item-response"" \
  --item '{""pkey"":{""S"":""test""},""val"":{""S"":""foo""}}' \
  --return-values ""ALL_OLD""

# Cleanup resources
awslocal dynamodb delete-table \
  --table-name ""put-item-response""
```",### Change: Removed `fix_return_consumed_capacity` method and added `fix_consumed_capacity` method.
816,"Enable multipage apps by default
## üìö Context

We initially merged the multipage apps PR with the `ui.hideSidebarNav` config option defaulting to
`True` to use the option as a kind of ""feature flag"" (we need the option either way in case we ever want
to turn off the nav component and instead build one into Cloud).

Now that we're closer to the release of the multipage apps feature, we can toggle the option to enable
MPAs by default.

- What kind of change does this PR introduce?

  - [x] Feature
",Update default value of `ui.hideSidebarNav` from `True` to `False` in `config.py`.
817,"Remove ansible-galaxy login
##### SUMMARY
* GitHub is removing the underlying API used to implement the `login` command. Since the general consensus seems to be that relatively nobody currently uses this command (in favor of explicit token passing), support was simply removed for interactive login. If a future need arises, this command should be reimplemented via OAuth Device Auth Grants (eg, see https://github.com/ansible/ansible/pull/71628)
* login or role login commands now produce a fatal error with a descriptive message
* updated 2.10 and 2.11 porting guide entries

fixes #71560

##### ISSUE TYPE
- Bugfix Pull Request

##### COMPONENT NAME
Galaxy CLI

##### ADDITIONAL INFORMATION




```paste below

```
","The Ansible Galaxy API has been updated to require an API token for publishing roles or collections to Galaxy. The `ansible-galaxy login` command has been removed, and users must now use a Galaxy API token to authenticate with the Galaxy API."
818,"Use call to `inspect_reponse()` that works on 0.24.x
Otherwise, when the spider is executed from a standalone script as described in http://doc.scrapy.org/en/stable/topics/practices.html#run-scrapy-from-a-script it triggers an exception like this:

```
.../site-packages/scrapy/shell.py"", line 131, in inspect_response
...
from scrapy.project import crawler
...
exceptions.ImportError: cannot import name crawler
```
",Update code to pass `self` to `inspect_response` function.
819,"Bug Fix - #8415
  - Issue: #8415 ",Update the `lookup` method to use `_get_gptcache` instead of `self.gptcache_dict.get`.
820,"Add self to data code owners
A couple of the `/data/` code owners are less active on the project recently and we have a few PRs to that directory piling up. Adding myself to code owners so I can review these PRs. Also created a code owner category for `/oasst-data/`.",Update CODEOWNERS file to add @olliestanley to the data section.
821,"[MRG+1] py3 fix HttpProxy and Retry Middlewares
","### DIFF:Added support for HTTP proxy authentication encoding. The `HttpProxyMiddleware` now accepts an `auth_encoding` parameter, which defaults to 'latin-1'. This allows for custom encoding of proxy authentication credentials."
822,"Add phishstats.info
","Added a new API endpoint for PhishStats, a phishing database."
823,"üåê Add German translation for `docs/de/docs/advanced/testing-websockets.md`
‚Üê `advanced/events.md` (#10693)
‚Üí `advanced/testing-events.md` (#10704)

[German translation progress](https://github.com/tiangolo/fastapi/discussions/10582)",Add new file `docs/de/docs/advanced/testing-websockets.md` with documentation on testing WebSockets using `TestClient`.
824,"Make st.write render sets as list instead of strings
<!--
Before contributing (PLEASE READ!)

‚ö†Ô∏è If your contribution is more than a few lines of code, then prior to starting to code on it please post in the issue saying you want to volunteer, then wait for a positive response. And if there is no issue for it yet, create it first.

This helps make sure:

  1. Two people aren't working on the same thing
  2. This is something Streamlit's maintainers believe should be implemented/fixed
  3. Any API, UI, or deeper architectural changes that need to be implemented have been fully thought through by Streamlit's maintainers
  4. Your time is well spent!

More information in our wiki: https://github.com/streamlit/streamlit/wiki/Contributing
-->

## üìö Context

_Please describe the project or issue background here_

- What kind of change does this PR introduce?

  - [ ] Bugfix
  - [ ] Feature
  - [ ] Refactoring
  - [x] Other, please describe:

This PR is related to [Issue-#4923](https://github.com/streamlit/streamlit/issues/4923). It makes `st.write` to serialize `set` as `list` instead of `string`, so the user can benefit on existing collapse mechanism when browsing the data.

## üß† Description of Changes

- _Add bullet points summarizing your changes here_

  - [ ] This is a breaking API change
  - [ ] This is a visible (user-facing) change

**Revised:**

_Insert screenshot of your updated UI/code here_

**Current:**

Currently `set` is serialized to string, when using `st.write`, like this:

```
{
  ""list"": [],
  ""set"": ""set()"",
  ""dict"": {}
}
```
I'd like to render them as `list`, so we can benefit on existing `st.json` collapse mechanism when browsing the data.

_Insert screenshot of existing UI/code here_

## üß™ Testing Done

- [ ] Screenshots included
- [ ] Added/Updated unit tests
- [ ] Added/Updated e2e tests

## üåê References

_Does this depend on other work, documents, or tickets?_

- **Issue**: Closes [#4923](https://github.com/streamlit/streamlit/issues/4923)

---

**Contribution License Agreement**

By submitting this pull request you agree that all contributions to this project are made under the Apache 2.0 license.
",Add functionality to convert sets to lists when serializing JSON data.
825,"[generation] consistently add eos tokens
Currently beam search returns inconsistent outputs - if hypos have different lengths we get eos, if they are the same - we don't. I found a sentence where if I change one letter in one of the input words the beam search outputs all suddenly lack eos, which is an inconsistent behavior.

This PR makes the output more consistent. (but not 100%, please see below)

---------


Also why not replace:

```
            if sent_lengths[i] < max_length:
                decoded[i, sent_lengths[i]] = eos_token_id
```
with:
```
            decoded[i, sent_lengths[i]] = eos_token_id
```
Shouldn't eos always be there? If generated data gets truncated, the caller needs to use a larger `max_length`.

Currently, if the hypos lengths are on the cusp of `max_length`, some of them will get eos, whereas others won't, which is again inconsistent.

Please correct me if my logic is flawed.

-----

I also looked at `_generate_no_beam_search` - there eos adding logic uses a somewhat different logic. 

Should the two functions (beam/no_beam) be consistent eos-injection wise?

",Add padding logic for shorter batches and eos token handling.
826,"Add `dtype=float32` to PadSequencesTest
For function `keras.utils.pad_sequences`, it says

    sequences: List of sequences (each sequence is a list of integers).

but floats here also work. This commit add test for float type

Modifications to the function itself and document may involve API stability, so no changes have been made, further consideration may be needed",Add support for padding and truncating sequences in `pad_sequences` function.
827,"Continue on Docker failure (all backends)
Signed-off-by: Glenn Jocher 

<!--
Thank you for submitting a YOLOv5 üöÄ Pull Request! We want to make contributing to YOLOv5 as easy and transparent as possible. A few tips to get you started:

- Search existing YOLOv5 [PRs](https://github.com/ultralytics/yolov5/pull) to see if a similar PR already exists.
- Link this PR to a YOLOv5 [issue](https://github.com/ultralytics/yolov5/issues) to help us understand what bug fix or feature is being implemented.
- Provide before and after profiling/inference/training results to help us quantify the improvement your PR provides (if applicable).

Please see our ‚úÖ [Contributing Guide](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md) for more details.
-->


## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Improved GitHub Actions workflow for YOLOv5 Docker image deployment üöÄ

### üìä Key Changes
- Enabled `continue-on-error: true` in the GitHub Actions workflow for the Docker job.
- The `continue-on-error` parameter has been moved from the ""Build and push arm64 image"" step to the job level.

### üéØ Purpose & Impact
- üõ†Ô∏è **Purpose:** Ensures that the entire Docker job can proceed in the face of errors, rather than stopping at the first failure within a specific step.
- ‚úÖ **Impact:** This should improve the robustness of the continuous integration/continuous deployment (CI/CD) pipeline. Users can expect more consistent Docker image updates, even if one part of the build process encounters issues.","Add `continue-on-error: true` to the ""Push Docker image to Docker Hub"" job."
828,"Unicode string that often causes rendering issues
Adding string containing three ‡∏î‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá‡πá‡πâ‡πâ‡πâ‡πâ‡πâ‡πá‡πá‡πá‡πá characters that often causes rendering issues.","Update to the Unicode Subscript/Superscript/Accents section in blns.txt, adding a new paragraph with Thai characters."
829,"Fix http.Response.make type annotation
",Update type hint for `headers` from `Dict` to `Mapping` in `make` function.
830,"Update tasks.json (Catalan translation).
Fixed Catalan translation:
- Given uniformity regarding formality (tu-v√≥s)
- Homogenized terminology
- Fixed typos",Update translations for tasks.json file.
831,"Roll back widget_id (commit 0577857)
Turns out the original commit broke the case where one widget depends on the output from another:

```py
country = st.selectbox(""Country"", countries)
states = country_state_map[country]
state = st.selectbox(""State"", states)
```",Changes to DeltaGenerator and tests for DeltaGenerator.
832,"Remove false positives
I currently do not have enough time to find a fix for the false positives. So those sites have been removed for now.","### DIFF:Changes to removed_sites.json and removed_sites.md files.Added new sites to the list of removed sites, including 500px, Badoo, Instagram, Pling, Realmeye, and Travellerspoint. These sites now return false positives.Changes to data.json file.Removed sites from the list of removed sites, including 500px, Badoo, Instagram, Pling, Realmeye, and Travellerspoint. These sites are now added to the list of removed sites in the removed_sites.json and removed_sites.md files.Note: The changes are related to the removal of sites from the list of removed sites and the addition of new sites to the list."
833,"[Whisper] Move decoder id method to tokenizer
# What does this PR do?

Moves the method `get_decoder_prompt_ids` from the processor to the tokenizer. The primary reason for this change is that the ASR pipeline class does not load the processor object, but rather the feature extractor and tokenizer separately (see [docs](https://github.com/huggingface/transformers/blob/699e90437f984d69ad3c9b891dd2e9d0fc2cffe4/src/transformers/pipelines/automatic_speech_recognition.py#L123)). Therefore, as things currently stand, pipeline does not have access to the processor method `get_decoder_prompt_ids`. By moving it to the tokenizer, it will be able to call this method with pipeline.

Note that this is not a breaking change: we retain a method `get_decoder_prompt_ids` in the processor. This method simply calls the `get_decoder_prompt_ids` from the tokenizer:

https://github.com/huggingface/transformers/blob/ca8b332d31a1b90e18f134620e69063418add69e/src/transformers/models/whisper/processing_whisper.py#L44-L45


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->
",Update language and task handling in `get_decoder_prompt_ids` method.
834,"Toward Devkit Consistency
## ÁõÆÊ†á

‰øÆÊîπ‰ª£Á†Å‰ª•ÊèêÂçáPaddleCVÂ•ó‰ª∂‰∏ÄËá¥ÊÄßÔºåÂêåÊó∂ÈÄÇÈÖçXÈ°πÁõÆ„ÄÇ‰øÆÊîπÂêéÔºåÂ•ó‰ª∂Â∫îÊª°Ë∂≥Â¶Ç‰∏ã‰∏ÄËá¥ÊÄßË¶ÅÊ±ÇÔºö

1. Â∞ÜËÆ≠ÁªÉËøáÁ®ã‰∏≠ÊúÄ‰ºòÔºà‰∏ÄËà¨ÊòØÈ™åËØÅÈõÜ‰∏äÁ≤æÂ∫¶ÊúÄÈ´òÔºâÁöÑÊ®°ÂûãÊùÉÈáçÂ≠òÂÇ®Âú®ËæìÂá∫ÁõÆÂΩïÁöÑ`best_model`Â≠êÁõÆÂΩï‰∏≠ÔºåÊñá‰ª∂ÂëΩÂêç‰∏∫`model.pdparams`„ÄÇ‰∏é‰πãÁõ∏ÈÖçÂ•óÁöÑ‰ºòÂåñÂô®ÂèÇÊï∞ÔºàÂ¶ÇÊûúÂ≠òÂÇ®ÁöÑËØùÔºâÊñá‰ª∂ÂëΩÂêç‰∏∫`model.pdopt`Ôºå‰πüÂ≠òÂÇ®Âú®ËØ•ÁõÆÂΩï‰∏≠„ÄÇ
2. ÊîØÊåÅÂ∞ÜËÆ≠ÁªÉ„ÄÅÈ™åËØÅËøáÁ®ã‰∏≠VisualDL‰∫ßÁîüÁöÑ.logÊ†ºÂºèÊó•ÂøóÊñá‰ª∂‰øùÂ≠òÂú®ËæìÂá∫ÁõÆÂΩï‰∏≠ÔºàÊó†ÂµåÂ•óÂ≠êÁõÆÂΩïÔºâ„ÄÇ
3. Â•ó‰ª∂Ê†πÁõÆÂΩïÂ≠òÂú®`requirements.txt`Êñá‰ª∂ÔºåÊåáÂÆö‰ΩøÁî®Â•ó‰ª∂Âü∫Á°ÄÂäüËÉΩÈúÄË¶ÅÁöÑ‰æùËµñ„ÄÇÂú®Â•ó‰ª∂Ê†πÁõÆÂΩïÂèØÈÄöËøá`pip install .`Êàñ`pip install -e .`ÊñπÂºèÔºàËá≥Â∞ëÂÖ∂‰∏≠‰∏ÄÁßçÔºâÂÆâË£ÖÂ•ó‰ª∂Ê†∏ÂøÉÂ∫ì„ÄÇ
4. ÂØπ‰∫éÊ®°ÂûãÂØºÂá∫ÂäüËÉΩÔºåÈªòËÆ§ÊîØÊåÅÊàñÈÄöËøáÂëΩ‰ª§Ë°åÈÄâÈ°π/ÈÖçÁΩÆÊñá‰ª∂Á≠âÊâãÊÆµÊîØÊåÅÂØºÂá∫‰∏∫FDÊ†ºÂºè„ÄÇ„ÄéFDÊ†ºÂºè„ÄèÁöÑÂØºÂá∫ÁªìÊûúÈÄöÂ∏∏ÂåÖÂê´Â¶Ç‰∏ãÊñá‰ª∂Ôºö
- `inference.pdiparams`Ôºö‰øùÂ≠òÊùÉÈáçÂèÇÊï∞„ÄÇ
- `inference.pdiparams.info`Ôºö‰øùÂ≠ò‰∏éÂèÇÊï∞ÊúâÂÖ≥ÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ
- `inference.pdmodel`Ôºö‰øùÂ≠òÊ®°ÂûãÁªìÊûÑÊèèËø∞‰ø°ÊÅØ„ÄÇ
- ÔºàÂèØÈÄâÔºâ`inference.yml`ÔºöÈ¢ÑÂ§ÑÁêÜÈÖçÁΩÆÊñá‰ª∂„ÄÇ

## ‰ª£Á†ÅÂèòÂä®

1. „Äê‰∏ÄËá¥ÊÄßÊèêÂçá„Äë‰ΩøËÆ≠ÁªÉÊó∂Ê£ÄÊü•ÁÇπÂ≠òÂÇ®‰ΩçÁΩÆÊª°Ë∂≥‰∏ÄËá¥ÊÄßË¶ÅÊ±Ç1„ÄÇ
2. „Äê‰∏ÄËá¥ÊÄßÊèêÂçá„Äë„Äê**‰∏çÂÖºÂÆπÂçáÁ∫ß**„Äë‰øÆÊîπVisualDLÊó•ÂøóÂ≠òÂÇ®‰ΩçÁΩÆ‰ª•Êª°Ë∂≥‰∏ÄËá¥ÊÄßË¶ÅÊ±Ç2„ÄÇ
3. „ÄêÂäüËÉΩÂ¢ûÂº∫„Äë‰øÆÊîπ`tools/infer_det.py`‰∏≠`raw_det_res`ÂØπ„ÄéÈõ∂ÁõÆÊ†áÊ°Ü„ÄèÊÉÖÂΩ¢ÁöÑÂ§ÑÁêÜÈÄªËæëÔºåÂú®ËØ•ÊÉÖÂΩ¢‰∏ã‰ªçÁÑ∂Â≠òÂÇ®ÂõæÂÉèÔºàÂ≠òÁöÑÊòØÂéüÂõæÔºâÔºå‰ªéËÄå‰ΩøÁî®Êà∑Âú®ÂêÑÁßçÊÉÖÂÜµ‰∏ãÂùáËÉΩËé∑ÂæóËæìÂá∫ÂõæÂÉèÔºåËÄåÊó†ÈúÄÂÖ≥Ê≥®ÁõÆÊ†áÊ°ÜÊï∞Èáè„ÄÇ
4. „ÄêÂäüËÉΩÂ¢ûÂº∫„ÄëË∞ÉÊï¥signal handlersÁöÑËÆæÁΩÆÊó∂Êú∫„ÄÇ‰øÆÊîπÂâçÔºåsignal handlersÂú®ÊØèÊ¨°ËΩΩÂÖ•`ppocr.data`Êó∂Ëá™Âä®ËÆæÁΩÆÔºå‰ΩÜËøôÁßçÂÅöÊ≥ïÊúâ‰ª•‰∏ãÁº∫ÁÇπÔºö1) Áî®Êà∑Âú®‰ΩøÁî®Python REPLÊàñËøõË°åÊñ≠ÁÇπË∞ÉËØïÊó∂ÁªèÂ∏∏ÈúÄË¶ÅËæìÂÖ•Ctrl+CÔºåËÄåËøôÂ∞ÜÁõ¥Êé•Ëß¶ÂèëÁ®ãÂ∫èÁöÑÁªàÊ≠¢Ôºå‰ª§Áî®Êà∑ÊÑüÂà∞ÁñëÊÉëÂíå‰∏ç‰æøÔºõ2) ‰ªéË∞ÉÁî®ÂÖ≥Á≥ªÁöÑËßíÂ∫¶Êù•ÁúãÔºå‰Ωú‰∏∫calleeÔºåÈÄªËæë‰∏äÊù•ËØ¥Â∫îÂΩìÂ∞ΩÂèØËÉΩÂáèÂ∞ëÂØπcallerÁöÑÂΩ±ÂìçÔºåËÄåÁõÆÂâçÁöÑÂÆûÁé∞ÊòæÁÑ∂‰∏çÊª°Ë∂≥Ëøô‰∏ÄÁÇπÔºõ3) Â¶ÇÊûúÂ∫ìÂú®Â≠êËøõÁ®ã‰∏≠Ë¢´Ë∞ÉÁî®Ôºå‰Ωú‰∏∫Â≠êËøõÁ®ãÔºåÂç≥‰Ωø‰∏çËÄÉËôëÊùÉÈôêÈóÆÈ¢òÔºåÂè™‰ªéËµÑÊ∫êÂõûÊî∂ÁöÑËßíÂ∫¶Êù•ÁúãÔºåËØïÂõæÈÄöËøá`SIGKILL`ÊùÄÊ≠ªÂåÖÊã¨Áà∂ËøõÁ®ã‰ª•ÂèäÂÖ∂ÂÆÉÊóÅÊîØËøõÁ®ãÂú®ÂÜÖÁöÑÊï¥‰∏™ËøõÁ®ãÁªÑÂÜÖÁöÑÊàêÂëò‰ªçÊòØÂç±Èô©ÁöÑË°å‰∏∫„ÄÇ‰øÆÊîπÂêéÔºåsignal handlersÂè™Âú®ÂΩìÂâçËøõÁ®ã‰∏∫ËøõÁ®ãÁªÑleader„ÄÅ‰∏îÁî®Êà∑ÈÄöËøáËÑöÊú¨ÊâßË°åÔºàÊéíÈô§‰∫ÜREPLÁöÑÊÉÖÂÜµÔºâÁöÑÊÉÖÂÜµ‰∏ãË¢´ËÆæÁΩÆ„ÄÇÊ≠§ÂÅöÊ≥ï‰ªçÊúâ‰ºòÂåñÁ©∫Èó¥ÔºåËØ¶ËßÅ`ppocr/data/__init__.py`‰∏≠ÁöÑÊ≥®Èáä„ÄÇ

## ÈÅóÁïôÈóÆÈ¢ò

1. ÂØπ‰∫é‰ª£Á†ÅÂèòÂä®1Âíå2Ôºå**Â∞öÊú™Êõ¥Êñ∞Áõ∏ÂÖ≥ÊñáÊ°£**„ÄÇËã•Ê≠§PRÂêàÂÖ•ÔºåÂèØËÉΩÈúÄË¶ÅÂ•ó‰ª∂ÂêåÂ≠¶ËøõË°åÊõ¥Êñ∞„ÄÇ
2. ËÄÉËôëÂà∞ÂèçÂêëÂÖºÂÆπÊÄßÔºåÊú¨Ê¨°ÊîπÂä®Êú™ÂØπPaddleOCRÂéüÊú¨ÁöÑÊúÄ‰ºòÊ®°ÂûãÂ≠òÂÇ®Êú∫Âà∂ËøõË°åÂèòÊõ¥ÔºåËÄåÂè™ÊòØËøΩÂä†‰∫Ü`best_model`ÁõÆÂΩï„ÄÇÂõ†Ê≠§ÔºåËæìÂá∫ÁõÆÂΩï‰∏≠Â∞ÜÂ≠òÂú®`model.pdparams`Âíå`model.pdopt`ÁöÑ‰∏§‰ªΩÊã∑Ë¥ù„ÄÇÂêéÁª≠ÈúÄËÄÉËôëÊ∏êËøõÂú∞ÊîπÂä®ÊóßÁöÑÂ≠òÂÇ®Êú∫Âà∂Ôºå‰ΩøÊúÄ‰ºòÊ®°ÂûãÂè™‰øùÁïô‰∏Ä‰ªΩ„ÄÇ","### DIFF:Changes were made to the PP-OCR codebase to add signal handling for the `set_signal_handlers` function. This function is used to set signal handlers for the current process, allowing it to catch and handle signals such as SIGINT (Ctrl+C) and SIGTERM.The changes were made in the following files:* `ppocr/data/__init__.py`: The `set_signal_handlers` function was added to the `ppocr.data` module.* `deploy/slim/prune/export_prune_model.py`, `deploy/slim/prune/sensitivity_anal.py`, `deploy/slim/quantization/export_model.py`, `deploy/slim/quantization/quant.py`, `deploy/slim/quantization/quant_kl.py`, `tools/eval.py`, `tools/export_center.py`, `tools/infer_det.py`, and `tools/train.py`: The `set_signal_handlers` function was called in these files to set signal handlers for the current process.These changes allow the PP-OCR codebase to catch and handle signals such as SIGINT (Ctrl+C) and SIGTERM, which can be useful for debugging and testing purposes."
835,"Spider constructor expects name as argument, not start_urls.
Fixes #4644",Update the `__init__` method of `StartUrlsSpider` to pass `name='StartUrlsSpider'` to the superclass.
836,"add param m3u8 , allow download video via m3u8 url
",Added support for downloading videos using m3u8 URLs.
837,"Fix ill-formed example C.65 (missing noexcept on declaration)
There cannot be a mismatch between the exception specification of a declaration and definition. The example in its current form fails to compile.",Add noexcept keyword to Foo& operator=(Foo&& a) in Foo class.
838,"Fix #903: docs version parsing
",Update the regular expression pattern to use single quotes instead of double quotes.
839,"Edited few more answers in the Linux section and fixed some typos
Please review and merge if things are fine.
Thanks,
susenj","Update to README.md: Added explanations for ""throughput"" and ""chmod"" commands, and added information about the purpose of ""~/.ssh/known_hosts"" file."
840,"JSONLoader Documentation Fix
- Description: 
Updated JSONLoader usage documentation which was making it unusable
- Issue: JSONLoader if used with the documented arguments was failing on various JSON documents. 
- Dependencies: 
no dependencies
- Twitter handle: @TheSlnArchitect

If no one reviews your PR within a few days, please @-mention one of @baskaryan, @eyurtsev, @hwchase17, @rlancemartin.
 -->
",Update JSONLoader configuration to include `text_content=False` and add `json_lines=True` option.
841,"Merge dev branch
","New file: extensions/openai/logits.py, containing functions `_get_next_logits` and `process_parameters` from `openai.completions` and `logits` modules, respectively."
842,"Remove unnecessary period in documentation
Except for this one, I also notice that there are [two sentences](http://flask.pocoo.org/docs/dev/testing/#testing-cli-commands) end with period instead of colon before a code snippet, should we change it?","Update documentation: changed text in ""docs/testing.rst"""
843,"cherry-pick #3236
",Add a new condition to break the loop and reset the index `i` to 0.
844,"C.138 initial content
Proposed content to the stub rule C.138 ""Create an overload set for a derived class and its bases with using"", based on bs_faq2 entry [Why doesn't overloading work for derived classes?](http://www.stroustrup.com/bs_faq2.html#overloadderived) and openoffice guideline on [Name hiding](https://wiki.openoffice.org/wiki/Writing_warning-free_code#Name_hiding).",Change the documentation of C++ Core Guidelines to include an example of using a using declaration to expose a hidden overload set.
845,"Fix some TF slow tests
# What does this PR do?

This PR fixes several slow tests related to saved model creation.
",Update the global attention mask calculation in `modeling_tf_longformer.py` to use the input IDs to determine which tokens to attend to.
846,"Check for GPU or MPS availability before using CPU
### Describe the changes you have made:

In `point.py`, check for GPU or MPS before defaulting to CPU.

### Reference any relevant issues (e.g. ""Fixes #000""):

- Fixes https://github.com/OpenInterpreter/open-interpreter/issues/1131


### Pre-Submission Checklist (optional but appreciated):

- [x] I have included relevant documentation updates (stored in /docs)
- [x] I have read `docs/CONTRIBUTING.md`
- [x] I have read `docs/ROADMAP.md`

### OS Tests (optional but appreciated):

- [ ] Tested on Windows
- [x] Tested on MacOS
- [ ] Tested on Linux
","Change the device assignment logic to use the available device (CPU, CUDA, or MPS) if available, otherwise fall back to CPU."
847,"Make A Better Readme
I added emoji's and a better introduction to make the README.md look nicer.",Update README.md to include new features and updates.
848,"[eval] update llama npu eval
## üìå Checklist before creating the PR

- [x] I have created an issue for this PR for traceability
- [x] The title follows the standard format: `[doc/gemini/tensor/...]: A concise description`
- [x] I have added relevant tags if possible for us to better distinguish different PRs


## üö® Issue number

> Link this PR to your issue with words like fixed to automatically close the linked issue upon merge
>
> e.g. `fixed #1234`, `closed #1234`, `resolved #1234`



## üìù What does this PR do?

> Summarize your work here.
> if you have any plots/diagrams/screenshots/tables, please attach them here.



## üí• Checklist before requesting a review

- [ ] I have linked my PR to an issue ([instruction](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue))
- [ ] My issue clearly describes the problem/feature/proposal, with diagrams/charts/table/code if possible
- [ ] I have performed a self-review of my code
- [ ] I have added thorough tests.
- [ ] I have added docstrings for all the functions/methods I implemented

## ‚≠êÔ∏è Do you enjoy contributing to Colossal-AI?

- [x] üåù Yes, I do.
- [ ] üåö No, I don't.

Tell us more if you don't enjoy contributing to Colossal-AI.
",Update the code to use the current device instead of hardcoding the CUDA device.
849,"[Test] [Cherrypick] Fix agent stress failure
Signed-off-by: SangBin Cho 





## Why are these changes needed?

Cherry pick agent stress test failure fix

## Related issue number

Closes https://github.com/ray-project/ray/issues/30494

## Checks

- [ ] I've signed off every commit(by using the -s flag, i.e., `git commit -s`) in this PR.
- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested :(
",Update Atari YAML configuration to use Python workload instead of PPO.
850,"expose AWS_REGION_NAME in S3FeedStorage
Similar PRs:
- https://github.com/scrapy/scrapy/pull/5092
- https://github.com/scrapy/scrapy/pull/4429
- https://github.com/scrapy/scrapy/pull/4417","Added support for custom ACL, custom endpoint, and region name for exported feeds."
851,"Fix health check for HTTPS endpoints with self-signed certs
Fix health check for HTTPS endpoints with self-signed certificates - fixes #2387","Update README.md to include new feature ""QLDB"" and modify safe_requests.get to allow for insecure connections."
852,"[Classifier]: Replace print with LOGGER
This PR replaces all top level print statements in classifier.py with LOGGER

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Switch from print statements to a unified logger in the classifier script.

### üìä Key Changes
- Replaced `print` function calls with `LOGGER.info` for better logging management.
- Changes applied across various functions including `train`, `test`, `classify`, and `imshow`.

### üéØ Purpose & Impact
- üé® **Enhanced Readability**: By using a centralized logging system, the output is standardized and formatted consistently.
- üîç **Improved Debugging**: Centralized logs make it easier to troubleshoot issues by looking at a single stream of log data.
- üìà **Easier Integration**: Logger setup can later be configured for different log levels and destinations (e.g., files, monitoring systems), making the classifier module more versatile in various environments.
- üë• **User Experience**: For end-users, this change means cleaner console output and potentially access to log files for a historical record of operations.",Update import statement for LOGGER.
853,"Fix account restoration on renewal
Fixes https://github.com/certbot/certbot/issues/9805.","Fixed a bug that caused the ACME account to not be properly restored on renewal, causing problems in setups where the user had multiple accounts with the same ACME server. Additionally, improved the restoration of renewal parameters to handle related values correctly."
854,"Homogenize Steering Fault Avoidance
Cleaned up the steering fault avoidance into a common function, since it will be common across a lot of cars.",### DIFF:Added common fault avoidance mechanism for EPS limits by cutting the request bit of LKAS message after a certain number of frames above the limit.
855,"[Veo] Add new extractor
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

https://github.com/ytdl-org/youtube-dl/issues/29445
",Added Veo extractor to yt-dlp.
856,"docs: improve readability of Langchain Expression Language get_started.ipynb
**Description:** A few grammatical changes to improve readability of the LCEL .ipynb and tidy some null characters.
**Issue:** N/A",Change the wording of the code comments to improve clarity and consistency.
857,"bpo-36710: Add runtime variable to Py_FinalizeEx()
* Add a 'runtime' variable to Py_FinalizeEx() rather than working
  directly on the global variable _PyRuntime
* Add a 'runtime' parameter to _PyGC_Fini(), _PyGILState_Fini()
  and call_ll_exitfuncs()


https://bugs.python.org/issue36710

",Change the function signature of `_PyGILState_Fini` and `_PyGC_Fini` to include `_PyRuntimeState *runtime` as a parameter.
858,"Generate localhost keys for tests on the fly
Fixes #4651",### Description of Pull Request:This pull request adds functionality to generate self-signed certificates for use in tests. It includes the creation of a new file `tests/keys/__init__.py` which contains the logic for generating the certificates. The certificates are stored in files `localhost.crt` and `localhost.key` in the `tests/keys` directory. The pull request also updates the `conftest.py` file to include the necessary imports and calls to the certificate generation function.
859,"Correctly handle single-byte Content-Range
HTTPie fails if it continues a download with a single byte left. For example:
```
$ http -b -d example.org
Downloading 1.23 kB to ""index.html""
Done. 1.23 kB in 0.00043s (2.81 MB/s)
$ python3 -c 'import sys; sys.stdout.write(open(""index.html"").read()[:-1])' > index-short.html
$ http -b -d -c -o index-short.html example.org

http: error: ContentRangeError: Invalid Content-Range returned: 'bytes 1255-1255/1256'
```
The range is inclusive on both ends.",Update ContentRangeError handling to correctly raise exceptions for invalid byte-range-resp-spec.
860,"Fix feedback from input api key
Removed getpass, added prompt_toolkit.
Now shows * when typing characters

### Describe the changes you have made:

### Reference any relevant issues (e.g. ""Fixes #000""):

### Pre-Submission Checklist (optional but appreciated):

- [ ] I have included relevant documentation updates (stored in /docs)
- [x] I have read `docs/CONTRIBUTING.md`
- [ ] I have read `docs/ROADMAP.md`

### OS Tests (optional but appreciated):

- [x] Tested on Windows
- [ ] Tested on MacOS
- [ ] Tested on Linux
",Update encoding in get_config function to be UTF-8.
861,"Rename of docs
some rename of .rst files

I tried to be smart about just renaming references to the client, but I've also changed some urls to certbot/certbot which will have to wait for the GH repo to be moved
",Update copyright notice to include Apache 2.0 license information and a link to the license terms.
862,"Build certbot snap without the isolation build
Fixes #8252

With @bmw we digged quite a lot on why the failure happens on ARM snap, and here we what we understood:
* the failure occurs since the version 50 of setuptools is available
* normally, we should not be impacted because the setuptools version used in the snap build should be the one installed by the `core20` base snap, because the build occurs in a `venv` created with `--system-site-packages`
* BUT associated with the build isolation provided by recent versions of pip (to implement PEP 517), a bad interaction happens: following the definition of the build system provided by `cryptography`, pip installs the most recent version of setuptools on a separate path for the build (because `cryptography` just asks for a minimal version of `setuptools`), then features of this version conflict with the old version of `setuptools` initially present
* the exact interaction is described here: https://github.com/pypa/pip/issues/6264#issuecomment-685230919. Basically the new version of `setuptools` triggers some hacks, that are then applied at runtime on the old version of `setuptools` that is also still available in `sys.path` at this point, and breaks the build.

To fix that, one can disable the isolation build on cryptography, by passing `PIP_NO_ISOLATION_BUILD=no` to pip. It is the purpose of this PR.

This will have the consequence to not be PEP 517 compliant: if needed the `cryptography` library will be built using the `setuptools` available in the system. In general I think it makes sense for the snap build purpose, since we control precisely the build environment, and makes consistent build that will not be broken by a new version of a build system if library maintainers did not provide a strict version of it in their build requirements. However we need now to take care about having a compatible build system for all libraries that may have specific requirements in their build system using the PEP 517 definition in `pyproject.toml`.

I think as of now that it is a safe move if we keep using the most recent version of `setuptools` available in Ubuntu 20.04, and it is the case here for snap builds. It may however be problematic if some libraries require another build system than `setuptools` and do not provide a fallback to a `setuptools` build. For the record, `dns-lexicon`, that I maintain, uses `poetry` and so a PEP 517 compliant definition of a build system, but provides also this fallback (https://github.com/AnalogJ/lexicon/blob/master/setup.py).


","Add PIP_NO_BUILD_ISOLATION: ""no"" to build-environment."
863,"Fix typo in doc comment of BitsAndBytesConfig
# What does this PR do?

Fix a typo in doc comments of `BitsAndBytesConfig` class.

It must be `nf4` instead of `fn4`.

Here is a comparison code in current main branch:

https://github.com/huggingface/transformers/blob/539e2281cd97c35ef4122757f26c88f44115fa94/src/transformers/utils/quantization_config.py#L166-L167

`qlora` implementations also uses `nf4`.

https://github.com/artidoro/qlora/blob/bdc655dfa71e5ef1553a078980fb5083c346a4cf/qlora.py#L138-L141

So I think it is better to fix doc, not a code.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@sgugger, @stevhliu @MKhalusova @younesbelkada and @sourabh112

Or @TimDettmers?
",Update `fp4` to `nf4` in `bnb_4bit_quant_type` parameter.
864,"Prefer subprocess run over call
1. Fix resource warning in the destructor
```
/Users/REDACTED/.asdf/installs/python/3.10.0/lib/python3.10/subprocess.py:1067: ResourceWarning: subprocess 46612 is still running
  _warn(""subprocess %s is still running"" % self.pid,
ResourceWarning: Enable tracemalloc to get the object allocation traceback

```

2. Use preferred, non-legacy API per https://docs.python.org/3/library/subprocess.html#subprocess.call
3. Older call style no longer needed with Python 3.6+",Update subprocess calls to use subprocess.run instead of subprocess.call.
865,"Fix fused qkv sizing for bloom
Fix bug mentioned in https://github.com/microsoft/DeepSpeed/pull/4157.

Thanks @dc3671 and @inkcherry ",Update the _bloom_type_transpose function to return the sliced input instead of the original input.
866,"change the variables
I have changed the variables according to the standard sorting algorithms",Change function signature from `bubble_sort(nums)` to `bubble_sort(Lists)` and update variable name from `nums` to `Lists`.
867,"Ignore broken E203
See https://github.com/python/black/issues/565",Update to Black's.flake8 config file to ignore E203 in addition to E501 and W503.
868,"Update CLI usage docs for --csr
With #2403 and #3046, `certbot` gained the ability to parse CSRs encoded as PEM and without a SAN extension. Update the CLI usage docs to reflect this change.
",Update help text for `--csr` option to allow PEM format and remove note about Subject Alternative Name field.
869,"improve docker image size
# Description

Improves the docker image size. Summary of various changes

* Added `--no-install-recommends`, newlined install items and removed the apt list and cache as per [best practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run) 
* Merged the `mujoco` RUN directive with the `apt` RUN directive to reduce layers
* Added `--no-cache-dir` to the `pip` installs since the pip cache isn't needed for a docker image
* `mujoco` is now downloaded and piped to tar to location, previously it the `mujoco210-linux-x86_64.tar.gz` was left in the docker image when it wasn't needed
* Added `SHELL [""/bin/bash"", ""-o"", ""pipefail"", ""-c""]` for safety
* Added quotes around various strings to ensure splitting doesn't occur

Followed rules from [hadolint](https://github.com/hadolint/hadolint) and [shellcheck](https://github.com/koalaman/shellcheck)


Running `docker build -t gym-orig --build-arg=PYTHON_VERSION=3 -f py.Dockerfile . --no-cache` with the original `Dockerfile`, the size (compressed) came out to 1.61GB and the same command with the `gym-new` tag it comes down to `1.45GB` compressed

```bash
$ docker images | grep gym
gym-new                                                                      latest          ab7f651f39f3   17 seconds ago   1.45GB
gym-orig                                                                     latest          aa7c83ec05ea   24 seconds ago   1.61GB
```

Similarly with the same build comparison but for x86 (since I'm on an M1 MacBook), i.e. ```DOCKER_DEFAULT_PLATFORM=linux/amd64 docker build -t gym-new --build-arg=PYTHON_VERSION=3 -f py.Dockerfile . --no-cache;```

```bash
$ docker images | grep gym
gym-orig                                                                     latest          9f513111ab2e   12 seconds ago   2GB
gym-new                                                                      latest          241544f82ab2   30 seconds ago   1.75GB
```

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

### Screenshots
Please attach before and after screenshots of the change if applicable.

<!--
Example:

| Before | After |
| ------ | ----- |
| _gif/png before_ | _gif/png after_ |


To upload images to a PR -- simply drag and drop an image while in edit mode and it should upload the image directly. You can then paste that source into the above before/after sections.
-->

# Checklist:

- [] I have run the [`pre-commit` checks](https://pre-commit.com/) with `pre-commit run --all-files` (see `CONTRIBUTING.md` instructions to set it up)
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes

<!--
As you go through the checklist above, you can mark something as done by putting an x character in it

For example,
- [x] I have done this task
- [ ] I have not done this task
-->
","Update Dockerfile to use `apt-get autoremove` and `apt-get clean` to remove unnecessary packages, and to download mujoco using `wget` and `tar`."
870,"BUG ensure that estimator_name is properly stored in the ROC display
We were storing `estimator.__class__.__name__` instead of `name` which wrongly label the curve when calling `disp.plot()` for the second time.",Fixed a bug in `metrics.plot_roc_curve` where the name of the estimator was passed in the `metrics.RocCurveDisplay` instead of the `name` parameter. This results in a different plot when calling `metrics.RocCurveDisplay.plot` for the subsequent times.
871,"Fix tests
And improve our `certbot-compatibility-test` Dockerfile as we never needed these dependencies.",Remove unnecessary environment variable.
872,"Stop auto queue on error
Today if there is an error the auto queue keeps queuing in loop, over and over again, this PR fix it.",Update condition for queue prompt to include check for app.lastExecutionError.
873,"Update pipeline.rst - Fix query in `merge_data()` task
The alias for the subquery was missing in the `merge_data()` function's query, so it just needed to be added. Otherwise, the `employees` table remains empty. I found the error in the postgres container logs:
```
2023-01-25 08:00:03 2023-01-25 14:00:03.256 UTC [70428] ERROR:  subquery in FROM must have an alias at character 74
2023-01-25 08:00:03 2023-01-25 14:00:03.256 UTC [70428] HINT:  For example, FROM (SELECT ...) [AS] foo.
2023-01-25 08:00:03 2023-01-25 14:00:03.256 UTC [70428] STATEMENT:  
2023-01-25 08:00:03                 INSERT INTO employees
2023-01-25 08:00:03                 SELECT *
2023-01-25 08:00:03                 FROM (
2023-01-25 08:00:03                     SELECT DISTINCT *
2023-01-25 08:00:03                     FROM employees_temp
2023-01-25 08:00:03                 )
2023-01-25 08:00:03                 ON CONFLICT (""Serial Number"") DO UPDATE
2023-01-25 08:00:03                 SET ""Serial Number"" = excluded.""Serial Number"";
```

Thank you!

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of an existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).
",Update pipeline query to include additional columns in the ON CONFLICT clause.
874,"Bugfix to Tool - Mask - add missing
Corrects error from variable mis-spelling.",Change the argument `detected_faces` to `detected_faces` in the `queue.put` call.
875,"Console: add error message for older Windows versions
This fixes #4908.",Error handling for Windows 10 Build 10586 or above.
876,"Fixed test modifying data created in setUpTestData
",Update the user refresh from db to directly retrieve the user object from the database.
877,"Expose full ClientHello object in tls_clienthello hook
This PR makes it possible to access the raw [`ClientHello`](https://github.com/mitmproxy/mitmproxy/blob/c4dd46c3db97eabedf14fc54a23dae8d167fdad2/mitmproxy/net/tls.py#L284) object in the `tls_clienthello` hook as `data.client_hello`.

```python
def tls_clienthello(data):
    print(data.client_hello)
```",Update the `ClientHelloData` class to include the `client_hello` attribute and add a docstring to describe its purpose.
878,"Fix race in websocket broadcaster
",Update the `broadcast` function to use a copy of `cls.connections` when iterating over connections.
879,"Optimized numpy functions in Converter
This PR optimized the conversion speed.
This is done mainly by choosing more performant variants for the numpy processing in the `Converter` class.

The Converter now also gets whole batches from the `Predict.queue_out_frames` method. This was mainly done to enable batch processing and potential replacing numpy functions with keras ones later on, but it also increased my speed by about 3it/s.

Optimizing the worker Plugins remains to be done. They did not take enough time in my setup to be really relevant. This may be different with other settings.

Benchmarks:
- Current staging: 16.45 it/s
- This PR WITHOUT providing whole batches to the converter: 21.40 it/s
- This PR as is: 24.69 it/s

Tested on 10k frames with a resolution of 1280x720.
The tested model on my system can do raw prediction (without anything else) with about 37.2 it/s
",### Change in Converter class: The Converter class has been modified to process a list of items from the in_queue instead of a single item. This change allows the converter to process multiple images in a single batch. The process method now iterates over the items in the in_queue and applies the patch_image method to each item. The patched faces are then put into the out_queue. The get_new_image method has been modified to return a tuple containing the new image and the background image. The post_warp_adjustments method has been modified to take the background image as an argument.
880,"Clarify result of lower_bound
To me, the previous formulation made it sound like the return value is always `last` if `val` is not in the range. It seems confusing to me, hence the clarification.",Update documentation for `lower_bound` function to include additional information about its return value.
881,"Improve language just a bit
",Update documentation to correct grammar and improve readability.
882,"Configurable JPG downscale threshold
## Allowing the user to configure the image downscale parameters in setting

### this PR is a alternative solution to the issue describe by PR [#7549 Changed flag for 4chan from 4MB to 2MB. Real limit](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/7549)

this exposes the image downscale parameters in settings
allowing the user to configure downscale size to their specific needs

### changes

- added 2 new settings entries

1. `target_side_length` defalut to 4000 (same as original)
2. `img_downscale_threshold`  defalut to 4 MB (same as original)

- changes to the description of `export_for_4chan`

### backwards compatibility
no issue
as this change uses the default values the current behavior is not changed",Changes to image processing options: - Added `img_downscale_threshold` and `target_side_length` options to control image downsizing for 4chan export.- Updated `export_for_4chan` option to use these new thresholds.- Improved handling of invalid input values for these options.
883,"Update Readme - Include nlp.js
Include nlp.js to javascript natural language processing libraries",Update README.md to include new NLP library.
884,"[ie/NeteaseMusic] more metadata
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information

<!--

Explanation of your *pull request* in arbitrary form goes here. Please **make sure the description explains the purpose and effect** of your *pull request* and is worded well enough to be understood. Provide as much **context and examples** as possible

-->

In the --embed-metadata option, add album title to the file metadata.

Closes #8530



Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check all of the following options that apply:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [ ] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))



Copilot Summary  

<!--
copilot:all
-->
### ü§ñ Generated by Copilot at d3d3824

### Summary
üéµüè∑Ô∏èüõ†Ô∏è

<!--
1.  üéµ - This emoji represents music and can be used to indicate that the change is related to a music extractor or service.
2.  üè∑Ô∏è - This emoji represents a label or tag and can be used to indicate that the change adds more metadata fields to the extracted information.
3.  üõ†Ô∏è - This emoji represents a tool or a fix and can be used to indicate that the change is part of a pull request that aims to improve the extractor and fix some issues.
-->
Add more metadata fields to `NetEaseMusicIE` extractor. Use existing helper functions to parse the JSON response from the API and extract fields such as `album_artist`, `album`, `thumbnail` and `average_rating`.

> _`NetEaseMusic` tracks_
> _More metadata extracted_
> _Winter of content_

### Walkthrough
* Add metadata fields for NetEaseMusic tracks ([link](https://github.com/yt-dlp/yt-dlp/pull/8531/files?diff=unified&w=0#diff-ef518849a6d7b0f3923c20058cda5da3f6931526cf36aadce1eda0f4e7620f5cL256-R265)) by using `traverse_obj` to access the JSON response and helper functions to convert the values




",Added album and album artist information to the NetEaseMusicIE class.
885,"Update `check_python(minimum=3.6.2)`
In looking at the `torch` package they have a minimum of 3.6.2, we should probably stick to that, which I think will keep the R Pi people happy who run on 3.6.9.
https://pypi.org/project/torch/

## üõ†Ô∏è PR Summary

Made with ‚ù§Ô∏è by [Ultralytics Actions](https://github.com/ultralytics/actions)

### üåü Summary
Updated the minimum required Python version for compatibility.

### üìä Key Changes
- The minimum Python version check has been lowered from `3.7.0` to `3.6.2`.

### üéØ Purpose & Impact
- **Purpose:** This change makes the software accessible to users who are running slightly older Python versions.
- **Impact:** Expands the user base by allowing installation on systems with Python `3.6.2+`, while previously systems needed `3.7.0+`. This can be particularly beneficial for those on environments with restrictions on software updates.üîÑ",Update minimum Python version from 3.7.0 to 3.6.2.
886,"Add troubleshooting instructions to the finish_release script for snapcraft credential expiry.
Inspired by the recent release process.",Update error message in `finish_release.py` to include instructions for expired credentials.
887,"TST/CI: disable hypothesis deadline for test_tick_add_sub
- [x] closes #24641 
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry",Update test_ticks.py to use hypothesis settings.
888,"ENH Better spacing & padding in html repr
After having conversations with some users, increasing the spacing & padding was suggested to improve the visualization:

#### PR


#### main


",Changes to CSS styles in `_write_estimator_html` function.
889,"Added CodeFormer to Credits
request from Shangchen Zhou","Added a new link to the README.md file, pointing to the CodeFormer repository."
890,"Checkpoint backwards-compatbility workaround
Fix #2382 
",Add import sys statement in bf16_optimizer.py and fragment_address function in deepspeed/utils/__init__.py.
891,"[requires.io] dependency update on master branch
",Update dependencies in setup.py and tox.ini files.
892,"[TagesschauIE] Add extractor for tagesschau.de (fixes #3049)
Fixes #3049.
",Added support for extracting videos from Tagesschau.de.
893,"Remove subpackages symlinks.
This is a follow-up for merged
https://github.com/letsencrypt/boulder/pull/459.
",Delete two files: letsencrypt_apache and letsencrypt_nginx.
894,"404 when thumb file not found
## Description
Response 404 instead of crash when thumb file is not found.

## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Add a check for file existence and raise an HTTPException if the file is not found.
895,"Fix order of arguments in rainmachine sensors

## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->

Fixes
```
2021-10-16 07:41:21 ERROR (MainThread) [homeassistant.components.binary_sensor] Error while setting up rainmachine platform for binary_sensor
Traceback (most recent call last):
  File ""/usr/src/homeassistant/homeassistant/helpers/entity_platform.py"", line 248, in _async_setup_platform
    await asyncio.shield(task)
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/binary_sensor.py"", line 133, in async_setup_entry
    [
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/binary_sensor.py"", line 134, in 
    async_get_sensor(description.api_category)(entry, controller, description)
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/__init__.py"", line 336, in __init__
    ""configuration_url"": f""https://{entry.data[CONF_IP_ADDRESS]}:{entry.data[CONF_PORT]}"",
KeyError: ip_address
2021-10-16 07:41:21 ERROR (MainThread) [homeassistant.components.sensor] Error while setting up rainmachine platform for sensor
Traceback (most recent call last):
  File ""/usr/src/homeassistant/homeassistant/helpers/entity_platform.py"", line 248, in _async_setup_platform
    await asyncio.shield(task)
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/sensor.py"", line 116, in async_setup_entry
    [
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/sensor.py"", line 117, in 
    async_get_sensor(description.api_category)(entry, controller, description)
  File ""/usr/src/homeassistant/homeassistant/components/rainmachine/__init__.py"", line 336, in __init__
    ""configuration_url"": f""https://{entry.data[CONF_IP_ADDRESS]}:{entry.data[CONF_PORT]}"",
KeyError: ip_address
```






## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [x] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [ ] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [x] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

The integration reached or maintains the following [Integration Quality Scale][quality-scale]:
<!--
  The Integration Quality Scale scores an integration on the code quality
  and user experience. Each level of the quality scale consists of a list
  of requirements. We highly recommend getting your integration scored!
-->

- [ ] No score or internal
- [ ] ü•à Silver
- [ ] ü•á Gold
- [ ] üèÜ Platinum

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html
[quality-scale]: https://developers.home-assistant.io/docs/en/next/integration_quality_scale_index.html
[docs-repository]: https://github.com/home-assistant/home-assistant.io
",Update the `async_get_sensor` function to remove the `entry` parameter.
896,"Update README.md for Latex injection
Hi üëã 

Adding a payload to read multi line file.
I also state that injections may need to be wrapped.

Cheers",Update README.md to include information on adjusting injection with wrappers and adding an example of using lstinputlisting.
897,"Improving performance for PDF loader
The PyMuPDF brings much better performance for PDF loading.

https://pymupdf.readthedocs.io/en/latest/about.html#performance
Here is a comparison article:
https://medium.com/social-impact-analytics/comparing-4-methods-for-pdf-text-extraction-in-python-fd34531034f

It's about **5x-30x faster** for text extraction.

And In my few test cases, PyMuPDF have better result in CJK characters, Although they both may display a lot of garbled characters.

The different is PDFMinerLoader make a single document object but PyMuPDFLoader make each page as a document object.

This PR already contains the change that I've made for this update:
https://github.com/imartinez/privateGPT/pull/560/files

* Remove read subscript [0] only for _loader.load()_
* Update _results.append(doc)_ to  _results.extend(docs)_",Update the PDFMinerLoader to PyMuPDFLoader in ingest.py and update the requirements.txt to use PyMuPDF instead of PDFMiner.six.
898,"MAINT Parameters validation for sklearn.model_selection.permutation_test_score
#### Reference Issues/PRs

Towards #24862.

#### What does this implement/fix? Explain your changes.

Automatic parameters validation for [sklearn.model_selection.permutation_test_score](https://github.com/scikit-learn/scikit-learn/blob/c3bfe86b4/sklearn/model_selection/_validation.py#L1207)",Add new parameter `n_permutations` to `permutation_test_score` function.
899,"fixbug#979 : `RuntimeError: fail to reduce message length` 
#979 
**Features**
- updated MAX-TOKEN according to openai document
- minior optimization of code style
- fixed issues `RuntimeError: fail to reduce message length`
   
**Influence**
before gpt-3.5-turbo will raise error, this pr fixed it
",Update prompt length calculation to use the model's maximum token count.
900,"Bump pygments from 2.7.3 to 2.7.4
Bumps [pygments](https://github.com/pygments/pygments) from 2.7.3 to 2.7.4.

Release notes
Sourced from pygments's releases.

2.7.4


Updated lexers:


Apache configurations: Improve handling of malformed tags (#1656)


CSS: Add support for variables (#1633, #1666)


Crystal (#1650, #1670)


Coq (#1648)


Fortran: Add missing keywords (#1635, #1665)


Ini (#1624)


JavaScript and variants (#1647 -- missing regex flags, #1651)


Markdown (#1623, #1617)


Shell

Lex trailing whitespace as part of the prompt (#1645)
Add missing in keyword (#1652)



SQL - Fix keywords (#1668)


Typescript: Fix incorrect punctuation handling (#1510, #1511)




Fix infinite loop in SML lexer (#1625)


Fix backtracking string regexes in JavaScript/TypeScript, Modula2
and many other lexers (#1637)


Limit recursion with nesting Ruby heredocs (#1638)


Fix a few inefficient regexes for guessing lexers


Fix the raw token lexer handling of Unicode (#1616)


Revert a private API change in the HTML formatter (#1655) --
please note that private APIs remain subject to change!


Fix several exponential/cubic-complexity regexes found by
Ben Caller/Doyensec (#1675)


Fix incorrect MATLAB example (#1582)


Thanks to Google's OSS-Fuzz project for finding many of these bugs.



Changelog
Sourced from pygments's changelog.

Version 2.7.4
(released January 12, 2021)


Updated lexers:


Apache configurations: Improve handling of malformed tags (#1656)


CSS: Add support for variables (#1633, #1666)


Crystal (#1650, #1670)


Coq (#1648)


Fortran: Add missing keywords (#1635, #1665)


Ini (#1624)


JavaScript and variants (#1647 -- missing regex flags, #1651)


Markdown (#1623, #1617)


Shell

Lex trailing whitespace as part of the prompt (#1645)
Add missing in keyword (#1652)



SQL - Fix keywords (#1668)


Typescript: Fix incorrect punctuation handling (#1510, #1511)




Fix infinite loop in SML lexer (#1625)


Fix backtracking string regexes in JavaScript/TypeScript, Modula2
and many other lexers (#1637)


Limit recursion with nesting Ruby heredocs (#1638)


Fix a few inefficient regexes for guessing lexers


Fix the raw token lexer handling of Unicode (#1616)


Revert a private API change in the HTML formatter (#1655) --
please note that private APIs remain subject to change!


Fix several exponential/cubic-complexity regexes found by
Ben Caller/Doyensec (#1675)


Fix incorrect MATLAB example (#1582)


Thanks to Google's OSS-Fuzz project for finding many of these bugs.



Commits

4d555d0 Bump version to 2.7.4.
fc3b05d Update CHANGES.
ad21935 Revert &quot;Added dracula theme style (#1636)&quot;
e411506 Prepare for 2.7.4 release.
275e34d doc: remove Perl 6 ref
2e7e8c4 Fix several exponential/cubic complexity regexes found by Ben Caller/Doyensec
eb39c43 xquery: fix pop from empty stack
2738778 fix coding style in test_analyzer_lexer
02e0f09 Added 'ERROR STOP' to fortran.py keywords. (#1665)
c83fe48 support added for css variables (#1633)
Additional commits viewable in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pygments&package-manager=pip&previous-version=2.7.3&new-version=2.7.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


",Update version of pygments from 2.7.3 to 2.7.4.
901,"Update CONTRIBUTING.md
Minor changes to align with proper project naming (gpt-engineer vs other variants of the name)","Update the CONTRIBUTING.md file to replace ""GPT-engineer"" with ""gpt-engineer"" throughout the document."
902,"fix get_anchors for VectorizedPoints
Fixes `VectorizedPoint.get_anchors()`, which previously returned an empty array when called on a `VectorizedPoint` (https://github.com/3b1b/manim/issues/455).

I tested by running the code found in that issue and verifying that the rectangles were placed properly.",Add a new condition to the `get_anchors` method to return the `self.points` directly when the shape of `self.points` is 1.
903,"boardd/can_recv_thread: improve caching for can_frame
",Add a new function to clear raw_can_data in can_recv_thread.
904,"Remove usage of buildkit
Fixes #8355 

During the troubleshooting of #8355, I came to the conclusion that using buildkit was creating the problem. Without it all docker images are built correctly. Initially buildkit was enabled to avoid a building problem in Azure Pipeline, but I also found in my recent tests that this problem was not there anymore.

You can find more details about the troubleshooting and reasoning in #8355.

As a consequence, I disable the usage of buildkit in this PR which will solve the issue.
",Update build script to use Docker BuildKit and fix Azure Pipelines failures.
905,"[3.10] gh-95166: cancel map waited on future on timeout (GH-95169)



* Issue: gh-95166

","Fix cancellation of currently waiting on future in `concurrent.futures.Executor.map` when an error occurs, such as TimeoutError or KeyboardInterrupt."
906,"Merge output format
As explained in https://github.com/rg3/youtube-dl/issues/3610, https://github.com/rg3/youtube-dl/issues/2765 there are cases when downloading separate video and audio and then merging when the video container (usually webm containing VP8/VP9) is incompatible with the audio codec (aac/mp3). By default, youtube-dl attempts to create a merge in the container format of the video file.

This is a redo of pull request https://github.com/rg3/youtube-dl/pull/4577. This time around I've added a video option called ""merge-ouput-format"" that specifies what container format the merge output should be put into. It is only used if merging is required (something like ""-f best"" will not use it for instance). If you provide nothing for it, the video part's container format will be used, just like before.
",Change the default output format for merged videos to be specified by the user.
907,"created a simple clock widget using Tkinter
Details instruction to run the widget is in the README.md ","Add a new README.md file to the UI-Apps directory with a description of the SIMPLE CLOCK WIDGET USING TKINTER and instructions for running the clock.py script. Also, add a new clock.py file to the UI-Apps directory, which is a simple clock widget using tkinter."
908,"Add Kalimdor.js - Machine Learning library for the web and Node!
Hi, I've recently developed a Machine Learning library in Typescript. It is still evolving every day and I've already solved a few Kaggle challenges using this library!

In short, the library is:
- Simple to use
- Solves real world problems
- Has an excellent documentation

Homepage: https://www.kalimdor.io/
Github: https://github.com/jasonshin/kalimdorjs

Cheers =)","Update README.md to include a new machine learning library, Kalimdor.js, and remove mil-tokyo."
909,"Enable compression for manifest.json


## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->
Now that compression is ~5x faster than base zlib, it makes sense to compress this since it works out to a ~70% reduction in size.


## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Deprecation (breaking change to happen in the future)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [x] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [x] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] I have followed the [perfect PR recommendations][perfect-pr]
- [ ] The code has been formatted using Ruff (`ruff format homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/development_checklist/
[manifest-docs]: https://developers.home-assistant.io/docs/creating_integration_manifest/
[quality-scale]: https://developers.home-assistant.io/docs/integration_quality_scale_index/
[docs-repository]: https://github.com/home-assistant/home-assistant.io
[perfect-pr]: https://developers.home-assistant.io/docs/review-process/#creating-the-perfect-pr
",Update the `get` method in `ManifestJSONView` to enable compression in the response.
910,"Subaru: fix forester weight
from https://github.com/commaai/openpilot/pull/31550

validation scripts:

```bash
STARTING_BRANCH=$(git rev-parse --abbrev-ref HEAD)

git checkout 556f9738960526d6eeaab9fe639d0340a6d5d6c9 # before any config has changed

python dump.py > original.txt

git checkout $STARTING_BRANCH

python dump.py > new.txt

cmp -s original.txt new.txt
```

```python
#!/usr/bin/env python3

from selfdrive.car import gen_empty_fingerprint
from selfdrive.car.car_helpers import interfaces
from selfdrive.car.fingerprints import all_known_cars

if __name__ == ""__main__"":
  for car in sorted(all_known_cars()):
    CarInterface, CarController, CarState = interfaces[car]
    car_fw = []
    fingerprints = gen_empty_fingerprint()
    CP = CarInterface.get_params(car, fingerprints, car_fw, experimental_long=False, docs=False)
    print(f""{CP.carFingerprint}: {CP.mass=} {CP.wheelbase=} {CP.steerRatio=} {CP.minEnableSpeed=} {CP.minSteerSpeed=} {CP.centerToFront=}"")
```",Update mass of Subaru Forester 2019-21 from 1668 to 1568.
911,"Require explicit confirmation of snap plugin permissions before connecting
Fixes #7667.

Implements the plan described in #7667.

Here's a terminal log showing that it does so:

```
# sudo snap connect certbot:plugin certbot-dns-dnsimple
error: cannot perform the following tasks:
- Run hook prepare-plug-plugin of snap ""certbot"" (run hook ""prepare-plug-plugin"": 
-----
Only connect this interface if you trust the plugin author to have root on the system
Run `snap set certbot trust-plugin-with-root=ok` to acknowledge this and then run this command again to perform the connection
-----)
# snap set certbot trust-plugin-with-root=ok
# sudo snap connect certbot:plugin certbot-dns-dnsimple
# sudo snap disconnect certbot:plugin certbot-dns-dnsimple:certbot
# sudo snap connect certbot:plugin certbot-dns-dnsimple
error: cannot perform the following tasks:
- Run hook prepare-plug-plugin of snap ""certbot"" (run hook ""prepare-plug-plugin"": 
-----
Only connect this interface if you trust the plugin author to have root on the system
Run `snap set certbot trust-plugin-with-root=ok` to acknowledge this and then run this command again to perform the connection
-----)
```",Require explicit confirmation of snap plugin permissions before connecting.
912,"Add A B√≠blia Digital


- [x] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not have more than 100 characters
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Added a new book to the ""Books"" section of the README.md file: ""A B√≠blia Digital"" with a description and API details."
913,"added Linktree
","Add new ""Linktree"" section to data.json with four new properties: errorType, url, urlMain, and username_claimed."
914,"Add Hardkernel ODROID-M1
<!--
  You are amazing! Thanks for contributing to our project!
  Please, DO NOT DELETE ANY TEXT from this template! (unless instructed).
-->
## Breaking change
<!--
  If your PR contains a breaking change for existing users, it is important
  to tell them what breaks, how to make it work again and why we did this.
  This piece of text is published with the release notes, so it helps if you
  write it towards our users, not us.
  Note: Remove this section if this PR is NOT a breaking change.
-->


## Proposed change
<!--
  Describe the big picture of your changes here to communicate to the
  maintainers why we should accept this pull request. If it fixes a bug
  or resolves a feature request, be sure to link to that issue in the
  additional information section.
-->
Add Hardkernel ODROID-M1 machine. ODROID-M1 is a Rockchip RK3568B2 SoC based single board computer with 4xCortex-A55, NVMe support and up to 8GB of RAM.


## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [ ] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [x] New feature (which adds functionality to an existing integration)
- [ ] Deprecation (breaking change to happen in the future)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [ ] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #
- This PR is related to issue: 
- Link to documentation pull request: 

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [ ] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.
- [ ] Untested files have been added to `.coveragerc`.

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html
[quality-scale]: https://developers.home-assistant.io/docs/en/next/integration_quality_scale_index.html
[docs-repository]: https://github.com/home-assistant/home-assistant.io
",Add support for ODROID M1 board.
915,"F.21 add loop increment in bad example
to mimic the previous examples and avoid an infinite loop.",Update the for loop to include a recursive call to get_string.
916,"Increased letsencrypt and letsencrypt-nginx cover minimums
Pretty self explanatory. Our coverage minimums can be increased.
",Change minimum value for 'letsencrypt' and 'letsencrypt_nginx' from 96 to 97.
917,"üë∑ Add GitHub Action gate/check
This adds a GHA job that reliably determines if all the required dependencies have succeeded or not.

It also allows to reduce the list of required branch protection CI statuses to just one ‚Äî `check`. This reduces the maintenance burden by a lot and have been battle-tested across a small bunch of projects in its action form and in-house implementations of other people.

It is now in use in aiohttp (and other aio-libs projects), CherryPy, attrs, coveragepy, some of the Ansible repositories, pip-tools, pydantic, spaceship-prompt, all of the jaraco's projects (like `setuptools`, `importlib_metadata`), some PyCQA, PyCA, PyPA and pytest projects, a few AWS Labs projects.

The story behind this is explained in more detail at https://github.com/marketplace/actions/alls-green#why.","Add a new job to the test workflow to check the branch protection using the ""all-green"" action."
918,"[MRG] Update joblib to 0.9.3
Bug fix release to fix joblib/joblib#263: interactively defined functions can not be used in parallel for Python 3 (because the default start method is forkserver).

This needs to be merged for 0.17, fix for #5623, overrides PR #5624 by @lesteve.
","Upgraded joblib to 0.9.3 to benefit from the new automatic batching of short tasks, improved speed of DictLearning with coordinate descent method, and fixed an issue with multiprocessing under OS X."
919,"Add Monitoring Tool | General-Purpose Machine Learning Section
","Added a new link to the README.md file, specifically [NannyML](https://bit.ly/nannyml-github-machinelearning), a Python library for estimating post-deployment model performance without access to targets."
920,"time checks in locationd


<!--- ***** Template: Car bug fix *****

**Description** [](A description of the bug and the fix. Also link any relevant issues.)

**Verification** [](Explain how you tested this bug fix.)

**Route**
Route: [a route with the bug fix]

-->

<!--- ***** Template: Bug fix *****

**Description** [](A description of the bug and the fix. Also link any relevant issues.)

**Verification** [](Explain how you tested this bug fix.)

-->

<!--- ***** Template: Car port *****

**Checklist**
- [ ] added to README
- [ ] test route added to [test_routes.py](../../selfdrive/test/test_routes.py)
- [ ] route with openpilot:
- [ ] route with stock system:

-->

<!--- ***** Template: Refactor *****

**Description** [](A description of the refactor, including the goals it accomplishes.)

**Verification** [](Explain how you tested the refactor for regressions.)

-->
",Add handling for messages from two IMUs at the same time and add a time check to detect and handle large time gaps between sensor readings and log time.
921,"updated eop imports
",Remove unnecessary imports and add a new import.
922,"Better RWKV Prompt




## Why are these changes needed?



## Related issue number (if applicable)



## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [x ] I've made sure the relevant tests are passing (if applicable).
",Update conversation system message to remove detailed description and keep it concise.
923,"Bump fetch-mock from 7.7.3 to 9.11.0 in /frontend
Bumps [fetch-mock](https://github.com/wheresrhys/fetch-mock) from 7.7.3 to 9.11.0.

Release notes
Sourced from fetch-mock's releases.

Babel upgrades
In theory, this should not cause anybody any problems
Fixes various types
No release notes provided.
Allow overwriting regex routes
No release notes provided.
Add lastResponse to type defs
No release notes provided.
Refactor routes to be wrapped in a class
No release notes provided.
Fix mocking requests with bodies in safari
Merge pull request #585 from lambdalisue/hotfix-584
Patch native fetch on Safari to fix #584
Make typescript types available when importing the esm build
No release notes provided.
Prevent responses from hanging in node-fetch
No release notes provided.
Add matchPartialBody to typings
No release notes provided.
Allow response bodies to be inspected
No release notes provided.
Correct release with response inspection
No release notes provided.
Allow inspecting responses
No release notes provided.
v9.9.1
No release notes provided.
Full support for coercing objects into query string matchers
No release notes provided.
Added support for multi-value query string matching
No release notes provided.
Added support for sticky routes
No release notes provided.


... (truncated)


Commits

4b54e6d browsers again
838f667 aaargh - lint!
18041fb fix a test that should've always failed reallyI guess node 15 is stricter abo...
f6f4d08 revert to old browsers docker image
bf6d08c correct browser docker image name
6ca3e64 babel config file name chamnge
7d4bb27 hopefully fix all failnig jobs
ac06de1 install browsers
a3665e0 use lts and current aliases for images
5a43afe fix some docker image names
Additional commits viewable in compare view





[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=fetch-mock&package-manager=npm_and_yarn&previous-version=7.7.3&new-version=9.11.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---


Dependabot commands and options


You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


",### Update of dependencies in package.json and yarn.lock filesThe pull request updates the dependencies in the `package.json` and `yarn.lock` files. The changes include:* Updating `fetch-mock` from version `^7.3.9` to `^9.11.0`* Updating `core-js` from version `^2.6.9` to `^3.0.0`* Updating `babel-polyfill` from version `^6.26.0` to an unknown version (removed)* Updating `regenerator-runtime` from version `^0.10.5` to `^0.11.1`These changes likely affect the project's dependencies and may require further updates to the codebase.
924,"Set up 2.0 pre-releases
This implements the majority of https://docs.google.com/document/d/1t02y0qQvTgc4eNPY4OwfYu7oW41DEBNhr8fq_O97kXo/edit?usp=sharing. The work that remains after this is merged into master is merging this into the 2.0 branch, setting up the 2.0-prerelease pipeline for the 2.0 branch, writing documentation on how to use that pipeline, doing a prerelease, and writing a post on the community forum.

One change I made in this PR that's not easily seen/understandable is I removed the reference to https://github.com/certbot/certbot/issues/7931 for the reason I wrote at https://github.com/certbot/certbot/issues/7931#issuecomment-1238650122.

I also set up a dummy 2.0 prerelease pipeline to test this at https://dev.azure.com/certbot/certbot/_build?definitionId=6&_a=summary which seems to be working properly.",Here is a concise summary of the changes:**Azure Pipelines Configuration Changes*** Updated the `2.0-prerelease.yml` pipeline to trigger on none and publish to the candidate channel.* Updated the `release.yml` pipeline to publish to the candidate channel instead of beta.* Updated the `snap-deploy-job.yml` template to use the candidate channel for publishing snaps.* Updated the `deploy-stage.yml` template to use the candidate channel for publishing snaps.* Updated the `finish_release.py` script to move snaps from the candidate channel to the stable channel.
925,"Fixed #26652 -- Documented removal of model instance _(default/base)_manager attributes.
",Update documentation for Django 1.10: added information about removed and deprecated features.
926,"Basic support for the temperature parameter
",Add 'temperature' parameter to the chat_completions function and use it in the ChatCompletion.create method.
927,"Fixed #29939 -- Increased Group.name max_length to 150 characters.
This PR increases the max_length of `django.contrib.auth` model `Group` field `name` to 191, to match recent changes to `username`, `first_name`, and `last_name` in the `User` model. ([Ticket](https://code.djangoproject.com/ticket/29939))",Increase the maximum length of the `name` field in the `Group` model from 80 to 150 characters.
928,"Run lint/typechecking in latest python + update pre-commit
- Lets move to latest and greatest of lints
- Update mypy + flake8",Update Python version in workflow to 3.7 and update Flake8 and MyPy versions to 3.9.0 and 0.812 respectively.
929,"üìù Add note to docs about Separate Input and Output Schemas with FastAPI version
üìù Add note to docs about Separate Input and Output Schemas with FastAPI version",Update documentation to include information about `separate_input_output_schemas` feature and its support in FastAPI `0.102.0`.
930,"Removed fit call from regularizers tests.
As suggested in #8990 I removed fit calls. 

Here in the regularization tests (the assert is all that is really needed).

The speed gain is going to be very small, but if we can reduce the number of lines in the codebase, It's not a bad thing.",Remove unnecessary imports and update test_data function to return only x_train and y_train.
931,"[TVer] remove FujiTV redirect
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)
- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them
- [x] Covered the code with tests (note that PRs without tests will be REJECTED)
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

These changes are taken from https://github.com/nao20010128nao/ytdl-patched/commit/4e1880fdbac7ff243a293cc7825605437163d03d , which is my own work.

### What is the purpose of your *pull request*?
- [x] Bug fix
- [ ] Improvement
- [ ] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information
It seems FujiTV have migrated to Brightcove as other stations do.    
This PR removes FujiTVFODPlus7IE and redirects to Brightcove.

This fixes #28767.
",Update the Brightcove extractor to handle the new reference_id format.
932,"Create Program to print table of given number
",Add a new program to print the multiplication table of a given number.
933,"Add FAQ entry referencing Request.meta usage
",Update to the FAQ section with the addition of a new question and answer regarding scraping items with attributes in different pages.
934,"Switching the deploy workflow to use environment secrets
Fixes #755 

This migrates the deployment workflows away from repository secrets to environment secrets.",Update NextAuth secret from DEV_WEB_NEXTAUTH_SECRET to NEXTAUTH_SECRET.
935,"Clean chat data
",Here is a concise summary of the changes:**Added/Updated Files:*** `clean_chat_data.py`: A new file that cleans chatbot arena battle log data.* `sample.py`: A sample script that counts unique users in a battle log file.**Changes to Existing Files:*** `docs/commands/local_cluster.md`: Updated model paths and names for various models.* `fastchat/serve/controller.py`: Added a new API endpoint `/test_connection`.* `fastchat/serve/model_worker.py`: Updated the `send_heart_beat` function to handle exceptions.* `fastchat/serve/monitor/basic_stats.py`: Updated the import statement for pandas to `pandas>=2.0.3`.* `fastchat/serve/monitor/clean_chat_data.py`: A new file that cleans chatbot arena battle log data.* `fastchat/serve/monitor/conv_release_scripts/sample.py`: A sample script that counts unique users in a battle log file.
936,"Add link to official Discord chat server
All Pallets projects should have a link to the official chat server on Discord.",Update to the README.rst file: added a new link to the official chat.
937,"implement SQS lambda destination

addresses https://github.com/localstack/localstack/issues/3373","### DIFF:Change the destination of the Lambda function's successful and failed invocations to use the `on_successful_invocation` and `on_failed_invocation` destinations, respectively."
938,"Device agnostic testing
This PR changes the test suite to allow running the tests on any hardware.

In order to do this I tried to make the test suite device agnostic and allow setting the device and backend through environment variables. This is hopefully a future proof way for allowing the test suite to be run on any hardware.

I've also added the functionality to change timeouts on tests that run on hardware.

In all the changes I've tried to keep previous functionality working if using the CI in the ""normal"" way. If the environment variables are not used, previous values are used as defaults.

Example usage:
`TORCH_DEVICE=fancy_new_hw TORCH_BACKEND=new_torch_backend pytest tests/`

",### DIFF:Changes were made to the tests for the timm library. The changes include:* Adding environment variables for TORCH_BACKEND and TORCH_DEVICE to control the device used for testing.* Importing the torch_device and torch_backend variables in the tests.* Adding checks for the device type in the tests.* Updating the tests to use the torch_device and torch_backend variables.* Adding a timeout to some of the tests.* Updating the tests to use the torch_device and torch_backend variables in the optimizer tests.* Adding checks for the device type in the optimizer tests.* Updating the tests to use the torch_device and torch_backend variables in the model tests.* Adding checks for the device type in the model tests.The changes were made to ensure that the tests are more robust and can be run on different devices.
939,"fix high CPU utilization when on network
",Add a 3-second delay before reporting a network failure in the `report_connect_fail` method.
940,"#509: Fixed correction on windows machine running other shells

Fix #509 ",Update assertion in test_run method to compare the entire output string. Update print statement in run method to use sys.stdout.write instead of print.
941,"Updated README in Japanese.
Hello :)

I found this great repository on github trends and wanted to create a PR to see if I could contribute in any way.
I have corrected some translations that looked strange from my Japanese point of view.

I hope it will be useful for many people üôè

Thanks.",Update README-ja-JP.md: Fix typos and formatting issues in the usage instructions.
942,"feat(perf tooltips): adding to index and latency cleanup
**Before:**
![Screen Shot 2020-04-03 at 9 34 28 AM](https://user-images.githubusercontent.com/4830259/78384055-54fe9e00-758e-11ea-9ff4-79deea2c370a.png)
![Screen Shot 2020-04-03 at 9 34 20 AM](https://user-images.githubusercontent.com/4830259/78384065-56c86180-758e-11ea-9ff7-43e3476dc592.png)

**After:**
![Screen Shot 2020-04-03 at 9 32 42 AM](https://user-images.githubusercontent.com/4830259/78383946-21bc0f00-758e-11ea-9921-f79891fabe89.png)
![Screen Shot 2020-04-03 at 9 38 08 AM](https://user-images.githubusercontent.com/4830259/78384337-d9e9b780-758e-11ea-88f7-9a0267ae384a.png)
![Screen Shot 2020-04-03 at 9 32 53 AM](https://user-images.githubusercontent.com/4830259/78383937-1ec11e80-758e-11ea-8df6-3b9a17f20300.png)","### Description:Changes were made to the Sentry performance charts to add tooltips to the y-axis labels. The tooltips provide additional information about the Apdex and Throughput metrics. Additionally, the styles for the charts were refactored and moved to a separate file."
943,"Error resolved in wave_sort.py
",### Description of Pull Request:Changes to the wave sort algorithm in wave_sort.py. The print statement now includes a space after the array element. Changes to the email id dictionary in dict1.py. The code now correctly counts the frequency of each email id and prints the most frequent one. The code also handles the case where the email id is not 'From '. Added a new file mbox-short.txt which contains a sample email message.
944,"[3.8] bpo-41162: Clear audit hooks later during finalization (GH-21222)
Co-authored-by: Konge 

<!--
Thanks for your contribution!
Please read this comment in its entirety. It's quite important.

# Pull Request title

It should be in the following format:

```
bpo-NNNN: Summary of the changes made
```

Where: bpo-NNNN refers to the issue number in the https://bugs.python.org.

Most PRs will require an issue number. Trivial changes, like fixing a typo, do not need an issue.

# Backport Pull Request title

If this is a backport PR (PR made against branches other than `master`),
please ensure that the PR title is in the following format:

```
[X.Y]  (GH-NNNN)
```

Where: [X.Y] is the branch name, e.g. [3.6].

GH-NNNN refers to the PR number from `master`.

-->



https://bugs.python.org/issue41162

",Audit hooks are now cleared later during finalization to avoid missing events.
945,"Do not include /etc/nginx/mime.types in nginx integration testing.
This file (or /etc/nginx in whole) might not exist on the target system.
",Comment out include /etc/nginx/mime.types; directive.
946,"Fix grammatical error
",Update to README.md: corrected typo in sentence about `Yo()._Yo__honey`
947,"Explicitly require setuptools, httpie/plugins/manager.py imports pkg_resources
",Update setup.py to include'setuptools' in install_requires.
948,"Fix some small code quality issues.

## What do these changes do?
Some code quality issues were fixed in this PR:
1. some methods should be declared as `const`.
2. some ctors should be specified `explicit`.
3. fix the difference between declaration and definition of `RegisterDriver`.",### Change the const correctness of the GetClientId and RegisterWorker methods in ClientConnection and WorkerPool classes.
949,"Ruby: add slim
","Update documentation for Server Side Template Injection, including new sections on Basic injections and adding references to Jinja2 template injection filter bypasses, Gaining Shell using Server Side Template Injection, EXPLOITING SERVER SIDE TEMPLATE INJECTION WITH TPLMAP, and Server Side Template Injection ‚Äì on the example of Pebble."
950,"Hyundai : AZERA_HEV_6TH_GEN (Update fingerprints.py)
Hyundai AZERA_HEV_6TH_GEN

<!--- ***** Template: Fingerprint *****

**Car**
Hyundai Azera HEV, 6th Generation, 2019

**Route**
3a0cde9552891b34/2024-03-03--10-33-30

-->",Update of Hyundai fingerprints data.
951,"[model_cards]: üáπüá∑ Add new ELECTRA small and base models for Turkish
Hi,

this PR introduces model cards for new ELECTRA small and base models for Turkish üáπüá∑.

More information (checkpoint evaluation, downstream evaluation on Pos Tagging and NER, loss curves, TensorBoards) see [this repository](https://github.com/stefan-it/turkish-bert/tree/electra/electra).","Open-sourced Turkish ELECTRA models for NLP tasks. The models are trained on a filtered and sentence-segmented version of the Turkish OSCAR corpus, a recent Wikipedia dump, various OPUS corpora, and a special corpus provided by Kemal Oflazer. The models are available on the Huggingface model hub and can be used for tasks such as PoS tagging and NER."
952,"fix:update file desc
",Rename file from test_translate.py to test_moderation.py.
953,"Fix: requirements.txt was not written to the disk
While the Python packages requirements are correctly detected and saved into the json task file, requirements.txt was always empty. Since it was trying to get packages with the wrong key, packages were always empty when writing in requirements.txt


**Features**


    
**Feature Docs**


**Influence**


**Result**


**Other**
","Update the ""Required Python packages"" field in the `_update_requirements` function to use ""Required Python packages"" instead of ""Required Python third-party packages""."
954,"Added Keras GPT Copilot to the list
Keras GPT Copilot is a Python package that integrates a Large Language Model (LLM) copilot into the Keras model development workflow. It provides iterative feedback for model enhancement, supports compatibility with OpenAI's GPT-4 and other LLMs.",Update README.md: added Keras GPT Copilot to the list of references.
955,"updated collective ops api
",Improved CPU group selection for collective operations.
956,"fix autocast
",Update version number from 2.1.847 to 2.1.848 in fooocus_version.py.
957,"Pin all build dependencies for the Windows installer
This PR uses pipstrap to bootstrap the venv used to build Windows installers. This effectively pin all build dependencies, since `pynsist` is already installed through `pip_install.py` script.",Upgrade NSIS version to '3.04' and modify pipstrap.py in pip_install.py.
958,"Allow Peft models to share their base model




## Why are these changes needed?

This adds a special environment variable that activates shared Peft model base weights.  Currently when loading two Peft models that have the same base model, those model weights are loaded once.  With this flag activated, all Peft models will share the same base model.

To make this work it requires a few work around due to how Huggingface's Peft model has implemented LoRA adapters, the most popular variant.  These modify the base model's pytorch modules directly and thus adapters sharing the same base model must live within the same model object and a `set_adapter` method must be called to switch between them.  

## Related issue number (if applicable)

Expands #1805

## Checks

- [x] I've run `format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed.
- [x] I've made sure the relevant tests are passing (if applicable).
",### DIFF:Update Peft model adapter to share base model weights.Changes include:* Added environment variable `PEFT_SHARE_BASE_WEIGHTS` to control sharing of base model weights.* Implemented caching of first Peft model loaded for a given base model.* Modified `load_model` method to load adapters by model_path and share base model weights when `PEFT_SHARE_BASE_WEIGHTS` is set to `true`.* Added checks to ensure correct loading of adapters and base model weights during inference.
959,"Backport PR #35882 on branch 1.1.x (BUG: item_cache invalidation in get_numeric_data)
Backport PR #35882: BUG: item_cache invalidation in get_numeric_data",Bug fixes and documentation updates.
960,"Fix grammar in README, backend docs and initializers docs
### Summary
Fixed several grammar problems in the README, backend docs and initializers docs.

### Related Issues
None

### PR Overview
Fixed grammar errors in README.md, backend.md and initializers.py

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",Update documentation to reflect changes in the description of Keras' backend engine.
961,"Fix amcrest detection of sensor reset
## Proposed change
Previous versions of pyamcrest passed a boolean to home assistant
depending on whether the event was the activation of a sensor or not.
This was used by the amcrest integration to decide the state of the
binary sensors.

The latest pyamcrest passes through a non-empty payload dictionary
instead, which is a true-value and the binary sensor would always
be deemed active.

Look into the payload for an ""action"": ""start"" item instead.

## Type of change
<!--
  What type of change does your PR introduce to Home Assistant?
  NOTE: Please, check only 1! box!
  If your PR requires multiple boxes to be checked, you'll most likely need to
  split it into multiple PRs. This makes things easier and faster to code review.
-->

- [ ] Dependency upgrade
- [x] Bugfix (non-breaking change which fixes an issue)
- [ ] New integration (thank you!)
- [ ] New feature (which adds functionality to an existing integration)
- [ ] Breaking change (fix/feature causing existing functionality to break)
- [ ] Code quality improvements to existing code or addition of tests

## Additional information
<!--
  Details are important, and help maintainers processing your PR.
  Please be sure to fill out additional details, if applicable.
-->

- This PR fixes or closes issue: fixes #50214

## Checklist
<!--
  Put an `x` in the boxes that apply. You can also fill these out after
  creating the PR. If you're unsure about any of them, don't hesitate to ask.
  We're here to help! This is simply a reminder of what we are going to look
  for before merging your code.
-->

- [ ] The code change is tested and works locally.
- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**
- [ ] There is no commented out code in this PR.
- [ ] I have followed the [development checklist][dev-checklist]
- [ ] The code has been formatted using Black (`black --fast homeassistant tests`)
- [ ] Tests have been added to verify that the new code works.

If user exposed functionality or configuration variables are added/changed:

- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]

If the code communicates with devices, web services, or third-party tools:

- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  
      Updated and included derived files by running: `python3 -m script.hassfest`.
- [ ] New or updated dependencies have been added to `requirements_all.txt`.  
      Updated by running `python3 -m script.gen_requirements_all`.
- [ ] Untested files have been added to `.coveragerc`.

The integration reached or maintains the following [Integration Quality Scale][quality-scale]:
<!--
  The Integration Quality Scale scores an integration on the code quality
  and user experience. Each level of the quality scale consists of a list
  of requirements. We highly recommend getting your integration scored!
-->

- [ ] No score or internal
- [ ] ü•à Silver
- [ ] ü•á Gold
- [ ] üèÜ Platinum

<!--
  This project is very active and we have a high turnover of pull requests.

  Unfortunately, the number of incoming pull requests is higher than what our
  reviewers can review and merge so there is a long backlog of pull requests
  waiting for review. You can help here!
  
  By reviewing another pull request, you will help raise the code quality of
  that pull request and the final review will be faster. This way the general
  pace of pull request reviews will go up and your wait time will go down.
  
  When picking a pull request to review, try to choose one that hasn't yet
  been reviewed.

  Thanks for helping out!
-->

To help with the load of incoming pull requests:

- [ ] I have reviewed two other [open pull requests][prs] in this repository.

[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone

<!--
  Thank you for contributing <3

  Below, some useful links you could explore:
-->
[dev-checklist]: https://developers.home-assistant.io/docs/en/development_checklist.html
[manifest-docs]: https://developers.home-assistant.io/docs/en/creating_integration_manifest.html
[quality-scale]: https://developers.home-assistant.io/docs/en/next/integration_quality_scale_index.html
[docs-repository]: https://github.com/home-assistant/home-assistant.io
",Update the Amcrest camera event handling to include the payload action in the event data.
962,"Subclass getstate from pydantic's BaseModel to fix cannot pickle and unpickle TextNode
# Description

Cannot pickle and then unpickle TextNode object. This is due to `BaseComponent`'s `__getstate__` completely overwrites `pydantic.BaseModels`'s `__getstate__`. Example:

```python
import pickle
from llama_index.schema import TextNode

abc = TextNode(text=""abc"")

# save ok
with open(""abc.pkl"", ""wb"") as fo:
    pickle.dump(abc, fo)

# error when load
with open(""abc.pkl"", ""rb"") as fi:
    data = pickle.load(fi)
```

Stack trace:
```
File ~/.cache/pypoetry/virtualenvs/llama-index-Jalitqr9-py3.9/lib/python3.9/site-packages/pydantic/main.py:416, in pydantic.main.BaseModel.__setstate__()

KeyError: '__dict__'
```

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense
- [x] Test with the snippet above

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",Update CHANGELOG.md to include new unreleased changes and bug fix. Update llama_index/schema.py to remove unpicklable entries from object serialization.
963,"Fix a docstring type in cli.py.
","Update documentation comment in flask/cli.py: ""Special application that dispatches to a flask application which is imported by name on first request."""
964,"add new gradio themes
## Description

* adds a few new hand selected gradio themes, all tested at least once so make sure they work properly

## Checklist:

- [X] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [X] I have performed a self-review of my own code
- [X] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [X] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Added 14 new themes to the `gradio_hf_hub_themes` list in `modules/shared.py`.
965,"Correct previous change to Exchange.aggregate
Expansion needs to be done on bidasks, rather than items",Change the bidask processing to include additional data in the processing.
966,"Allow open ranges for chapters
**IMPORTANT**: PRs without the template will be CLOSED

### Description of your *pull request* and other information



This will allow `--download-sections` and its relatives to allow using range with the other end missing (e.g. `*2-`, `*-60`)


Template 

<!--

# PLEASE FOLLOW THE GUIDE BELOW

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes `[ ]` relevant to your *pull request* (like [x])
- Use *Preview* tab to see how your *pull request* will actually look like

-->

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [contributing guidelines](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions) including [yt-dlp coding conventions](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#yt-dlp-coding-conventions)
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8) and [ran relevant tests](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [ ] Fix or improvement to an extractor (Make sure to add/update tests)
- [ ] New extractor ([Piracy websites will not be accepted](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#is-the-website-primarily-used-for-piracy))
- [x] Core bug fix/improvement
- [ ] New feature (It is strongly [recommended to open an issue first](https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#adding-new-feature-or-making-overarching-changes))
",Change the parsing of time ranges in `validate_outtmpl` and `parse_chapters` to allow for infinite duration.
967,"[wip/s2s] DistributedSortishSampler
This allows sortish sampler logic to work on multiple GPU.

The strategy is 
1) find the indices that the current rank's data loader should be using (like `DistributedSampler`)
2) reorder those using the `SortishSampler` logic.

### Results 
The results on a small MT task are similar to the 1 GPU setting. 
+ 2 GPU, random sampler: 13 mins/epoch  BLEU 8.6
+ 2 GPU, `DistributedSortishSampler`: 10 mins/epoch, BLEU 8.6

In the chart below, you can see that the sortish sampler gets to a higher BLEU score in the same number of minutes (x axis) (because it has finished a full epoch rather than 70% of one).

![image](https://user-images.githubusercontent.com/6045025/92787937-87030000-f377-11ea-822a-69e50debe95d.png)

@sgugger let me know if you want this in Trainer!",Change the `sortish_sampler` and `gpus` parameters in the `test_multigpu` method to `True` and `2` respectively.
968,"fix:const.py add CONFIG_ROOT
**Features**



- xx
- yy
    
**Feature Docs**


**Influence**


**Result**


**Other**
",Update the METAGPT_PROJECT_ROOT constant to use the CONFIG_ROOT variable.
969,"[MRG+1] Use ""url"" variable in the example
Instead of hardcoded http://www.example.com, as noticed by Andr√©s P√©rez-Albela H.: without it url variable is unused and only one request will make it past dupefilter.
",Update documentation for downloader middleware to use `cookiejar` meta key in `Request` object.
970,"[mypy] Fix type annotations for maths directory
### Describe your change:



* [ ] Add an algorithm?
* [x] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### Checklist:
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
* [x] If this pull request resolves one or more open issues then the commit message contains `Fixes: #{$ISSUE_NO}`.
",### DIFF:Changes in the maths directory:* `average_mode.py`: Improved type hints and docstrings.* `gamma_recursive.py`: Improved type hints and docstrings.* `proth_number.py`: Improved type hints and docstrings.* `geometric_series.py`: Improved type hints and docstrings.* `p_series.py`: Improved type hints and docstrings.Changes in the `mypy.ini` file:* Excluded some files from type checking.Description:The changes in the maths directory are minor and focus on improving type hints and docstrings. The `mypy.ini` file has been updated to exclude some files from type checking.
971,"dir buttons start with / so only the correct dir will be shown and no‚Ä¶
## Description

* fixed  #13895 
when show dirs is selected, a / is not only inserted after the folder name, but also before it. so that only the selected folder is actually displayed, and not also other folders whose name is a substring of the selected folder.

## Screenshots/videos:
![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/65122811/b848cfcb-97be-43bc-81af-c76b744b7834)


## Checklist:

- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [x] I have performed a self-review of my own code
- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
","Add functionality to show directory buttons with a '/' at the beginning when the ""Extra Networks"" option is enabled."
972,"Removed a Python 2 fallback in certbot.Reverter.
## Pull Request Checklist

- [x] If the change being made is to a [distributed component](https://certbot.eff.org/docs/contributing.html#code-components-and-layout), edit the `master` section of `certbot/CHANGELOG.md` to include a description of the change being made.
- [x] Add or update any documentation as needed to support the changes in this PR.
- [x] Include your name in `AUTHORS.md` if you like.
",Remove sys version checks and fix csv module usage.
973,"[Peertube] Add playlist extractor and update instances list
## Please follow the guide below

- You will be asked some questions, please read them **carefully** and answer honestly
- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])
- Use *Preview* tab to see how your *pull request* will actually look like

---

### Before submitting a *pull request* make sure you have:
- [x] At least skimmed through [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) sections
- [x] [Searched](https://github.com/yt-dlp/yt-dlp/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests
- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)

### In order to be accepted and merged into yt-dlp each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:
- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)
- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)

### What is the purpose of your *pull request*?
- [x] Bug fix
- [ ] Improvement
- [x] New extractor
- [ ] New feature

---

### Description of your *pull request* and other information

Explanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.
",Added PeerTubePlaylistIE extractor and updated PeerTubeIE extractor to support PeerTube playlist.
974,"Fix GitHub Actions build warnings, Marker style should be `*`
Addressed:
> 16132:1-16132:75   warning  Marker style should be `*`  unordered-list-marker-style
> 16133:1-16133:39   warning  Marker style should be `*`  unordered-list-marker-style
> 16134:1-16134:57   warning  Marker style should be `*`  unordered-list-marker-style
> 16135:1-16135:62   warning  Marker style should be `*`  unordered-list-marker-style
> 16136:1-16136:240  warning  Marker style should be `*`  unordered-list-marker-style
> 16137:1-16137:194  warning  Marker style should be `*`  unordered-list-marker-style
> 16141:1-16141:72   warning  Marker style should be `*`  unordered-list-marker-style
> 16142:1-16142:49   warning  Marker style should be `*`  unordered-list-marker-style",Changes to CppCoreGuidelines.md: Added `*` to bullet points.
975,"Update rod_cutting.py
A hopefully clearer implementation without dependence on global variables.",Change the dynamic programming solution to a recursive solution for cutting a rod to maximize profit.
976,"BLD Removes post from release on website
Removes post from release name on the website. This is so the name on the top of the webpage does not have ""post"" in it:

With https://github.com/scikit-learn/scikit-learn/pull/16609

![Screen Shot 2020-03-03 at 12 14 07 PM](https://user-images.githubusercontent.com/5402633/75803941-3cac2100-5d4d-11ea-819c-ea2d173dbebf.png)

Cherry picking this PR on top:

![Screen Shot 2020-03-03 at 12 46 25 PM](https://user-images.githubusercontent.com/5402633/75803952-433a9880-5d4d-11ea-8360-bd57f14b4512.png)



",Update parsing of sklearn version to remove post-release information.
977,"fix: fix syntax errors
## Description

python syntax error.

## Screenshots/videos:

![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/40930677/243aa6ca-f863-478f-a169-a433e774d711)

## Checklist:

- [ ] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)
- [ ] I have performed a self-review of my own code
- [ ] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)
- [ ] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)
",Change type of `scalers` from empty list to a list type.
978,"Fix unexpected extensions load after gradio restart
## Checklist:

- [x] I have read the [Contributing guidelines](https://github.com/oobabooga/text-generation-webui/wiki/Contributing-guidelines).
",Add new variable'state' to the 'load_extensions' function.
979,"Support for yum invalid commands.
This rule replaces incorrect yum commands, such as `yum isntall` into `yum install` and `yum remove` into `yum uninstall`.","### DIFF:Changes to the README.md and addition of a new file `thefuck/rules/yum_invalid_operation.py` and `thefuck/specific/yum.py`. The new file `thefuck/rules/yum_invalid_operation.py` contains a rule for handling invalid yum commands, and the `thefuck/specific/yum.py` contains a check for yum availability.### Description:This pull request adds a new rule to the thefuck package to handle invalid yum commands. The rule checks if the command is not in the list of valid yum commands and suggests a corrected command. The rule is enabled by default on platforms where yum is available."
980,"Fix and document Zero Shot Image Classification
# What does this PR do?

This PR fixes 3 issues with `zero-shot-image-classification`

* It does not pick a default model, although there is one. This leads to weird error message

```python
pipe = pipeline(""zero-shot-image-classification"")
>> ValueError: The task defaults can't be correctly selected. You probably meant ""translation_XX_to_YY""
```

* The pipeline is not in the list at top of https://huggingface.co/docs/transformers/main_classes/pipelines
* The filter for available models has a typo, although right now this still leads to no models (ok just one).

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@Narsil @sgugger 
",Add support for ZeroShotImageClassificationPipeline in the Transformers library.
981,"DOC fix of sample_weight in _multinomial_loss_grad and _multino‚Ä¶
Fixes #13020
","Remove optional type hint for `sample_weight` in `_multinomial_loss`, `_multinomial_loss_grad`, and `_multinomial_grad_hess` functions."
982,"Updated fibonacci.py with dp
",Add dynamic programming support to the Fibonacci number calculation.
983,"Create factorial_iterative.py
### **Describe your change:**



* [x] Add an algorithm?
* [ ] Fix a bug or typo in an existing algorithm?
* [ ] Documentation change?

### **Checklist:**
* [x] I have read [CONTRIBUTING.md](https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md).
* [x] This pull request is all my own work -- I have not plagiarized.
* [x] I know that pull requests will not be merged if they fail the automated tests.
* [x] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.
* [x] All new Python files are placed inside an existing directory.
* [x] All filenames are in all lowercase characters with no spaces or dashes.
* [x] All functions and variable names follow Python naming conventions.
* [x] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).
* [x] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.
* [x] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.
",Change implementation of factorial function from recursive to iterative in maths/factorial_iterative.py and add docstrings and error handling in both maths/factorial_iterative.py and maths/factorial_recursive.py.
984,"ignore generated alert header file
",Add selfdrive/ui/translations/alerts_generated.h to.gitignore.
985,"Fix Certbot Apache plugin on Oracle Linux Server, a clone of CentOS, RHEL
[Oracle Linux Server](https://en.wikipedia.org/wiki/Oracle_Linux) is a [clone of CentOS/RHEL](https://www.oracle.com/linux/index.html). The Certbot Apache plugin cannot auto-detect the Oracle Linux Apache setup, causing a generic (but incorrect) Apache Linux configuration to be used, which emits this error:

> The apache plugin is not working; there may be problems with your existing configuration.
> The error was: NoInstallationError('Cannot find Apache control command apache2ctl',). Skipping.

On Oracle Linux Server, [the Apache control command](https://docs.oracle.com/cd/E52668_01/E54669/html/ol7-s2-websvc.html) is `apachectl` just like CentOS/RHEL.

The `/etc/os-release` file [on Oracle Linux](https://www.veritas.com/support/en_US/article.000087964) contains `ID=""ol""` which does not match any existing override class. My simple one-line PR adds this missing OS override. After this one-line change is applied, then Certbot Apache plugin works as designed on Oracle Linux.

Without this code change, a hack/workaround for Oracle Linux is [to add (_missing_)](https://www.freedesktop.org/software/systemd/man/os-release.html#ID_LIKE=) `ID_LIKE=""centos rhel""` to the [`/etc/os-release` file](http://0pointer.de/blog/projects/os-release.html). This forces the Certbot Apache plugin to treat OL like CentOS/RHEL.
",Add support for Oracle Linux (OL) in the entrypoint configuration.
986,"Hijack VQModelInterface back to AutoEncoder (to get LDSR working again)
The LDSR upscaler relies on the `VQModelInterface` as defined in the `ldm\models\autoencoder.py` of the Compvis Stable Diffusion repo.

The Autoencoder shipped in the Stability AI SD v2.0 git repo has the VQModelInterface removed, which obviously broke LDSR.

This hijack is to add `VQModelInterface` back so that LDSR (and maybe other things that relies on it) working again.

Related to issue #5087","### DIFF:A new file `sd_hijack_autoencoder.py` was added to the `modules` directory. This file contains the implementation of the `VQModel` and `VQModelInterface` classes, which are used in the stable diffusion model. The `VQModel` class is responsible for encoding and decoding images using a vector quantization (VQ) model, while the `VQModelInterface` class provides an interface to the `VQModel` class."
987,"BUG: Non-unique indexing via loc and friends fixed when slicing (GH3659_)
closes #3659

This is if you try a non_monotonic selection on a non_unique index (a mouthful)!
The reason is we cannot determinate a proper start/end point on what to include

```
In [11]: df = DataFrame({'A' : [1,2,3,4,5,6], 'B' : [3,4,5,6,7,8]}, 
           index = [0,1,0,1,2,3])

In [18]: df
Out[18]: 
   A  B
0  1  3
1  2  4
0  3  5
1  4  6
2  5  7
3  6  8

In [12]: df.loc[1:]
KeyError: 'cannot perform a slice operation on a non-unique non-monotonic index'
```

On a non_unique, but monotonic index, however, slicing works normally
(notice, since we are using loc, that both endpoints ARE included)

```
In [13]: df = DataFrame({'A' : [1,2,3,4,5,6], 'B' : [3,4,5,6,7,8]}, 
       index = [0,1,0,1,2,3]).sort(axis=0)

In [14]: df
Out[14]: 
   A  B
0  1  3
0  3  5
1  2  4
1  4  6
2  5  7
3  6  8

In [15]: df.loc[1:]
Out[15]: 
   A  B
1  2  4
1  4  6
2  5  7
3  6  8

In [16]: df.loc[1:2]
Out[16]: 
   A  B
1  2  4
1  4  6
2  5  7
```
",Fixed bug in non-unique indexing with a slice via `loc` and friends.
988,"Error out earlier in apache installer when mod_ssl is not available
Fixes #7612.

Testing for `ssl_module` patterned after `openssl_version()` code here: https://github.com/certbot/certbot/blob/master/certbot-apache/certbot_apache/_internal/configurator.py#L274

Previously we were getting tests to pass by setting `ssl_module` to `None`, because `enable_mod` figures out if it needs to enable the mod by [testing inclusion](https://github.com/certbot/certbot/blob/master/certbot-apache/certbot_apache/_internal/override_debian.py#L108). Now we error even if it exists but is `None`, so tests need to be updated. Also, I checked through all the places we expect errors to be raised in Apache tests to make sure we weren't accidentally catching the wrong error now, and I couldn't find any past the ones that are fixed here. I'd also expect coverage to catch those, and the ones it was catching are now taken care of.",Improved error message in apache installer when mod_ssl is not available.
989,"HomeMatic dependency upgrade + IP Wall Thermostat support
## Description:
This PR upgrades the dependency and adds support for the HmIP-STHD Wall Thermostat.
The pyhomematic upgrade fixes the BC-PB-2-WM device and adds HM-LC-Ja1PBU-FM to the list of supported covers.

## Checklist:

If the code communicates with devices, web services, or third-party tools:
  - [x] Local tests with `tox` run successfully. **Your PR cannot be merged unless tests pass**
  - [x] New dependencies have been added to the `REQUIREMENTS` variable ([example][ex-requir]).
  - [x] New dependencies have been added to `requirements_all.txt` by running `script/gen_requirements_all.py`.

[ex-requir]: https://github.com/home-assistant/home-assistant/blob/dev/homeassistant/components/keyboard.py#L14
[ex-import]: https://github.com/home-assistant/home-assistant/blob/dev/homeassistant/components/keyboard.py#L54
",Update pyhomematic version from 0.1.28 to 0.1.29.
990,"bpo-24334: Remove inaccurate match_hostname call
Commit 141c5e8c re-added match_hostname() call. The resurrection of the
function call was never intended and was solely a merge mistake.

Signed-off-by: Christian Heimes 



https://bugs.python.org/issue24334

",Remove check_hostname needs server_hostname argument and add match_hostname call.
991,"Pin memory only when training on GPU
`pin_memory()` gives the following error when training on CPU or MPS.
Tested with `pytorch 1.13.1` and `MacBook Air M2`.

## Training command
```
python train.py \
  --dataset=shakespeare \
  --n_layer=4 \
  --n_head=4 \
  --n_embd=64 \
  --compile=False \
  --eval_iters=1 \
  --block_size=64 \
  --batch_size=8 \
  --device=mps
```
## MPS:
```
Traceback (most recent call last):
  File ""/Users/nyn/Github/nanoGPT/train.py"", line 232, in 
    X, Y = get_batch('train') # fetch the very first batch
  File ""/Users/nyn/Github/nanoGPT/train.py"", line 116, in get_batch
    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
NotImplementedError: Could not run 'aten::_pin_memory' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [MPS, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
```
## CPU:
```
Traceback (most recent call last):
  File ""/Users/nyn/Github/nanoGPT/train.py"", line 232, in 
    X, Y = get_batch('train') # fetch the very first batch
  File ""/Users/nyn/Github/nanoGPT/train.py"", line 116, in get_batch
    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
NotImplementedError: Could not run 'aten::_pin_memory' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [MPS, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
```","Change the code to handle GPU/CPU training by checking if the device is ""cuda"" or not."
992,"Add Dailymotion API
Add Dailymotion API under Video. Dailymotion is a video-hosting site similar to Vimeo and YouTube.
","Added new API, Dailymotion, to the list of JSON APIs for web development."
993,"Network: use active AP signal
This signal is safe to add as it only saves the contents of the signal (rather than do calls on the signal, blocking UI)",Remove unused method get_active_ap and replace it with activeAp property.
994,"Removed 500px API
Removed 500px API as it was shutdown on June 15th
https://support.500px.com/hc/en-us/articles/360002435653-API-

Thank you for taking the time to work on a Pull Request for this project!

To ensure your PR is dealt with swiftly please check the following:

- [ x] Your submissions are formatted according to the guidelines in the [contributing guide](CONTRIBUTING.md)
- [ ] Your additions are ordered alphabetically
- [x] Your submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column should be padded with one space on either side
- [x] You have searched the repository for any relevant issues or pull requests
- [ ] Any category you are creating has the minimum requirement of 3 items
- [ x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
",Update to the Photography section in README.md: removed 500px API documentation link.
995,"Add Icon Horse API to Design


- [x] My submission is formatted according to the guidelines in the [contributing guide](/CONTRIBUTING.md)
- [x] My addition is ordered alphabetically
- [x] My submission has a useful description
- [x] The description does not end with punctuation
- [x] Each table column is padded with one space on either side
- [x] I have searched the repository for any relevant issues or pull requests
- [x] Any category I am creating has the minimum requirement of 3 items
- [x] All changes have been [squashed][squash-link] into a single commit

[squash-link]: 
","Added new API documentation for Icon Horse, including a description, auth method, and CORS support."
996,"Fixes wobble on new CRV
Prior to actuator delay update, I felt like .8/.24 was working well here. After this change though, steer oscillation is very apparent, maybe worst than before. Dropping this value proportionately by 1/4 seems to effectively remove wobble and drastically improves drive-quality.","Change value of steerKpV and steerKiV from [0.8, 0.24] to [0.6, 0.18]."
997,"Add missing instructions to make test passed
",Update installation instructions for virtualenv and pip.
998,"added the py311 to target-version config
<!-- Hello! Thanks for submitting a PR. To help make things go a bit more
     smoothly we would appreciate that you go through this template. -->

### Description

The documentation currently doesn't share that python 3.11 is supported as part of the `-t, --target-version` flag. This PR updates the documentation to support that.

### Checklist - did you ...

<!-- If any of the following items aren't relevant for your contribution
     please still tick them so we know you've gone through the checklist.

    All user-facing changes should get an entry. Otherwise, signal to us
    this should get the magical label to silence the CHANGELOG entry check.
    Tests are required for bugfixes and new features. Documentation changes
    are necessary for formatting and most enhancement changes. -->

- [ ] Add an entry in `CHANGES.md` if necessary?
- [ ] Add / update tests if necessary?
- [x] Add new / update outdated documentation?

<!-- Just as a reminder, everyone in all psf/black spaces including PRs
     must follow the PSF Code of Conduct (link below).

     Finally, once again thanks for your time and effort. If you have any
     feedback in regards to your experience contributing here, please
     let us know!

     Helpful links:

      PSF COC: https://www.python.org/psf/conduct/
      Contributing docs: https://black.readthedocs.io/en/latest/contributing/index.html
      Chat on Python Discord: https://discord.gg/RtVdv86PrH -->
Issue: https://github.com/psf/black/issues/3897",Update the documentation to reflect the change in supported Python versions from 3.7 to 3.11.
999,"Add literal + - * / characters to input options 
Also, if the user enters an invalid character, it will ask again","### Description:Improved user input handling and added a main function to the SimpleCalculator. The calculator now accepts '+' for addition, '-' for subtraction, '*' for multiplication, and '/' for division."
